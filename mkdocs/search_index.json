{
    "docs": [
        {
            "location": "/",
            "text": "Bienvenido a la documentaci\u00f3n del Proyecto Red OnLife\n\n\nLa documentaci\u00f3n completa se encuentra en OneNote: Ciberconectividad\n\n\n\n\n\n\nArquitectura \u2013 describe la arquitectura dise\u00f1ada para el Caso 1\n\n\n\n\n\n\nInstalaci\u00f3n \u2013 descripci\u00f3n del montaje e ``instalaci\u00f3n de los componentes\n\n\n\n\n\n\nOperaci\u00f3n \u2013 detalles de la operaci\u00f3n de la primera fase\n\n\n\n\n\n\nCasos de uso\n\n\n\n\n\n\nRepositorio del c\u00f3digo\n\n\nEl c\u00f3digo se encuentra en Telefonica.github.com/CTpd",
            "title": "Inicio"
        },
        {
            "location": "/#bienvenido-a-la-documentacion-del-proyecto-red-onlife",
            "text": "La documentaci\u00f3n completa se encuentra en OneNote: Ciberconectividad    Arquitectura \u2013 describe la arquitectura dise\u00f1ada para el Caso 1    Instalaci\u00f3n \u2013 descripci\u00f3n del montaje e ``instalaci\u00f3n de los componentes    Operaci\u00f3n \u2013 detalles de la operaci\u00f3n de la primera fase    Casos de uso",
            "title": "Bienvenido a la documentaci\u00f3n del Proyecto Red OnLife"
        },
        {
            "location": "/#repositorio-del-codigo",
            "text": "El c\u00f3digo se encuentra en Telefonica.github.com/CTpd",
            "title": "Repositorio del c\u00f3digo"
        },
        {
            "location": "/Arquitectura_Caso_1/",
            "text": "Tabla de ilustraciones\n\n\nIlustraci\u00f3n 1 Arquitectura objetivo de la red programable\n 3\n\n\nIlustraci\u00f3n 2 Simplificaci\u00f3n de protocolos de la red de servicios programables\n 4\n\n\nIlustraci\u00f3n 3 Diagrama arquitect\u00f3nico de los servicios residenciales del caso 1\n 5\n\n\nIlustraci\u00f3n 4 Arquitectura del sistema de administraci\u00f3n y gesti\u00f3n\n 6\n\n\nIlustraci\u00f3n 5 Esquema del caso 1\n 6\n\n\nIlustraci\u00f3n 6 Descomposici\u00f3n del terminal \u00f3ptico de la C.T.\n 13\n\n\nIlustraci\u00f3n 7 Circuito de control del enrutador\n 15\n\n\nIlustraci\u00f3n 8 Adaptar el controlador del tejido a IPv6\n 16\n\n\nIlustraci\u00f3n 9 Esquema del caso 2 - encadenamiento de servicios\n 23\n\n\nIlustraci\u00f3n 10 Diagrama arquitect\u00f3nico de los servicios residenciales del caso 2\n 23\n\n\nIlustraci\u00f3n 11 Arquitectura del sistema de administraci\u00f3n y gesti\u00f3n\n 24\n\n\nIlustraci\u00f3n 12 Despliegue actual de ONOS/SDN-IP en el mundo\n 26\n\n\nIlustraci\u00f3n 13 Despliegue de ONOS SDN enlazando 3 continentes\n 26\n\n\nIlustraci\u00f3n 14 Mapa geogr\u00e1fico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S\n 26\n\n\nIlustraci\u00f3n 15 Diagrama arquitect\u00f3nico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S\n 27\n\n\nIlustraci\u00f3n 16 Mapa geogr\u00e1fico del despliegue de ONOS / SDN-IP en la red FIU/RedClara\n 27\n\n\nIlustraci\u00f3n 17 Diagrama arquitect\u00f3nico del despliegue de ONOS / SDN-IP en la red FIU/RedClara\n 28\n\n\nIlustraci\u00f3n 18 Despliegue de ONOS en Europa\n 28\n\n\nIlustraci\u00f3n 19 Diagrama arquitect\u00f3nico de servicios remotos para empresas\n 29\n\n\nIlustraci\u00f3n 20 Caso de uso de integraci\u00f3n con Atrium \"peering router\"\n 29\n\n\nIlustraci\u00f3n 21 Prueba del caso de uso \"router peering\"\n 30\n\n\nIlustraci\u00f3n 22 Caso de uso programabilidad red IP sobre \u00d3ptico\n 31\n\n\nIlustraci\u00f3n 23 Topolog\u00eda del caso de uso\n 31\n\n\nIlustraci\u00f3n 24 Sistema de administraci\u00f3n y gesti\u00f3n\n 32\n\n\nIlustraci\u00f3n 25 Entorno de administraci\u00f3n y gesti\u00f3n de red\n 32\n\n\nIlustraci\u00f3n 26 Cronograma\n 34\n\n\n\n\nPrueba de concepto del Onlife Network\n\n\nLa idea de estas pruebas se fundamenta en una arquitectura de red de servicios programables, con casos de uso de servicios extremo a extremo y no en la utilizaci\u00f3n de componentes aislados de la red o los sistemas; de esta manera nos aseguramos que cumplimos con las expectativas de las distintas \u00e1reas usuarias de nuestros recursos tecnol\u00f3gicos.\n\n\n\n\nIlustraci\u00f3n 1 Arquitectura objetivo de la red programable\n\n\nLa motivaci\u00f3n es realizar una prueba de concepto que nos permita validar las siguientes hip\u00f3tesis,\n\n\n\n\n\n\nCasos de uso de servicios extremo a extremo\n\n\n\n\n\n\nUtilizaci\u00f3n de componentes f\u00edsicos marca blanca\n\n\n\n\n\n\nUtilizaci\u00f3n de desarrollos en c\u00f3digo abierto\n\n\n\n\n\n\nIntegraci\u00f3n como si se tratara de un servicio programable m\u00e1s unas aplicaciones o servicios de terceros, ofreci\u00e9ndoles como el poder estar muy pr\u00f3ximos al borde de la red\n\n\n\n\n\n\nComprobaci\u00f3n de la facilidad de integraci\u00f3n de distintos y diversos componentes en varias tecnolog\u00edas\n\n\n\n\n\n\nEvaluaci\u00f3n de nuestra capacidad interna para dise\u00f1ar, desarrollar, desplegar y operar las tecnolog\u00edas de red y sistemas\n\n\n\n\n\n\nEl proyecto OnLife Network se ha planificado en varias fases coincidentes con la metodolog\u00eda establecida en la Innovation Call 2016 que consta de varias fases. En la primera fase nos dedicaremos a comprobar y validar las hip\u00f3tesis propuestas en un entorno de laboratorio. En la segunda fase se espera realizar pruebas de campo en los entornos de prueba de Telef\u00f3nica de Espa\u00f1a.\n\n\n\n\nIlustraci\u00f3n 2 Fases del proyecto OnLife Network\n\n\nEsto requiere de una simplificaci\u00f3n extrema de la red que hemos venido construyendo a lo largo de los a\u00f1os, la siguiente figura ilustra el impacto de esta arquitectura en el entorno de una central telef\u00f3nica.\n\n\n\n\nIlustraci\u00f3n 3 Simplificaci\u00f3n de protocolos de la red de servicios programables\n\n\nAcondicionar la C.T. como un centro de servicios programables\n\n\nEste caso de uso se centra en la provisi\u00f3n de servicios de conectividad residenciales desde una central telef\u00f3nica utilizando virtualizaci\u00f3n, ciberconectividad y procesamiento en la nube; teniendo en cuenta que las tres vertientes tecnol\u00f3gicas suman un todo dentro de la central haci\u00e9ndola un ente aut\u00f3nomo.\n\n\nEl caso es similar a uno patrocinado por AT&T y demostrado exitosamente en el Open Networking Summit 2016, nosotros vamos a modificar ciertos componentes para flexibilizar la soluci\u00f3n basados en conceptos similares a los utilizados por Deutsche Telekom en su h\u00edper simplificaci\u00f3n de la red, con el claro objetivo de hacer despliegue m\u00e1s sencillo y estable.\n\n\nUtilizando la \u00faltima versi\u00f3n de ONOS CORD, la soluci\u00f3n de c\u00f3digo abierto para operadores que est\u00e1 desarrollando el ON.Lab, vamos a facilitar la adecuaci\u00f3n a nuestra arquitectura objetivo de los casos de uso y aprovechar en la medida de lo factible el desarrollo que ya est\u00e1 disponible.\n\n\n\n\nIlustraci\u00f3n 4 Diagrama arquitect\u00f3nico de los servicios residenciales del caso 1\n\n\nLa nebulizaci\u00f3n de funciones de red (NFaaS) se realizar\u00e1 con una visi\u00f3n de servicio, contrario a la virtualizaci\u00f3n de elementos aislados e inconexos que est\u00e1n haciendo algunos fabricantes tradicionales. El caso de uso se enfocar\u00e1 en una red de acceso GPON, pero que sin mayor esfuerzo o coste puede migrar a XGS-PON.\n\n\nA diferencia de la propuesta de arquitectura del ETSI, cuya integraci\u00f3n con elementos legados est\u00e1 encontrando demasiadas dificultades, debido en gran parte a la complejidad de su dise\u00f1o y el no haber tenido en cuenta desde un inicio el ingrediente fundamental de la ciberconectividad; nosotros vamos a resolver el caso de uso en el plano de los servicios y no de sus componentes, utilizando componentes virtuales menos complejos y mejor adecuados a la funcionalidad que exigen la ciberconectividad y la programabilidad de la nueva red en un entorno de nube come se muestra en la siguiente ilustraci\u00f3n.\n\n\n\n\nIlustraci\u00f3n 5 Arquitectura del sistema de administraci\u00f3n y gesti\u00f3n\n\n\nRequisitos y beneficios\n\n\nPara asegurar su validez, la prueba de campo deber\u00e1 demostrar los siguientes beneficios\n\n\n\n\nY al igual que CORD hemos tenido en cuenta cumplir con 5 requisitos b\u00e1sicos:\n\n\nHabilitar servicios innovadores\n\n\nLa CTpd como un centro de servicios programables debe habilitar un amplio abanico de servicios, no limitados a servicios de acceso ni debe constre\u00f1ir sin necesidad la implementaci\u00f3n de nuevos servicios. Espec\u00edficamente la CTpd debe habilitar servicios extra\u00eddos de estos v\u00e9rtices:\n\n\n\n\n\n\nAmbos servicios, el de acceso y los de nube convencionales\n\n\n\n\n\n\nServicios desplegados tanto en el plano de datos (NFV) como servicios implementados en el plano de control\n\n\n\n\n\n\nNuestros servicios que son siempre confiables y los no tan confiables de terceros\n\n\n\n\n\n\nExtensible y controlable\n\n\nLa CTpd es una plataforma configurable y no es una soluci\u00f3n cerrada, proporciona los medios para que el operador pueda especificar el portafolio de servicios deseados y las dependencias entre esos servicios. Esto permite que la CTpd sea configurable para distintos mercados y redes de acceso: residencial, empresarial y m\u00f3vil. Tambi\u00e9n debe proporcionar los mecanismos para provisionar y parametrizar estos servicios para atenerse a nuestros objetivos de negocio y operacionales.\n\n\nEficiencia de la infraestructura Marco Polo\n\n\nCTpd est\u00e1 concebida para utilizar infraestructura marca blanca, apoy\u00e1ndonos en el conocimiento adquirido en el proyecto Marco Polo lo extendemos a los servidores, conmutadores y terminales \u00f3pticos de central; con el consiguiente ahorro de costes y probada robustez de estos aparatos especificados por el Open Compute Project. La CTpd debe correr sobre servidores y conmutadores marca blanca, trabajando directamente con los fabricantes de microcircuitos\n\n\nRobustez operativa\n\n\nLa CTpd debe tener en cuenta escenarios de fallo parcial e intermedio, por ello ha sido dise\u00f1ada teniendo en cuenta la posibilidad que el comportamiento en operaci\u00f3n del sistema no est\u00e1 siempre sincronizado con el estado deseado del sistema.\n\n\nSeguridad multiprop\u00f3sito\n\n\nLa seguridad de la CTpd no debe limitarse a distinguir entre gestores y usuarios del sistema, pero debe ser capaz de asegurar el acceso al sistema de varios actores; como pueden ser operadores globales y locales, desarrolladores de servicios, gestores de servicios y abonados al servicio.\n\n\nDecisiones de dise\u00f1o y tecnol\u00f3gicas\n\n\nComo no es pr\u00e1ctico probar todas las variantes de nuestras redes de acceso y segmentos de clientes, esta primera prueba se centra en el acceso G-PON residencial, y el XGS-PON tan pronto est\u00e9 disponible, pero la arquitectura es la misma que utilizaremos para el acceso de cobre, cable, m\u00f3vil y tambi\u00e9n para el segmento empresarial y PyMES.\n\n\nEl dise\u00f1o es para una red innovadora que ofrezca servicios y micro servicios programables, y que ella misma sea programable; no se contempla compatibilidad con funciones de red existentes para los servicios primarios descritos en los casos de uso, por ello se ha realizado un esfuerzo en utilizar elementos programables de c\u00f3digo abierto sobre los cuales vamos a construir la soluci\u00f3n m\u00e1s adecuada a nuestras necesidades.\n\n\nInspirados en algunos principios de las redes del \u201cnuevo-IP\u201d para esta prueba los aplicamos seg\u00fan se describe en esta tabla.\n\n\n\n\n\n\n\n\nPrincipios\n\n\nAplicados al dise\u00f1o de la prueba\n\n\n\n\n\n\n\n\n\n\nReducir la cantidad de tecnolog\u00edas utilizadas\n\n\nUtilizar \u00fanicamente IPv6 y transmisi\u00f3n \u00f3ptica.\n\n\n\n\n\n\nUtilizar una red para todos los servicios \u2013 internet, TV, empresas, etc.\n\n\nUna \u00fanica red de paquetes convergente\n\n\n\n\n\n\nDimensionar la red con capacidad para todo el tr\u00e1fico IP sin p\u00e9rdidas de paquetes\n\n\nUso m\u00e1s eficiente de los recursos de red, dimensionar la red para el tr\u00e1fico IP en hora punta\n\n\n\n\n\n\nEvitar interfaces internas\n\n\nMinimizar el n\u00famero de interfaces internas o de interconexi\u00f3n.\n\n\n\n\n\n\nDistribuir las interconexiones a internet, entregar el tr\u00e1fico saliente r\u00e1pidamente\n\n\n\n\n\n\n\n\nGestionada alrededor de dispositivos y su l\u00f3gica \n\n\nGesti\u00f3n centrada en abstracciones de la red a trav\u00e9s de programas o aplicaciones, con aplicaciones de c\u00f3digo abierto\n\n\n\n\n\n\nLa pol\u00edtica de servicio de los paquetes van por fuera de la carga\n\n\nSeparar el plano de control del de datos utilizando las cabeceras nativas IPv6 para codificar la informaci\u00f3n necesaria, como el tipo de servicio, clase de tr\u00e1fico, direcci\u00f3n, etc.\n\n\n\n\n\n\nUtilizar IPv6 para todas las funciones y servicios internos\n\n\nEn la red no se soportar\u00e1 IPv4 nativo que se convierte en un servicio\n\n\n\n\n\n\n\n\n                                                                                     El \u201cservicio Ethernet de operadora\u201d basado en IPv6                                                                                                                                   |\n\n\n\n| \nRutas determin\u00edsticas y la m\u00e1s corta para todo el tr\u00e1fico en red\n                  | Las tramas de red ser\u00e1n gestionadas por ONOS de acuerdo a las plantillas de tr\u00e1fico establecidas por los planificadores                                                             |\n| \nLos CPD est\u00e1n conectados directamente a los CTpd\n                                  | Los CPD est\u00e1n directamente conectados a las CTPd para evitar construir interfaces internas adicionales para los grandes flujos de tr\u00e1fico                                           |\n\n\nInfraestructura CTpd\n\n\nLa soluci\u00f3n se apoya en infraestructura marca blanca especificada bajo los auspicios del Open Compute Project, toda la infraestructura est\u00e1 montada en bastidores Open CloudServer, que permite un despliegue sencillo y robusto en una central telef\u00f3nica.\n\n\nDispositivos Marco Polo\n\n\nLa soluci\u00f3n propuesta consta de 3 elementos que ya han sido especificados por el OCP y est\u00e1n disponibles comercialmente:\n\n\n\n\n\n\nAcceso GPON\n \u2013 terminal \u00f3ptico en una bandeja con 48 puertos de 2,5 Gbps fabricada por Celestica con el OLT MAC de PMC Sierra\n\n\n\n\n\n\nConmutadores\n \u2013 con 32 puertos de 40 Gbps fabricados por Accton modelo 6712 con ASIC de Broadcom\n\n\n\n\n\n\nServidores\n \u2013 en placas OCS con doble procesador Intel Xeon, 128 GB de memoria y 4 TB de disco, suministrados por AMAX totalmente montados en su bastidor OCS e integrado de f\u00e1brica con los otros 2 componentes.\n\n\n\n\n\n\nEl sistema operativo de los servidores es Linux con Open vSwitch. Los conmutadores se basan en la pila Atrium de ONF, incluyendo Open Network Linux y el agente de OpenFlow Indigo (OF 1.3), y el OpenFlow Data Plane Abstraction (OF-DPA).\n\n\nConfiguraci\u00f3n y dimensionamiento\n\n\nLa plataforma debe tener en consideraci\u00f3n la siguiente escala:\n\n\n\n\n\n\n800 cabeceras de fibra dispersas en la geograf\u00eda nacional donde residir\u00e1 al menos una plataforma CTpd\n\n\n\n\n\n\nLas cabeceras de fibra concentran entre 6.000 y 117.000 l\u00edneas de fibra \u00f3ptica con un objetivo de ocupaci\u00f3n del 50%\n\n\n\n\n\n\nCada l\u00ednea iluminada requerir\u00e1 varios aparatos virtuales para componer su servicio, podemos estimar entre 5 y 10 m\u00e1quinas encadenadas por abonado\n\n\n\n\n\n\nEn este primer bastidor hemos sobredimensionado la capacidad de procesamiento y conmutaci\u00f3n para poder estresar en las pruebas un m\u00e1ximo que aproxime tr\u00e1fico de una central y que luego pueda ser reutilizado en pruebas reales en una central, e incluye.\n\n\n\n\n\n\nAcceso GPON para 6.048 l\u00edneas, distribuido en 2 bandejas 1U de 48 puertos c/u.\n\n\n\n\n\n\nSeis conmutadores dispuestos en un tejido CLOS para mover el de oeste a este y permiti\u00e9ndonos dimensionar horizontalmente la CTpd para ajustarse a las demandas particulares de cada cabecera de fibra. Los 6 conmutadores en esta prueba tienen suficiente para conectar bastantes m\u00e1s servidores y bandejas de acceso que las que estamos conectando en este momento.\n\n\n\n\n\n\nOcho servidores con suficiente capacidad para integrar posibles servicios de empresa que queramos integrar de manera independiente y tambi\u00e9n realizar pruebas con servicios de terceros como Microsoft Office 365, Akamai o Facebook.\n\n\n\n\n\n\nProgramas \u2013 Sistema Onlife C.T.\n\n\nEl sistema de la CTpd se ha formulado en la integraci\u00f3n de proyectos en c\u00f3digo abierto, pero manteniendo la simplicidad de las soluciones evitando la creaci\u00f3n innecesaria de interfaces o capas de gesti\u00f3n que los sistemas seleccionados deber\u00edan ya tener solucionados.\n\n\nN\u00facleo\n\n\nLos componentes seleccionados para esta primera implementaci\u00f3n y realizada en base a la documentaci\u00f3n disponible y la facilidad con que se puede adecuar las soluciones a nuestras necesidades:\n\n\n\n\n\n\nONOS\n \u2013 nuevo sistema operativo de la red, dise\u00f1ado y desarrollado espec\u00edficamente para operadores de telecomunicaciones. Utilizaremos la versi\u00f3n m\u00e1s reciente de ONOS que se distribuye trimestralmente y es compatible hacia atr\u00e1s. ONOS se encarga de controlar la conectividad del servicio y el tejido CLOS, instanciar los circuitos que conforman la red programable y aloja los dispositivos virtuales que controlan el plano de datos.\n\n\n\n\n\n\nOpenNebula\n \u2013 gestor y controlador de nube que gestiona la infraestructura, las m\u00e1quinas y dispositivos virtuales, encaden\u00e1ndolas cuando seg\u00fan sea necesario y se encarga de la creaci\u00f3n y eliminaci\u00f3n din\u00e1mica de las instancias necesarias para proveer el servicio, incluyendo el silic\u00f3n encargado del plano de datos. Tambi\u00e9n gestionar\u00e1 y controlar\u00e1 las aplicaciones virtuales que se generen dentro de contenedores.\n\n\n\n\n\n\nOneFlow\n \u2013 encargados del ensamblaje y composici\u00f3n de los servicios, este subsistema realiza 3 funciones cl\u00e1sicas: modelo de datos, sincronizador e interfaz\n\n\n\n\n\n\nModelo de datos \u2013 es el estado autoritativo de lo que deber\u00eda ser el servicio\n\n\n\n\n\n\nSincronizador \u2013 fuerza a los elementos para que lleguen a ese estado objetivo, sincronizando los conmutadores, servidores y m\u00e1quinas virtuales\n\n\n\n\n\n\nInterfaces \u2013 proveen las funciones de configuraci\u00f3n y controlador\n\n\n\n\n\n\n\n\n\n\nONOS juega dos papeles en esta implementaci\u00f3n, la primera es gestionar el tejido CLOS e implementar los circuitos virtuales que necesita la plataforma; esto se consigue con Forwarding y VTN, que son un par de aplicaciones de ONOS y se acceder\u00e1 a ellas a trav\u00e9s de la interfaz OneFlow de OpenNebula. El segundo papel de ONOS es la de proporcionar una plataforma donde alojar los programas del plano de control de los servicios primarios.\n\n\nOneFlow sirven para controlar la CTpd en su conjunto, y por ende es la interfaz superior de la CTpd. Por ahora empezaremos con una interfaz RESTful y varias interfaces gr\u00e1ficas que ya han sido desarrolladas para la primera prueba de CORD.\n\n\nInventario de servicios\n\n\nLos servicios primarios incluidos en esta prueba ya han sido desarrollados en el proyecto CORD y es posible que requieran peque\u00f1as modificaciones de nuestro lado para adaptarlas a nuestro entorno, estos programas se comunican con ONOS utilizando la interfaz FlowObjectives.\n\n\n\n\n\n\nvOLT\n \u2013 este aplicativo se ejecuta como un componente de ONOS, y cumple las funciones del plano de control que hemos desagregado de los terminales \u00f3pticos monol\u00edticos utilizados s d\u00eda de hoy\n\n\n\n\n\n\nv\nErC \n\u2013 es el servicio encargado de completar y controlar los circuitos del abonado que le permiten conectarse a la red externa, en CORD este servicio es el vRouter que debemos adaptar a nuestra\n\n\n\n\n\n\nvPdC\n \u2013 es la pasarela de cliente que sustituye parte de las funciones realizadas actualmente por el dispositivo del hogar, que ser\u00e1n activadas y ejecutadas desde la C.T.\n\n\n\n\n\n\nTejido CLOS\n \u2013 cumple la funci\u00f3n de distribuir y encaminar el tr\u00e1fico entre la red de acceso, la infraestructura de la central y la red de transporte\n\n\n\n\n\n\nAura vCDN\n \u2013 es un servicio de CDN en el borde desarrollado por Akamai y que su despliegue es din\u00e1mico y el\u00e1stico ajust\u00e1ndose a la demanda del momento en la geograf\u00eda servida por la CTpd\n\n\n\n\n\n\nServicios: todo como un servicio\n\n\nDe los conceptos de nebulizaci\u00f3n que convierten la infraestructura o las plataformas en un servicio y del que Amazon, Google y Microsoft son ahora referentes, utilizaremos en la CTpd la visi\u00f3n de que \u201ctodo es un servicio\u201d y lo trataremos tal cual, lo que nos permite ofrecer soluciones que conjuntan la virtualizaci\u00f3n de funciones de red, la ciberconectividad y los recursos compartidos en un todo al que abstraemos en t\u00e9rminos de servicio.\n\n\nComo los servicios a los que estamos acostumbrados a provisionar en nuestras redes, estos servicios OnLife debemos crearlos y componerlos en productos para ofrecerlos a nuestros clientes; estos nuevos servicios conllevan un par de retos, el primero, los recursos en los que se apoyan pasan de ser infraestructura monol\u00edtica a funcionar en infraestructura Marco Polo, y segundo, debemos amalgamar la nueva infraestructura con los nuevos componentes l\u00f3gicos para poder estar a la altura de Amazon, Google o Microsoft a la hora de ofrecer nuestros servicios.\n\n\nConsiderando todo como un servicio, lo que estamos haciendo es traer la oferta de la nube tradicional al terreno de Telef\u00f3nica, o cualquier otro operador de telecomunicaciones, pasando de alquilar infraestructura (IaaS) o plataformas (PaaS), a proveer servicios de extremo a extremo en la red de acceso. Apalancados en la proximidad geogr\u00e1fica de nuestras centrales al consumidor de datos, ofreceremos estos servicios en ambas direcciones, a nuestros abonados y a los suministradores de contenido.\n\n\nDesde un punto de vista de la red esta arquitectura de servicios nos obliga a tener en cuenta que toda la funcionalidad es escalable y que los servicios son multicomponente, que podemos separar en tres capas y que se ajustan a los procesos eTOM que rigen el desarrollo de nuestros sistemas,\n\n\n\n\n\n\nServicios cara al cliente -> \nproducto\n\n\n\n\n\n\nServicios de usuario -> \nservicios\n\n\n\n\n\n\nServicios b\u00e1sicos -> \nrecursos\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 7 Producto, servicio y recurso en procesos eTOM modificado\n\n\nPara que esta concepci\u00f3n del sistema propuesto sea realizable debemos asumir e incluir en el desarrollo las pautas descritas a continuaci\u00f3n,\n\n\n\n\n\n\nUnificar SDN + NFV + nube porque de manera independiente ninguno de estas soluciones puede resolver la problem\u00e1tica de los servicios de telecomunicaci\u00f3n\n\n\n\n\n\n\nSoportar una multiplicidad de actores que han de intervenir sobre los servicios programables de la red, reconociendo las peculiaridades de cada uno de ellos.\n\n\n\n\n\n\nDesarrolladores de servicios, internos e incluyendo a terceros\n\n\n\n\n\n\nSuministradores de servicios, internos e incluyendo a terceros\n\n\n\n\n\n\nAbonados\n\n\n\n\n\n\nContemplar la gesti\u00f3n autom\u00e1tica del estado del servicio, sin asumir que el estado de todos los servicios es igual, propiciando que existan 2 tipos de estado\n\n\n\n\n\n\nAutoritativo, que define el estado deseado del sistema\n\n\n\n\n\n\nOperacional, que define el estado actual, fluctuante y algunas veces err\u00f3neo del sistema en este momento\n\n\n\n\n\n\nPotenciar redes o circuitos virtuales, distinguiendo entre red y circuito atendiendo la complejidad de la conexi\u00f3n demandada por el abonado, para el segmento residencial hablamos de circuitos y para empresas de redes.\n\n\n\n\n\n\nPrincipio de menor privilegio\n\n\n\n\n\n\nImplementar funcionalidad\n\n\n\n\n\n\nCapas de abstracci\u00f3n\n\n\nPara su gesti\u00f3n creamos una abstracci\u00f3n de las varias capas de los servicios, lo que nos permite pensar en un concepto que podemos denominar plano de control del servicio. En el caso de CORD se ha desarrollado el sistema XOS, que es el mecanismo que realiza la representaci\u00f3n de la estructura de los servicios, este contiene un lenguaje para escribir programas de control, y un ejecutable que aplica estas pol\u00edticas en el sistema operativo\n\n\n\n\n\n\nModelo de datos autoritativo basado en Django\n\n\n\n\n\n\nInterfaces programables para las pol\u00edticas tanto RESTful como OASIS-TOSCA\n\n\n\n\n\n\nSincronizador que utiliza Ansible para mantener el estado operacional de los recursos sincronizados con el estado autoritativo de CORD\n\n\n\n\n\n\nONOS define una abstracci\u00f3n del grafo de la red en un conjunto de conmutadores marca blanca, ONE define un conjunto de primitivas de recursos de la nube sobre unos servidores marca blanca, y sobre esta base dual, la CTpd define tres capas de abstracci\u00f3n que podemos resumir as\u00ed.\n\n\n\n\nEl \ngrafo del servicio\n existente en CORD, pendiente de adaptar a nuestra arquitectura y dise\u00f1o, representa la relaci\u00f3n de dependencia ente un conjunto de servicios que a la vez conforman un producto. CORD modela la composici\u00f3n del servicio como una relaci\u00f3n de tenencia entre un servicio b\u00e1sico y un servicio de tenencia. La tenencia del servicio est\u00e1 anclada a un \nTenedor Principal\n, como ser\u00eda un abonado, que est\u00e1 unido a una o m\u00e1s \ncuentas de usuario\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl \nservicio\n representa un programa multitenencia, el\u00e1sticamente escalable, incluyendo la manera de instanciarle, controlarle y escalar su funcionalidad. CORD modela un servicio como un \ncontrolador de servicio\n que exporta una interfaz multitenencia y un conjunto de \ninstancias del servicio\n que es el\u00e1sticamente escalable, y que colectivamente se instancian en una \nrebanada\n.\n\n\n\n\n\n\nUna r\nebanada\n representa un conjunto o contenedor de recursos de todo el sistema dentro del que se ejecutan los servicios, incluyendo el c\u00f3mo se especifica la inclusi\u00f3n de estos recursos en la infraestructura que les soporta. En la CTpd modelamos una rebanada como un grupo de \nm\u00e1quinas virtuales\n, que se implementan en OpenNebula y un conjunto de \ncircuitos virtuales\n que las implementa ONOS.\n\n\n\n\n\n\nUn \ncircuito virtual\n representa una interconexi\u00f3n de comunicaci\u00f3n entre un conjunto de instancias, se soportan varios tipos de circuitos virtuales, incluy\u00e9ndose los \nprivados\n que conectan instancias con una rebanada, de \nacceso directo\n utilizado por un servicio de tenencia para acceder a un servicio b\u00e1sico direccionando directamente cada instancia del servicio b\u00e1sico, y de \nacceso indirecto\n utilizado por servicio de tenencia para acceder al servicio b\u00e1sico direccionando el servicio como un todo.\n\n\n\n\n\n\nEl mecanismo subyacente en la CTpd que soporta los Circuitos Virtuales se implementa con un par de aplicaciones de control que corren en ONOS. La primera de ellas, llamada VTN, instala reglas de flujo en los OvS que corren en cada servidor para implementar el direccionamiento directo o indirecto. La segunda es Forwarding e implementa flujos entre los servidores y otrs dispositivos f\u00edsicos de la central a trav\u00e9s del tejido de conmutaci\u00f3n.\n\n\nTerminal \u00f3ptico virtual - vOLT\n\n\nEste concepto en ONOS es producto de desmantelar los actuales terminales \u00f3pticos monol\u00edticos de las centrales, por una combinaci\u00f3n de silic\u00f3n Marco Polo y una aplicaci\u00f3n de c\u00f3digo abierto. La primera implementaci\u00f3n se ha enfocado en el acceso GPON, pero es igualmente extrapolable a XGS-PON remplazando la bandeja de silic\u00f3n, algo que AT&T ya ha especificado dentro del Open Compute Project.\n\n\nLas bandejas de puertos \u00f3pticos GPON fabricados bajo la especificaci\u00f3n de AT&T dentro del OCP ya est\u00e1n disponibles en mercado, el proveedor es Celestica quien cuenta con plantas de fabricaci\u00f3n en Valencia. La bandeja incluye los microcircuitos esenciales de GPON MAC controlados por un programa de control remoto que a su vez es gestionado v\u00eda OpenFlow por una aplicaci\u00f3n de alto nivel.\n\n\n\n\nIlustraci\u00f3n 8 Descomposici\u00f3n del terminal \u00f3ptico de la C.T.\n\n\nHay dos piezas de c\u00f3digo que funcionan conjuntamente para implementar la funcionalidad del vOLT. La primera es un agente vOLT que se instancia en una m\u00e1quina virtual y facilita la conexi\u00f3n entre ONOS y el equipo, este agente expone hacia arriba una interfaz OpenFlow lo que permite que sea controlado por ONOS; de ah\u00ed mapea lo mensajes OF a las API nativas del equipo y mensajes OCMI que gestionan las ONT de la red \u00f3ptica pasiva. La segunda pieza de c\u00f3digo es un conjunto de funciones que gestionan algunas de las funciones tradicionales de una terminal \u00f3ptica, como 802.1X, IGMP Snooping, puentes VLAN y OAM; estas funciones de control est\u00e1n implementadas como aplicaciones que se ejecutan sobre ONOS, facilitan instanciar un abonado, autenticaci\u00f3n.\n\n\nArquitectura del circuito de acceso GPON\n\n\nEl tr\u00e1fico de los abonados se identifica dentro de la central telef\u00f3nica por dos etiquetas en los circuitos virtuales, el terminal \u00f3ptico en destino y en la central son responsables del etiquetado y desetiquetado del tr\u00e1fico de cada abonado en lo que este va y viene; ONOS le instruye a la OLT que circuito utilizar mediante mensajes OpenFlow.\n\n\nLa siguiente ilustraci\u00f3n muestra donde ocurre el etiquetado seg\u00fan el tr\u00e1fico transita desde la sede del abonado hasta internet. Dentro del hogar no hay etiquetas\u2026\n\n\n\n\nIlustraci\u00f3n 9 Arquitectura del circuito de acceso PON\n\n\nComponentes l\u00f3gicos\n\n\nAgente vOLT\n\n\nEste agente est\u00e1 construido con \nIndigo\n, \nnetconfd\n, las interfaces propietarias del silic\u00f3n de PMC Sierra, ahora MicroSemi, y una pila OMCI.\n\n\nAplicaciones en ONOS\n\n\n\n\n\n\nvOLT (onos-app-olt) est\u00e1 encargada de configurar las etiquetas del circuito en la OLT\n\n\n\n\n\n\nAAA (onos-app-aaa) tramita la autenticaci\u00f3n entre el dispositivo del hogar y el servidor Radius; cuando el usuario ha sido autorizado, la aplicaci\u00f3n debe instanciar los servicios del usuario en OneFlow\n\n\n\n\n\n\nEnrutador de Central virtualizado - \nErC\n\n\nEsta aplicaci\u00f3n de control de la red se ejecuta en ONOS y es la encargada de agregar los circuitos de los abonados y canalizar el tr\u00e1fico desde/hacia la red de transporte. Como es de esperar esta aplicaci\u00f3n, que la podemos visualizar como un servicio, debe modificarse para acomodar las necesidades espec\u00edficas de las redes de transporte de Telef\u00f3nica y de la red de transporte de Telef\u00f3nica de Espa\u00f1a para este case de uso. En CORD esta aplicaci\u00f3n se conoce como vRouter.\n\n\nRequisitos del ErC\n\n\n\n\n\n\nRealizar enrutamiento mono difusi\u00f3n de y hacia la C.T.; participar en protocolos de enrutamiento din\u00e1mico\n\n\n\n\n\n\nSe\u00f1alizaci\u00f3n y entrega de multidifusi\u00f3n\n\n\n\n\n\n\nAplicar pol\u00edticas de calidad del servicio (QoS)\n\n\n\n\n\n\nRealizar funcionalidad de traducci\u00f3n de direcciones de red (NAT)\n\n\n\n\n\n\nNo realiza todas las funciones de los actuales BNG monol\u00edticos\n\n\n\n\n\n\nDise\u00f1o\n\n\nEl dise\u00f1o del servicio \nErC\n est\u00e1 compuesto de dos partes que se pueden considerar relativamente independientes, la parte de plano de control y la del plano de datos.\n\n\nPlano de Control\n\n\nLa funcionalidad primordial del \nErC\n es la hablar protocolos de enrutamiento con enrutadores externos; para evitar tener que implementar protocolos de enrutamiento dentro de una aplicaci\u00f3n de ONOS, hemos elegido la utilizaci\u00f3n del sistema de enrutamiento Quagga, que es un sistema de c\u00f3digo abierto y soporta una gran variedad de protocolos de enrutamiento. Para el caso de uso de una C.T. no anticipamos problemas de desempe\u00f1o que se podr\u00eda tener con enrutadores globales.\n\n\nQuagga se configurar\u00e1 para comunicarse con los enrutadores de transporte de Telef\u00f3nica y de cara a la CTpd utilizaremos la interfaz \nFIB Push Interface (FPI)\n para comunicar las rutas desde Quagga a ONOS. Desde aqu\u00ed la aplicaci\u00f3n \nErC\n act\u00faa como el gestor del plano de entrega, \nForwarding Plane Manager (FPM)\n, y es capaz de recibir y descodificar rutas de Quagga, y entonces utiliza estas rutas para programar el plano de datos de manera correspondiente.\n\n\nEl dise\u00f1o de este plano de control es similar a la aproximaci\u00f3n utilizada en la aplicaci\u00f3n ONOS SDN-IP, es decir utilizando la filosof\u00eda de construir sobre bloques ya desarrollados. La diferencia reside en que necesitamos soportar m\u00e1s que BGP, porque necesitamos tambi\u00e9n soportar un protocolo de pasarela interior, \nInterior Gateway Protocol (IGP)\n y como mencionamos arriba utilizamos la interfaz FPM para comunicar Quagga y ONOS mientras que en la SDN-IP se utiliza iBGP como conexi\u00f3n.\n\n\nCircuito de control\n\n\nPara que Quagga se comunique con los enrutadores aguas arriba, cierto tr\u00e1fico de control debe fluir entre el servidor Quagga y el enrutador externo. Previamente a intercambiar rutas, la primera tarea del \nErC\n es programar el plano de datos para que este tr\u00e1fico fluya. El servidor Quagga est\u00e1 conectado a un puerto en el plano de datos del \nErC\n, y los paquetes de enrutamiento entrantes y salientes se direccionan a ese puerto; esto permite circunvenir la funci\u00f3n usual de enrutamiento del \nErC\n porque este tr\u00e1fico es de control destinado \u00fanicamente al enrutador mismo.\n\n\n\n\nIlustraci\u00f3n 10 Circuito de control del enrutador\n\n\nMultidifusi\u00f3n\n\n\nEl \nErC\n necesita soportar se\u00f1alizaci\u00f3n de multidifusi\u00f3n PIM-SSM aguas arriba, pero Quagga no gestionar\u00e1 PIM-SSM por lo que una aplicaci\u00f3n dedicada de ONOS se har\u00e1 cargo de gestionar los mensajes PIM.\n\n\nPlano de datos\n\n\nPor ahora la soluci\u00f3n de CORD utiliza un conmutador dedicado para el plano de datos del vRouter y no utiliza el tejido CLOS, eso se hizo por razones de tiempo y aparente complejidad. En el \nErC\n vamos a intentar resolver esto porque no tiene mucho sentido redundar conmutadores fuera del tejido, adem\u00e1s el gestor del tejido dentro de ONOS debemos mejorarlo para que soporte NAT y QoS.\n\n\nvPdC Pasarela de Cliente equivalente al vSG de CORD\n\n\nLa idea de mover a la C.T. parte de la funcionalidad del dispositivo de interconexi\u00f3n del hogar del cliente, no solo por temas de ahorro de coste mantenimiento, sino para poder ofrecer funcionalidad adicional que hoy no es posible ofertar debido en parte al alto coste de despliegue en casa del cliente.\n\n\nEn esta primera fase la funcionalidad que se ha ofrece desde la CTpd se basa en Linux y se ejecutar\u00eda en la m\u00e1quina virtual, o contenedor, de cada cliente. Por ahora son funciones b\u00e1sicas de conectividad con internet y algunos servicios de valor a\u00f1adido, como suspender/reanudar, control parental, etc.\n\n\nLa pasarela cumplir\u00e1 con los principios de arquitectura del \nRFC 7368\n \u201cRedes IPv6 del hogar\u201d que tiene permite tener m\u00faltiples subredes, por ejemplo, para facilitar tener una red privada y otra de invitados, capas de enlace heterog\u00e9neas, componentes inteligentes de servicios p\u00fablicos, y tener suficiente espacio de direccionamiento disponible para permitir que cada dispositivo tenga una \u00fanica direcci\u00f3n global. No se debe esperar que los usuarios en el hogar configuren sus redes, por tanto, el RFC 7368 asume que en la medida de lo posible la red del hogar se auto organiza y auto configura, es decir, que funcionar\u00e1 sin una gesti\u00f3n dedicada por parte del cliente residencial.\n\n\nEs importante distinguir entre direccionamiento y ser contactado, mientras IPv6 ofrece direccionamiento global mediante la utilizaci\u00f3n de direcciones \u00fanicas en el hogar, si un dispositivo puede ser contactado globalmente o no depender\u00e1 de alg\u00fan cortafuegos o configuraci\u00f3n de filtrado, y no de la presencia o utilizaci\u00f3n de un NAT como en IPv4.\n\n\nLa vPdC tambi\u00e9n debe tener en cuenta lo descrito en el \nRFC 7084\n \u201cRequisitos b\u00e1sicos de los enrutadores IPv6 de cliente\u201d, actuar como el borde entre la red del hogar y las redes externas, ser capaz de proporcionar prefijos para la creaci\u00f3n de subredes dentro del hogar, gestionar eficientemente direcciones locales y direcciones globales de cada dispositivo.\n\n\n\n\nIlustraci\u00f3n 11 Arquitectura de la red IPv6 del hogar\n\n\nTejido CLOS, virtualizaci\u00f3n y composici\u00f3n de servicios\n\n\nLa arquitectura de la CTpd y por ende del tejido CLOS tiene las siguientes caracter\u00edsticas:\n\n\n\n\n\n\nEs una soluci\u00f3n de ciberconectividad basada en un tejido CLOS, con conmutadores marca blanca y programas de conmutaci\u00f3n de c\u00f3digo abierto, aunque se podr\u00edan utilizar otros protocoles, la arquitectura se basa totalmente en OpenFlow.\n\n\n\n\n\n\nEl tejido tiene las siguientes caracter\u00edsticas:\n\n\n\n\n\n\nLa conmutaci\u00f3n de nivel 2 en cada bastidor la hacen los conmutadores TOR\n\n\n\n\n\n\nUtilizamos ECMP para el flujo de nivel 3 entre los bastidores\n\n\n\n\n\n\nVLAN cross-connect feature to switch QinQ packets entre las bandejas de acceso \u00f3ptico y las instancias del vSG\n\n\n\n\n\n\nMultidifusi\u00f3n IPv6 para los flujos de IPTV desde las cabeceras de v\u00eddeo hasta los abonados\n\n\n\n\n\n\nSoporte del \nErC\n para conectarse a los enrutadores de transporte si es necesario para establecer rutas p\u00fablicas\n\n\n\n\n\n\nFacilidad de utilizar el tejido CLOS en despliegues mono o multibastidor\n\n\n\n\n\n\nEl tejido CLOS forma la red subyacente en una arquitectura de redes sub y supra yacente, la red supra yacente, a veces referida como el tejido exterior, tambi\u00e9n se basa en SDN con estas caracter\u00edsticas:\n\n\n\n\n\n\nUtilizaci\u00f3n de conmutadores virtuales, OvS con DPDK, con una conexi\u00f3n a medida para el encadenamiento de servicios\n\n\n\n\n\n\nEquilibradores de carga distribuidos para servicios en cada OvS\n\n\n\n\n\n\nT\u00faneles VxLAN en OvS para redes virtuales supra yacentes\n\n\n\n\n\n\nLa ventaja m\u00e1s notoria de utilizar un control de SDN com\u00fan para la infraestructura suprayacente y para el tejido subyacente es que pueden ser orquestados conjuntamente para dar las prestaciones y servicios que demanda una Central Telef\u00f3nica, con la agilidad y eficiencias de las operaciones de un CPD.\n\n\n\n\nIlustraci\u00f3n 12 Adaptar el controlador del tejido a IPv6\n\n\nAplicaciones de control de ONOS\n\n\nEl tejido en el centro de este dise\u00f1o es el encargado de interconectar todos los componentes de la CTpd, incluyendo las bandejas de acceso GPON, nodos de computaci\u00f3n y las tarjetas de interconexi\u00f3n a la red de transporte. La aplicaci\u00f3n de control del tejido se ejecuta sobre ONOS e interact\u00faa con otras aplicaciones necesarias para proporcionar los varios servicios de la CTpd.\n\n\nEn la implementaci\u00f3n para CORD y para facilitar el desarrollo de las varias aplicaciones, se han ejecutado dos instancias de ONOS separando el control del tejido del resto de aplicaciones de servicio, lo recomendado por ON.Lab es utilizar una instancia de ONOS para controlar CORD, y es lo que hemos planificado para la CTpd.\n\n\nInfraestructura y programas del tejido\n\n\nLos conmutadores en el tejido se apalancan y utilizan infraestructura y programas de varios proyectos con licencias abiertas, tal como se ilustra a continuaci\u00f3n.\n\n\n\n\nIlustraci\u00f3n 13 Infraestructura y programas del tejido\n\n\nTodos los conmutadores son id\u00e9nticos, utilizan los mismos programas de control, gesti\u00f3n y conmutaci\u00f3n, la \u00fanica diferencia es su posici\u00f3n en el tejido, esta soluci\u00f3n fue propuesta y desarrollada en el proyecto Atrium de la ONF, e incluye los componentes utilizados en esta soluci\u00f3n, ONL y ONIE como sistema operativo de los conmutadores, tambi\u00e9n utiliza el OF-DPA de Broadcom para abrir varias de sus interfaces propietarias en t\u00e9rminos entendibles por OpenFlow y que se encuentran en su SDK. Esto permite que, en este caso, ONOS pueda programar todas las tablas de encaminamiento en el ASIC del conmutador, para as\u00ed poder utilizar toda la funcionalidad existente en los ASIC de nueva generaci\u00f3n.\n\n\nSoporte del tejido para la comunicaci\u00f3n entre la OLT y el vSG\n\n\nLos abonados residenciales se conectan a la CTpd a trav\u00e9s de la red de acceso GPON, y al descomponer la OLT monol\u00edtica, todas las funciones que se transforman en programas se trasladan a la nube, y una de ellas es el vSG.\n\n\nTal como describimos con anterioridad, el tr\u00e1fico entre la cSG se etiqueta doblemente, con la etiqueta externa identificando la red PON a la que pertenece el abonado, y la etiqueta interior identifica al abonado individualmente.\n\n\nEsta infraestructura del acceso residencial es manejada por la aplicaci\u00f3n de control vOLT, coordinada con OneFlow. Una vez que el cliente ha sido identificado y autenticado, las doble etiquetas del circuito asignadas para el cliente, y se ha instanciado su vSG en un nodo de c\u00f3mputo, la aplicaci\u00f3n que controla el tejido es informada de cu\u00e1l es la OLT a donde est\u00e1 conectado el cliente y el nodo de computaci\u00f3n donde se ha instanciado el vSG de cliente; entonces la aplicaci\u00f3n programa las reglas de encaminamiento dentro del tejido que permiten que la comunicaci\u00f3n fluya dentro del mismo.\n\n\nComposici\u00f3n del servicio\n\n\nLos servicios en la CTpd se ensamblan utilizando las mejores pr\u00e1cticas de las operaciones en la nube; los servicios los instancia el operador o gestor de red utilizando OneFlow, a su vez OneFlow le presenta a ONOS un grafo del servicio para el tr\u00e1fico del abonado. Este grafo es luego descompuesto en reglas de tr\u00e1fico que se programan en la infraestructura de red de la CTpd por la aplicaci\u00f3n VTN de ONOS. En esta secci\u00f3n se hace un breve resumen de las alternativas de implementaci\u00f3n hechas para realizar la composici\u00f3n del servicio en la infraestructura de red.\n\n\nEn la CTpd la composici\u00f3n del servicio se implementa utilizando redes suprayacentes y virtualizaci\u00f3n de la red, brevemente,\n\n\n\n\n\n\nLos servicios tienen su propio circuito virtual, CV \u2013 las m\u00e1quinas virtuales o contenedores que instancian el servicio son parte del mismo CV, y estas instancias se pueden crear en cualquier lugar de la nube del CTpd, es decir en cualquier nodo de c\u00f3mputo o en distintos bastidores\n\n\n\n\n\n\nIncrementar o reducir din\u00e1micamente la cantidad de m\u00e1quinas virtuales / contenedores, y por ende del mismo CV, esta caracter\u00edstica es primordial para tener escalabilidad en la nube\n\n\n\n\n\n\nCada nodo de c\u00f3mputo aloja m\u00e1quinas virtuales o contenedores, pertenecientes a CV de m\u00faltiples servicios, conectadas a OvS que act\u00faan como conmutadores de hipervisor muy programables y controlados con OpenFlow\n\n\n\n\n\n\nCada CV o servicio tiene su propio balanceador de carga distribuido a lo largo de cada OvS en la red. La funci\u00f3n del balanceador de carga es seleccionar la instancia de una m\u00e1quina virtual que est\u00e1 instanciando un servicio, entre todas las m\u00e1quinas virtuales dentro del CV del servicio\n\n\n\n\n\n\nProceso de composici\u00f3n de un servicio: Digamos que una m\u00e1quina virtual S1B, que es una instancia en el CV Servicio 1, descubre que\n\n\n\n\n\n\nConstrucci\u00f3n de la prueba\n\n\nEl caso base que vamos a construir y que ser\u00e1 la implementaci\u00f3n de referencia, comprende los siguientes fundamentos:\n\n\n\n\n\n\nEst\u00e1 desarrollada con componentes de c\u00f3digo abierto\n\n\n\n\n\n\nEs un sistema integrado que est\u00e1 completo para someter a pruebas de campo\n\n\n\n\n\n\nVa a ejercitar todas las funcionalidades para las que se ha dise\u00f1ado\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 6 Esquema del caso 1\n\n\nLa arquitectura convencional de CTpd ser\u00e1 simplificada en el objetivo de validar el dise\u00f1o de esta implementaci\u00f3n. Esta secci\u00f3n describe los componentes involucrados en este dise\u00f1o.\n\n\nComponentes\n\n\nFigura 2.\n Arquitectura de la maqueta\n\n\nEn la \nFigura 2\n podemos identificar los componentes de la prueba de concepto, que est\u00e1n a su vez detallados en la \nTabla 1\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\n\n\nTipo\n\n\nDescripci\u00f3n\n\n\n\n\n\n\nNodos de Computaci\u00f3n\n\n\nNodo F\u00edsico\n\n\n2 Nodos F\u00edsicos. Todas las VLANs est\u00e1n conectadas a ambos nodos. Exceptio la red de Cliente que solo est\u00e1 conectada a \ntest-oln-hn-01\n.\n\n\n\n\n\n\nONE\n\n\nM\u00e1quina Virtual\n\n\nInstalaci\u00f3n de OpenNebula y de OneFlow. Responsable del despliegue de la MVs y de delegar la configuraci\u00f3n de red en ONOS..\n\n\n\n\n\n\nRadius\n\n\nM\u00e1quina Virtual\n\n\nServidor de autenticaci\u00f3n. Ser\u00e1 accedido por la aplicaci\u00f3n AAA de ONOS.\n\n\n\n\n\n\nONOS\n\n\nM\u00e1quina Virtual\n\n\nInstalaci\u00f3n del SDN ONOS. Las siguientes aplicaci\u00f3n estar\u00e1 desplegadas:\n\n\n\n\n\n\n\n\n                                           -   AAA\n\n                                           -   OLT\n\n                                           -   FWD\n\n                                           -   VR     |\n\n\n\n| vOLT                      | M\u00e1quina Virtual | Esta MV act\u00faa como el punto de entrada del tr\u00e1fico del cliente al CTpd. Dispone de una instancia de Open vSwitch que est\u00e1 controlado por ONOS y que enruta el tr\u00e1fico al tejido CLOS.                                                                                                                                                                                                                                  \n\n                                               Ya que est\u00e1 MV est\u00e1 conectada a la red del Cliente deber\u00e1 ser ejecutada en \ntest-oln-hn-01\n.      |                                                                            \n\n| CLOS                      | M\u00e1quina Virtual | Una instancia de mininet que simula el tejido CLOS.                                                                                                                                   |\n| DNS                       | M\u00e1quina Virtual | Servicio DNS.                                                                                                                                                                         |\n| Portal Cautivo            | M\u00e1quina Virtual | El servicio Telco 3.0, que estar\u00e1 conectado a la base de datos de los usuarios y determinar\u00e1 si un cliente tiene acceso o no a internet.                                              |\n| vErC                      | M\u00e1quina Virtual | Enrutador de la Central. Provee acceso a internet.                                                                                                                                    |\n| vPdC                      | M\u00e1quina Virtual | Pasarela de Cliente. Esta MV ser\u00e1 instanciada por OneFlow para cada cliente e implementar\u00e1 los servicios b\u00e1sicos de conectividad: configuraci\u00f3n DNS, DHCP y anuncio de prefijos IPv6. |\n| Bridge de red de Cliente  | Bridge de Linux | Este bridge est\u00e1 conectado a un HGU (Home Gateway Unit) que generar\u00e1 el tr\u00e1fico de Cliente. Se trata de una red puramente ethernet/L2 que est\u00e1 conectada al vOLT.                     |\n| Bridge de red de Servicio | Bridge de Linux | La red interna de Servicios.                                                                                                                                                          |\n| Bridge de red de Gesti\u00f3n  | Bridge de Linux | Todas las MVs estar\u00e1n conectadas a la red de Gesti\u00f3n de atrav\u00e9s de este bridge.                                                                                                       |\n\n\nTable 1.\n PoC Architecture Components\n\n\nRedes\n\n\nExisten varias redes utilizadas en este proyecto, que est\u00e1n detalladas en la \nTabla 2\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\n\n\nVLAN\n\n\nTipo/Direccionamiento\n\n\nDescripci\u00f3n\n\n\n\n\n\n\nGesti\u00f3n (OLN_MGMT)\n\n\n302\n\n\nEthernet L2\n\n\nRed de Gesti\u00f3n para todas las MVs de la maqueta.\n\n\n\n\n\n\nCliente (OLN_CLI)\n\n\n304\n\n\nIPv6 2a02:9008:4:b110::/64\n\n\nRed de Cliente.\n\n\n\n\n\n\nServicio (OLN_SRV)\n\n\n303\n\n\nIPv4\n\n\n\n\n\n\n\n\n10.95.84.0/26\n\n\nRed de Servicio, usada por los servicios internos del CLOS.\n\n\n\n\n\n\n\n\n\n\n\n\nTabla 2.\n Redes de la Prueba de Concepto.\n\n\nRecursos de Hardware\n\n\nLos recursos de Hardware utilizados en esta prueba de concepto est\u00e1n descritos en la \nTabla 3\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNombre\n\n\nDescripci\u00f3n\n\n\n\n\n\n\ntest-oln-cn-01\n\n\nNodo de Computaci\u00f3n que ser\u00e1 utilizado como un hipervisor de OpenNebula, utilizando la tecnolog\u00eda KVM como herramienta de virtualizaci\u00f3n.\n\n\n\n\n\n\n\n\n              -   CPU: 8x Intel(R) Xeon(R) CPU E5450 @ 3.00GHz\n\n              -   Memoria: 16 GB\n\n              -   Almacenamiento: 236 GB                                                                                                                                  |\n\n\n\n| test-oln-hn-01 | Nodo de Computaci\u00f3n que ser\u00e1 utilizado como un hipervisor de OpenNebula, utilizando la tecnolog\u00eda KVM como herramienta de virtualizaci\u00f3n.                  \n\n\n              -   CPU: 8x Intel(R) Xeon(R) CPU E5450 @ 3.00GHz\n\n              -   Memoria: 32 GB\n\n              -   Almacenamiento: 224 GB                                                                                                                                  |\n\n\n\n| HGU            | Un Home Gateway Unit que generar\u00e1 y simular\u00e1 el tr\u00e1fico del Cliente.                                                                                       \n\n\n              Estar\u00e7a conectado a la maqueta a trav\u00e9s de la red del Cliente y del componente vOLT                                                                         |\n\n\n\n| Soporte de Red | La prueba de concepto tiene lugar en el seno de una red que particiona las VLANs 302, 303 y 304, y que a su vez provee enrutado para el acceso a internet. |\n\n\nTabla 3.\n PoC Hardware Resources\n\n\nOpenNebula\n\n\nDise\u00f1o\n\n\nEn la \nTabla 4\n se muestra un resumen de la configuraci\u00f3n del despliegue de OpenNebula.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSistema Operativo\n\n\nUbuntu 14.04 tanto en el Front-end como en los nodos de computaci\u00f3n.\n\n\n\n\n\n\nHipervisor\n\n\n2 nodos de computaci\u00f3n usando KVM.\n\n\n\n\n\n\nBase de Datos\n\n\nMySQL\n\n\n\n\n\n\nRedes\n\n\nBridges de Linux y tecnolog\u00eda 802.1Q para la implementaci\u00f3n de la segmentaci\u00f3n por VLANs.\n\n\n\n\n\n\nAlmacenamiento\n\n\nAlmacenamiento local del Hipervisor. ~ 230 GB.\n\n\n\n\n\n\nAutenticaci\u00f3n\n\n\nNativo de OpenNebula\n\n\n\n\n\n\nInterfaces\n\n\nOneFlow, CLI, Sunstone\n\n\n\n\n\n\n\n\n\nTable 4.\n PoC Hardware Resources\n\n\nTanto la MV de OpenNebula como los nodos f\u00edsicos de Computaci\u00f3n han sido provisionados usando Ansible. El playbook est\u00e1 disponible aqu\u00ed: \nhttps://github.com/Telefonica/ctpd/tree/master/ansible\n. Esto permitir\u00e1 una reinstalaci\u00f3n uniforme e id\u00e9ntica en el futuro. Dado que este playbook contiene los detalles exactos de la provisi\u00f3n, para m\u00e1s detalles revisar este recurso.\n\n\nFront-end de OpenNebula\n\n\nOpenNebula ha sido desplegada en una M\u00e1quina Virtual que est\u00e1 siendo ejecutada en el nodo \ntest-oln-cn-01\n. Ha sido desplegada manualmente y configurada para exponer los siguientes servicios:\n\n\n\n\n\n\nOpenNebula 4.14.2 (oned)\n\n\n\n\n\n\nPlanificador (mm_sched)\n\n\n\n\n\n\nOneFlow (oneflow-server)\n\n\n\n\n\n\nSunstone (sunstone-server)\n\n\n\n\n\n\nL\u00ednea de Comandos de Linux\n\n\n\n\n\n\nActualmente solo se ha desplegado un \u00fanico Front-end, por lo que no hay Alta Disponibilidad. Sin embargo el entorno est\u00e1 preparado para configurarse en modo de Alta Disponibilidad tras desplegar un segundo nodo de Front-end.\n\n\nNodos de Computaci\u00f3n\n\n\nLos nodos de computaci\u00f3n son responsables de proveer a las M\u00e1quinas Virtuales con los recursos necesario (por ejemplo CPU, Memoria, acceso a la red). OnLife es homog\u00e9nea en cuanto a la tecnolog\u00eda utilizada para virtualizar, manejando 2 nodos con el hipervisor KVM. La configuraci\u00f3n de los nodos tambi\u00e9n es homog\u00e9nea en t\u00e9rmino de componentes de software instalados.\n\n\nPara los nodos de computaci\u00f3n se han instalado el siguiente software:\n\n\n\n\n\n\nLibvirt y qemu-kvm\n\n\n\n\n\n\nPaquete de nodo de OpenNebula 4.14.2 KVM\n\n\n\n\n\n\nEl usuario \noneadmin\n tiene acceso de administraci\u00f3n al almacenamiento, redes y virtualizaci\u00f3n.\n\n\n\n\n\n\nConexi\u00f3n SSH sin password entre los nodos y el Front-end.\n\n\n\n\n\n\nRuby instalado (utilizado por las sondas y drivers de OpenNebula)\n\n\n\n\n\n\nAlmacenamiento\n\n\nOnLife utilizar\u00e1 el almacenamiento local disponible en los nodos, usando el driver SSH. Esto significa que no se ha implementado ninguna configuraci\u00f3n espec\u00edfica para el almacenamiento.\n\n\nRedes\n\n\nTodav\u00eda por determinar.\n\n\nAutenticaci\u00f3n\n\n\nLos usuarios se mantendr\u00e1n en la base de datos de OpenNebula, como usuarios nativos, y la autenticaci\u00f3n se realizar\u00e1 mediante usuario/password.\n\n\nConfiguraci\u00f3n del Front-end\n\n\nOpenNebula se ha instalado y configurado para ejecutar los servicios b\u00e1sicos de OpenNebula: \noned\n, el planificador \nmm_sched\n, \nOneFlow\n y \nSunstone\n).\n\n\nEl sistema operativo es Ubuntu 14.04 obtenido directamente de los repositorios de Ubuntu.\n\n\nEn la \nTabla 5\n se detallan las modificaciones a los ficheros de configuraci\u00f3n pertenecientes a OpenNebula.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFichero de Configuraci\u00f3n\n\n\nDescripci\u00f3n\n\n\n\n\n\n\noned.conf\n\n\n# Sample configuration for MySQL\n\n\n\n\n\n\n\n\n                            DB = \\[ backend = \"mysql\",\n\n                            server = \"localhost\",\n\n                            port = 0,\n\n                            user = \"oneadmin\",\n\n                            passwd = \"opennebula\",\n\n                            db\\_name = \"opennebula\" \\]         |\n\n\n\n| sunstone-server.conf         | :host: 0.0.0.0                    |\n\n\n\nTabla 5.\n Resumen de los cambios de configuraci\u00f3n de OpenNebula (relativos a /etc/one)\n\n\nConfiguraci\u00f3n de los Nodos de Computaci\u00f3n\n\n\nLos nodos de computaci\u00f3n han sido provisionados siguiendo este procedimiento:\n\n\n\n\n\n\nHabilitar el repositorio de OpenNebula.\n\n\n\n\n\n\nInstalaci\u00f3n del paquete \nopennebula-node\n.\n\n\n\n\n\n\nLos ficheros de configuraci\u00f3n alterados durante la instalaci\u00f3n est\u00e1n descritos en la \nTabla 6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFichero de Configuraci\u00f3n\n\n\nDescripci\u00f3n\n\n\n\n\n\n\n/etc/apparmor.d/abstractions/libvirt-qemu\n\n\n/srv/** rw,\n\n\n\n\n\n\n\n\n                                         /var/lib/one/datastores/\\*\\* rw  |\n\n\n\n\nTabla 6.\n Resumen de los cambios en los ficheros de configuraci\u00f3n.\n\n\nIm\u00e1genes y Templates\n\n\nSe har\u00e1 disponible una \u00fanica imagen en la Nube de OpenNebula: una Ubuntu 14.04. Esta imagen ser\u00e1 utilizada como Sistema Operativo para todas las M\u00e1quinas Virtuales y servicios que se utilizar\u00e1n en esta maqueta: vOLT, ONOS, Portal Cautivo, etc.\n\n\nCuando se despliegue un nuevo servicio basado en una M\u00e1quina Virtual, la imagen de base deber\u00e1 ser clonada y convertida en persistente. A su vez se deber\u00e1 de crear un nuevo template referenciando esa imagen. Opcionalmente las IPs se podr\u00e1n listar de manera espec\u00edfica en el template de forma que un mismo servicio siempre tenga la misma IP.\n\n\nNodos / Clusters / VDCs\n\n\nLos dos nodos de KVM son homog\u00e9neos. No se crear\u00e1n clusters y todos los recursos ser\u00e1n incluidos en el VDC por defecto. Los dos nodos registrados en la nube no disponen de la misma capacidad, sin embargo OpenNebula monitorizar\u00e1 la capacidad disponible y escoger\u00e1 d\u00f3nde desplegar cada M\u00e1quina Virtual.\n\n\nEl nodo \ntest-oln-hn-01\n es el \u00fanico conectado a la red \nOLN_CLI,\n por lo que la M\u00e1quina Virtual de vOLT deber\u00e1 ser ejecutada en este nodo espec\u00edficamente.\n\n\nAlmacenamiento y Datastores\n\n\nEl storage por defecto ser\u00e1 local en los nodos de Computaci\u00f3n. Por lo tanto, tanto el Datastore de Sistema como el de Im\u00e1genes deber\u00e1n utilizar TM_MAD=\"ssh\". Las im\u00e1genes ser\u00e1n almacenadas en formato Qcow2.\n\n\nRedes Virtuales.\n\n\nPor determinar.\n\n\nUsuarios y Grupos\n\n\nExistir\u00e1 un \u00fanico usuario: \noneadmin\n, al cual le pertenecer\u00e1n todos los recursos.\n\n\nONOS (Telcaria)\n\n\n\n\nSistema de supervisi\u00f3n y gesti\u00f3n\n\n\nEste servicio debe ser capaz de supervisar, y si es necesario, analizar la marcha de distintos componentes de la CTpd en la prestaci\u00f3n de los servicios encomendados. Los principales objetivos y requisitos de este sistema son\n\n\n\n\n\n\nSer una plataforma gen\u00e9rica para el an\u00e1lisis\n\n\n\n\n\n\nDebe ser escalable y soportar multitenencia\n\n\n\n\n\n\nDebe ser posible introducir instrumentos o sondas en los servicios m\u00e1s all\u00e1 de los dispositivos de c\u00f3mputo y conmutaci\u00f3n\n\n\n\n\n\n\nDebe ser posible ajustar el nivel de sondeo en los dispositivos subyacentes\n\n\n\n\n\n\nDebe ser posible agrupar la informaci\u00f3n de las sondas\n\n\n\n\n\n\nDebe ser posible redireccionar los flujos de tr\u00e1fico a trav\u00e9s de una \u201csonda virtual\u201d para obtener una inspecci\u00f3n m\u00e1s profunda que no es posible obtener de los dispositivos subyacentes\n\n\n\n\n\n\nComo se muestra en la siguiente ilustraci\u00f3n, el sistema de supervisi\u00f3n consigue la informaci\u00f3n de las sondas desde distintos elementos de red en la CTpd, incluyendo nodos de c\u00f3mputo, conmutadores, dispositivos de acceso a central y servicios programables ejecut\u00e1ndose en los nodos de c\u00f3mputo; poni\u00e9ndola a disposici\u00f3n de otras aplicaciones de an\u00e1lisis ejecut\u00e1ndose en la CTpd. El sistema se integrar\u00e1 con los servicios residenciales como el vSG, el ErC, la vOLT, aunque la arquitectura permite extenderlo a futuras plataformas de la CTpd como el acceso m\u00f3vil o empresarial.\n\n\n\n\nEnsamblaje del bastidor\n\n\nEstas son las instrucciones esquematizadas para armar el bastidor, en los repositorios \ngithub\n referenciados se encuentran las instrucciones detalladas. El bastidor de la prueba ya vendr\u00e1 con la infraestructura ensamblada de origen y los programas precargados en la planta de ensamblaje del suministrador.\n\n\nInfraestructura\n\n\nLos servidores, conmutadores y bandejas de acceso se pueden ensamblar en varias configuraciones virtuales como se ilustra a seguir\n\n\n\n\nDescribir circuito de control y administraci\u00f3n basado en el OCP/OCS de Microsoft\n\n\nProgramas\n\n\nLa instalaci\u00f3n de los programas que conforman el sistema se har\u00e1 en el orden descrito a continuaci\u00f3n\n\n\n\n\n\n\nOpenNebula\n\n\n\n\n\n\nONOS-CTpd\n\n\n\n\n\n\nONOS-Tejido\n\n\n\n\n\n\nOpenMano + OneFlow\n\n\n\n\n\n\nInstalaci\u00f3n de OpenNebula\n\n\nLas instrucciones para instalar OpenNebula se encuentran en esta direcci\u00f3n IP\n\n\nhttp://docs.opennebula.org/4.14/design_and_installation/building_your_cloud/ignc.html\n\n\ny se puede descargar desde este sitio\n\n\nhttp://downloads.opennebula.org/packages/opennebula-4.14.2/\n\n\nLa instalaci\u00f3n ser\u00eda la siguiente:\n\n\n\n\n\n\n# wget -q -O- \nhttp://downloads.opennebula.org/repo/Ubuntu/repo.key\n | apt-key add -\n\n\n\n\n\n\n# echo \"deb \nhttp://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/\n stable opennebula\" \\\u00a0\u00a0 > /etc/apt/sources.list.d/opennebula.list\n\n\n\n\n\n\n# apt-get update\n\n\n\n\n\n\n# apt-get install opennebula opennebula-sunstone nfs-kernel-server\n\n\n\n\n\n\nCambiar en el Fichero\u00a0/etc/one/sunstone-server.conf\u00a0 \u201c:host: 127.0.0.1\u201d por \u00a0\u201c:host: 0.0.0.0\u201d\n\n\n\n\n\n\n# /etc/init.d/opennebula-sunstone restart\n\n\n\n\n\n\nA\u00f1adir al Fichero vi /etc/exports file \u00a0\u201c/var/lib/one/ *(rw,sync,no_subtree_check,root_squash)\u201d\n\n\n\n\n\n\n# service nfs-kernel-server restart\n\n\n\n\n\n\n# su - oneadmin\n\n\n\n\n\n\n$ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys\n\n\n\n\n\n\n$ cat << EOT > ~/.ssh/config\n\n\n\n\n\n\nHost *\n\n\n\n\n\n\nStrictHostKeyChecking no\n\n\n\n\n\n\nUserKnownHostsFile /dev/null\n\n\n\n\n\n\nEOT\n\n\n\n\n\n\n$ chmod 600 ~/.ssh/config\n\n\n\n\n\n\n# wget -q -O- \nhttp://downloads.opennebula.org/repo/Ubuntu/repo.key\n | apt-key add -\n\n\n\n\n\n\n# echo \"deb \nhttp://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/\n stable opennebula\" > \\\u00a0\u00a0\u00a0\u00a0 /etc/apt/sources.list.d/opennebula.list\n\n\n\n\n\n\n# apt-get update\n\n\n\n\n\n\n# apt-get install opennebula-node nfs-common bridge-utils\n\n\n\n\n\n\nConfigurar fichero red: /etc/network/interfaces\n\n\n\n\n\n\nauto lo\n\n\n\n\n\n\niface lo inet loopback\n\n\n\n\n\n\nauto br0\n\n\n\n\n\n\niface br0 inet dhcp\n\n\n\n\n\n\nbridge_ports eth0\n\n\n\n\n\n\nbridge_fd 9\n\n\n\n\n\n\nbridge_hello 2\n\n\n\n\n\n\nbridge_maxage 12\n\n\n\n\n\n\nbridge_stp off\n\n\n\n\n\n\n# /etc/init.d/networking restart\n\n\n\n\n\n\n# cat << EOT > /etc/libvirt/qemu.conf\n\n\n\n\n\n\nuser\u00a0 = \"oneadmin\"\n\n\n\n\n\n\ngroup = \"oneadmin\"\n\n\n\n\n\n\ndynamic_ownership = 0\n\n\n\n\n\n\nEOT\n\n\n\n\n\n\n# service libvirt-bin restart\n\n\n\n\n\n\nCon esta ejecuci\u00f3n se puede acceder al portal de OpenNebula en la URL:\u00a0 \nhttp://IP:9869\n. La password de oneadmin est\u00e1 en el fichero \u00a0~/.one/one_auth.\n\n\nInstalaci\u00f3n de ONOS\n\n\nLas instrucciones para instalar ONOS se encuentran en esta direcci\u00f3n IP, empezando en el punto 2. de las mismas\n\n\nhttps://wiki.onosproject.org/display/ONOS/ONOS+from+Scratch\n\n\nPero las instrucciones se modificaron de esta manera:\n\n\nInstala \nGit\n:\n\n\n$sudo apt-get install git-core\n\n\nDescarga \nKaraf\n y \nMaven\n:\n\n\nCreate two directories called\u00a0~/Downloads\u00a0and\u00a0~/Applications. Download the\u00a0\nKaraf 3.0.5\n\u00a0and\u00a0\nMaven 3.3.9\n\u00a0binaries (the\u00a0tar.gz\u00a0versions of both) into\u00a0~/Downloads\u00a0and extract it to\u00a0~/Applications. Keep the tar archives in\u00a0~/Downloads; we'll need that later.\n\n\n| Si decides utilizar directorios distintos, como se puede ver a continuaci\u00f3n, entonces tendr\u00e1s que editar el fichero ~/onos/tools/dev/bash_profile (en el caso de la raspberry pi es /home/pi/onos/tools/dev/bash_profile) y cambiar la configuraci\u00f3n all\u00ed. \n\n\n$ mkdir Downloads                                                                                                                                                                                                                                             \n\n\n$ mkdir /opt/Apps                                                                                                                                                                                                                                             \n\n\n$ cd Downloads                                                                                                                                                                                                                                                \n\n\n$ wget \nhttp://archive.apache.org/dist/karaf/3.0.5/apache-karaf-3.0.5.tar.gz\n                                                                                                         \n\n\n$ wget \nhttp://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz\n                                                               \n\n\n$ tar -zxvf apache-karaf-3.0.5.tar.gz -C /opt/Apps/                                                                                                                                                                                                           \n\n\n\n\n\n\n\n\n$ tar -zxvf apache-maven-3.3.9-bin.tar.gz -C /opt/Apps/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstala \njava8\n:\n\n\n$ sudo apt-get install software-properties-common -y\n\n\n$ sudo add-apt-repository ppa:webupd8team/java -y\n\n\n$ sudo apt-get update\n\n\n$ sudo apt-get install oracle-java8-installer oracle-java8-set-default -y\n\n\nEn el caso de que no funcionara, descarga manualmente java (\nhttp://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n)\n\n\nE inst\u00e1lalo:\n\n\n$sudo update-alternatives --install \"/usr/bin/java\" \"java\" \"/opt/Apps/jdk1.8.0_77/bin/java\" 1\n\n\n$sudo update-alternatives --install \"/usr/bin/javac\" \"javac\" \"/opt/Apps/jdk1.8.0_77/bin/javac\" 1\n\n\n$sudo update-alternatives --set java /opt/Apps/jdk1.8.0_77/bin/java\n\n\n$sudo update-alternatives --set javac /opt/Apps/jdk1.8.0_77/bin/javac\n\n\nM\u00e1s info en: \nhttp://www.rpiblog.com/2014/03/installing-oracle-jdk-8-on-raspberry-pi.html\n\n\nDescarga ONOS:\n\n\n$git clone \nhttps://gerrit.onosproject.org/onos\n\n\nA partir de ahora suponemos que ONOS est\u00e1 instalado en el home del usuario, e.g. /home/pi/onos\n\n\nIncluye las siguientes lineas en el fichero \n.bashrc\n:\n\n\n. ~/onos/tools/dev/bash_profile\n\n\nexport ONOS_ROOT=~/onos\n\n\nsource $ONOS_ROOT/tools/dev/bash_profile\n\n\nEjecuta el \nbash\n para que se carguen:\n\n\n$bash\n\n\nComprueba que se han cargado correctamente:\n\n\n$ echo $ONOS_ROOT\n\n\n/home/pi/onos\n\n\n$ echo $KARAF_ROOT\n\n\n/opt/Apps/apache-karaf-3.0.5\n\n\nConfigura la instalaci\u00f3n:\n\n\nEdita el fichero de configuraci\u00f3n:\n\n\n$sudo nano /opt/Apps/apache-karaf-3.0.5/etc/org.apache.karaf.features.cfg\n\n\nIncluye el siguiente texto en la secci\u00f3n de \nfeaturesRepositories:\n\n\nmvn:org.onosproject/onos-features/1.5.0-SNAPSHOT/xml/features\n\n\nCompila con \nmaven\n ONOS:\n\n\n$cd ~/onos\n\n\n$ mvn clean install\u00a0 # or use the alias 'mci'\n\n\nSi todo ha ido bien deber\u00eda compilarse entero, si tienes problemas, aqu\u00ed hay posibles soluciones:\n\n\n\n\n\n\nActualizar java a la \u00faltima versi\u00f3n disponible\n\n\n\n\n\n\nNo hay forma de compilar onos-incubator-rpc-grpc, por lo que la \u00fanica opci\u00f3n es no hacerlo, para ello basta con comentar la siguiente l\u00ednea en el fichero ~/onos/incubator/pom.xml\n\n\n\n\n\n\n\n\n<!--\n\n\n<module>rpc-grpc</module>\n\n\n-->\n\n\n\n\nAhora hay que construir ONOS:\n\n\nhttps://wiki.onosproject.org/display/ONOS/Running+ONOS+locally+on+development+machine\n\n\n$cd /home/pi/onos/tools/build\n\n\n$onos-build\n\n\nEjecutar ONOS:\n\n\n$onos-karaf\n\n\nPara acceder v\u00eda web:\n\n\nhttp://192.168.0.200:8181/onos/ui/login.html#/topo\n\n\nkaraf/karaf\n\n\nonos/rocks\n\n\nAntes de que se me olvide, al intentar instalar mininet dice esto (hay que comprobar que funciona para CentOS):\n\n\nInstall.sh currently only supports Ubuntu, Debian, RedHat and Fedora.\n\n\nLos entregables definidos a la fecha son\n\n\n\n\nCaso 2: Encadenamiento de servicios\n\n\nEsta prueba tiene como objetivo:\n\n\n\n\n\n\nConectar un servicio de conectividad privada (MPLS, principalmente) con un elemento de computaci\u00f3n en la nube que permita activar funciones de red dedicadas al cliente.\n\n\n\n\n\n\nVerificar la capacidad de orquestaci\u00f3n global del entorno desde el BSS hasta la configuraci\u00f3n del servicio de manera autom\u00e1tica.\n\n\n\n\n\n\nVerificar las necesidades operativas que se derivan de este nuevo tipo de servicios\n\n\n\n\n\n\nAnalizar la estructura de costes de estos servicios y proponer soluciones eficientes en costes.\n\n\n\n\n\n\nAlgunas conclusiones:\n\n\n\n\n\n\nLa automatizaci\u00f3n debe incluir tambi\u00e9n una parte, aunque sea controlada de configuraci\u00f3n autom\u00e1tica de la red.\n\n\n\n\n\n\nSe requieren nuevos conocimientos tecnol\u00f3gicos que compaginen conocimientos de routing, programaci\u00f3n y sistemas de gesti\u00f3n, perfiles que a\u00fan no est\u00e1n desarrollados en las empresas como tales.\n\n\n\n\n\n\nSe requiere un alto volumen de negocio para justificar las inversiones globales. Se deben probar soluciones de bajo coste (open source) que permitan reducir costes.\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 14 Esquema del caso 2 - encadenamiento de servicios\n\n\n\n\nIlustraci\u00f3n 15 Diagrama arquitect\u00f3nico de los servicios residenciales del caso 2\n\n\n\n\nIlustraci\u00f3n 16 Arquitectura del sistema de administraci\u00f3n y gesti\u00f3n\n\n\nLos beneficios a obtener en este caso\n\n\n\n\nLos entregables planificados son\n\n\n\n\nCaso 3: SDN a IP (SDN-IP) interconexi\u00f3n de redes ciberdefinidas con redes IP legadas\n\n\nEsta prueba tiene como objetivo:\n\n\n\n\n\n\nProporcionar conectividad nivel 3 sin utilizar enrutadores legados en el n\u00facleo de red\n\n\n\n\n\n\nTransformar con OpenFlow los Sistemas Aut\u00f3nomos (AS) en redes de tr\u00e1nsito IP BGP\n\n\n\n\n\n\nConectar redes ciberdefinidas con redes IP legadas utilizando BGP\n\n\n\n\n\n\nProbar un mecanismo para la migraci\u00f3n progresiva de nuestras redes IP a redes ciberdefinidas\n\n\n\n\n\n\nDemostrar que podemos sustituir en el n\u00facleo de la red los enrutadores por conmutadores\n\n\n\n\n\n\nAgregar en confederaciones BGP distintos dominios administrativos ciberdefinidos, haciendo el plano de control m\u00e1s escalable\n\n\n\n\n\n\nActualmente ONOS se ha desplegado globalmente en varias redes de Am\u00e9rica y Europa, como una soluci\u00f3n de conectividad en nivel 3 para los usuarios, puramente en OpenFlow v\u00eda BGP, sin la necesidad de enrutadores en el n\u00facleo.\n\n\n\n\nIlustraci\u00f3n 17 Despliegue actual de ONOS/SDN-IP en el mundo\n\n\n\n\nIlustraci\u00f3n 18 Despliegue de ONOS SDN enlazando 3 continentes\n\n\nEl primer despliegue sobre una red en explotaci\u00f3n se realiz\u00f3 en EE. UU. en la red Internet2.\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 19 Mapa geogr\u00e1fico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S\n\n\nLa arquitectura utilizada se muestra en la siguiente ilustraci\u00f3n, en nuestro caso queda por definirla para esta prueba, intentando que sea algo muy parecido.\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 20 Diagrama arquitect\u00f3nico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S\n\n\nONOS se ha desplegado al resto de Am\u00e9rica aprovechado la RedCLARA universitaria y de investigaci\u00f3n y se ha extendido a Europa en la red G\u00c9ANT\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 21 Mapa geogr\u00e1fico del despliegue de ONOS / SDN-IP en la red FIU/RedClara\n\n\nLa arquitectura que es similar a la utilizada en Internet2 se muestra en la siguiente ilustraci\u00f3n\n\n\n\n\nq\n\n\n\n\nIlustraci\u00f3n 22 Diagrama arquitect\u00f3nico del despliegue de ONOS / SDN-IP en la red FIU/RedClara\n\n\n\n\nIlustraci\u00f3n 23 Despliegue de ONOS en Europa\n\n\nEn el despliegue europeo se ha utilizado una aplicaci\u00f3n desarrollada para ONOS, llamada ICONA y que permite gestionar de manera eficiente la intercomunicaci\u00f3n de clusters de ONOS geogr\u00e1ficamente distribuidos.\n\n\nCaso 4: Servicios remotos para empresas\n\n\nEste caso de uso se defini\u00f3 conjuntamente con el proyecto Atrium de la Open Networking Foundation con la finalidad de incorporar en ONOS funcionalidad suficiente para permitir realizar \u201crouter peering\u201d con soluciones abiertas y de marca blanca.\n\n\n\n\nIlustraci\u00f3n 24 Diagrama arquitect\u00f3nico de servicios remotos para empresas\n\n\n\n\nIlustraci\u00f3n 25 Caso de uso de integraci\u00f3n con Atrium \"peering router\"\n\n\nEste a\u00f1o se realiz\u00f3 la prueba del caso de uso \u201crouter peering\u201d con enrutadores localizados en Australia y EE.UU.\n\n\n\n\nIlustraci\u00f3n 26 Prueba del caso de uso \"router peering\"\n\n\nCaso 4: \nGesti\u00f3n y programabilidad de las redes IP \u00f3ptica\ns\n\n\nEste caso de uso patrocinado por AT&T puede resumirse como una implementaci\u00f3n de paquetes \u00f3pticos ciberconectados SDN para redes. Incluido en este caso de uso hay aspectos como abstracciones de red multicapa en SDN, una aplicaci\u00f3n, calendario, de ancho de banda bajo demanda de paquetes \u00f3pticos y refinamientos de la interfaz de usuario ONOS para orquestaci\u00f3n, configuraci\u00f3n de capa de datos y visualizaci\u00f3n de las redes flexibles y el\u00e1sticas de transporte \u00f3pticas.\n\n\n\n\n\n\n\n\nIlustraci\u00f3n 27 Caso de uso programabilidad red IP sobre \u00d3ptico\n\n\n\n\nIlustraci\u00f3n 28 Topolog\u00eda del caso de uso\n\n\nSistema de administraci\u00f3n y gesti\u00f3n\n\n\nEl sistema para la administraci\u00f3n y gesti\u00f3n de la virtualizaci\u00f3n y ciberconectividad, as\u00ed como su integraci\u00f3n con los otros sistemas de gesti\u00f3n de la empresa, es clave para el \u00e9xito de estas soluciones, y a la vez est\u00e1 probando ser el punto de inflexi\u00f3n en las distintas integraciones que est\u00e1n intentando vender distintos fabricantes.\n\n\nPara mantener cierta independencia de los fabricantes tradicionales, vamos a utilizar componentes desarrollados en c\u00f3digo abierto por comunidades de usuarios y no tanto por fabricantes tradicionales, esto nos permitir\u00e1 conjuntar un sistema de administraci\u00f3n m\u00e1s acorde con nuestras necesidades globales, pero que al mismo tiempo nos permita reutilizar infraestructura que ya est\u00e1 en producci\u00f3n. Para ello hemos seleccionado OpenMano con OpenNebula, ONOS y queda pendiente verificar la necesidad del componente XOS, soluciones estas muy ligeras de c\u00f3digo pero con las prestaciones que necesita una operadora de telecomunicaciones, a diferencia de soluciones m\u00e1s comerciales como OpenStack, OpenDaylight u otras.\n\n\n\n\nIlustraci\u00f3n 29 Sistema de administraci\u00f3n y gesti\u00f3n\n\n\n\n\nLos componentes del sistema de gesti\u00f3n y administraci\u00f3n a utilizar son:\n\n\n\n\n\n\n\n\nOpenMano\n \u2013 orquestador de las funciones de red virtualizadas\n\n\n\n\n\n\nOpenNebula\n \u2013 gestor y orquestador de nubes que funciona sobre infraestructura heterog\u00e9nea y de forma nativa resuelve la creaci\u00f3n, gesti\u00f3n y administraci\u00f3n de CPDs virtuales geogr\u00e1ficamente dispersos\n\n\n\n\n\n\nONOS\n \u2013 sistema operativo de la red desarrollado por ON.Lab, versi\u00f3n Cardinal o Drake\n\n\n\n\n\n\nIntegraci\u00f3n\n\n\nEl sistema debe ser capaz de administrar una topolog\u00eda nube como la ilustrada\n\n\n\n\nIlustraci\u00f3n 30 Entorno de administraci\u00f3n y gesti\u00f3n de red\n\n\nTeniendo en cuenta que los emplazamientos pueden estar dispersos en una amplia zona geogr\u00e1fica de un pa\u00eds o pueden prestar servicios de una operadora a otra en el mismo o distintos continentes.\n\n\n\n\nDefinici\u00f3n y dimensionamiento de componentes\n\n\nLos principales componentes y su dimensionamiento a utilizar\u2026\n\n\n\n\n\n\nInfraestructura\n\n\n\n\n\n\nServidores: \nOCP\n\n\n\n\n\n\nAlmacenamiento: \nOCP + CEPH\n\n\n\n\n\n\nConmutadores: \nOCP Accton\n\n\n\n\n\n\nCPE: cualquiera\n\n\n\n\n\n\nOLT: OCP PMC-Sierra\n\n\n\n\n\n\n\n\n\n\nSO\n\n\n\n\nComputaci\u00f3n: \nLinux\n\n\n\n\n\n\n\n\nHipervisor: \nKVM\n\n\n\n\n\n\nGestor de la nube y servicios: \nOpenNebula\n\n\n\n\n\n\nS.O. de la red: \nONOS\n\n\n\n\n\n\nAplicaciones: \nvCPE NEC, OpenWRT\n, etc.\n\n\n\n\n\n\nCronograma de actividades\n\n\nEl cronograma de actividades se resume en la siguiente tabla\n\n\n\n\nIlustraci\u00f3n 31 Cronograma\n\n\nEvaluaci\u00f3n de nuestra capacidad para dise\u00f1ar, desarrollar, desplegar y operar las tecnolog\u00edas de red y sistemas\n\n\nEn las varias implantaciones de estas tecnolog\u00edas en operadoras de nuestra envergadura, como ATT, DT, NTT y Verizon, ha quedado demostrado que el conocimiento interno ha sido un factor determinante para conseguir resultados significativos que les han permitido avanzar en implantar redes virtualizadas. En esta prueba vamos a determinar el tipo de conocimiento interno que debemos adquirir para dise\u00f1ar, desarrollar, implantar, operar y mantener esta soluci\u00f3n.",
            "title": "Arquitectura"
        },
        {
            "location": "/Arquitectura_Caso_1/#prueba-de-concepto-del-onlife-network",
            "text": "La idea de estas pruebas se fundamenta en una arquitectura de red de servicios programables, con casos de uso de servicios extremo a extremo y no en la utilizaci\u00f3n de componentes aislados de la red o los sistemas; de esta manera nos aseguramos que cumplimos con las expectativas de las distintas \u00e1reas usuarias de nuestros recursos tecnol\u00f3gicos.   Ilustraci\u00f3n 1 Arquitectura objetivo de la red programable  La motivaci\u00f3n es realizar una prueba de concepto que nos permita validar las siguientes hip\u00f3tesis,    Casos de uso de servicios extremo a extremo    Utilizaci\u00f3n de componentes f\u00edsicos marca blanca    Utilizaci\u00f3n de desarrollos en c\u00f3digo abierto    Integraci\u00f3n como si se tratara de un servicio programable m\u00e1s unas aplicaciones o servicios de terceros, ofreci\u00e9ndoles como el poder estar muy pr\u00f3ximos al borde de la red    Comprobaci\u00f3n de la facilidad de integraci\u00f3n de distintos y diversos componentes en varias tecnolog\u00edas    Evaluaci\u00f3n de nuestra capacidad interna para dise\u00f1ar, desarrollar, desplegar y operar las tecnolog\u00edas de red y sistemas    El proyecto OnLife Network se ha planificado en varias fases coincidentes con la metodolog\u00eda establecida en la Innovation Call 2016 que consta de varias fases. En la primera fase nos dedicaremos a comprobar y validar las hip\u00f3tesis propuestas en un entorno de laboratorio. En la segunda fase se espera realizar pruebas de campo en los entornos de prueba de Telef\u00f3nica de Espa\u00f1a.   Ilustraci\u00f3n 2 Fases del proyecto OnLife Network  Esto requiere de una simplificaci\u00f3n extrema de la red que hemos venido construyendo a lo largo de los a\u00f1os, la siguiente figura ilustra el impacto de esta arquitectura en el entorno de una central telef\u00f3nica.   Ilustraci\u00f3n 3 Simplificaci\u00f3n de protocolos de la red de servicios programables",
            "title": "Prueba de concepto del Onlife Network"
        },
        {
            "location": "/Arquitectura_Caso_1/#acondicionar-la-ct-como-un-centro-de-servicios-programables",
            "text": "Este caso de uso se centra en la provisi\u00f3n de servicios de conectividad residenciales desde una central telef\u00f3nica utilizando virtualizaci\u00f3n, ciberconectividad y procesamiento en la nube; teniendo en cuenta que las tres vertientes tecnol\u00f3gicas suman un todo dentro de la central haci\u00e9ndola un ente aut\u00f3nomo.  El caso es similar a uno patrocinado por AT&T y demostrado exitosamente en el Open Networking Summit 2016, nosotros vamos a modificar ciertos componentes para flexibilizar la soluci\u00f3n basados en conceptos similares a los utilizados por Deutsche Telekom en su h\u00edper simplificaci\u00f3n de la red, con el claro objetivo de hacer despliegue m\u00e1s sencillo y estable.  Utilizando la \u00faltima versi\u00f3n de ONOS CORD, la soluci\u00f3n de c\u00f3digo abierto para operadores que est\u00e1 desarrollando el ON.Lab, vamos a facilitar la adecuaci\u00f3n a nuestra arquitectura objetivo de los casos de uso y aprovechar en la medida de lo factible el desarrollo que ya est\u00e1 disponible.   Ilustraci\u00f3n 4 Diagrama arquitect\u00f3nico de los servicios residenciales del caso 1  La nebulizaci\u00f3n de funciones de red (NFaaS) se realizar\u00e1 con una visi\u00f3n de servicio, contrario a la virtualizaci\u00f3n de elementos aislados e inconexos que est\u00e1n haciendo algunos fabricantes tradicionales. El caso de uso se enfocar\u00e1 en una red de acceso GPON, pero que sin mayor esfuerzo o coste puede migrar a XGS-PON.  A diferencia de la propuesta de arquitectura del ETSI, cuya integraci\u00f3n con elementos legados est\u00e1 encontrando demasiadas dificultades, debido en gran parte a la complejidad de su dise\u00f1o y el no haber tenido en cuenta desde un inicio el ingrediente fundamental de la ciberconectividad; nosotros vamos a resolver el caso de uso en el plano de los servicios y no de sus componentes, utilizando componentes virtuales menos complejos y mejor adecuados a la funcionalidad que exigen la ciberconectividad y la programabilidad de la nueva red en un entorno de nube come se muestra en la siguiente ilustraci\u00f3n.   Ilustraci\u00f3n 5 Arquitectura del sistema de administraci\u00f3n y gesti\u00f3n",
            "title": "Acondicionar la C.T. como un centro de servicios programables"
        },
        {
            "location": "/Arquitectura_Caso_1/#requisitos-y-beneficios",
            "text": "Para asegurar su validez, la prueba de campo deber\u00e1 demostrar los siguientes beneficios   Y al igual que CORD hemos tenido en cuenta cumplir con 5 requisitos b\u00e1sicos:  Habilitar servicios innovadores  La CTpd como un centro de servicios programables debe habilitar un amplio abanico de servicios, no limitados a servicios de acceso ni debe constre\u00f1ir sin necesidad la implementaci\u00f3n de nuevos servicios. Espec\u00edficamente la CTpd debe habilitar servicios extra\u00eddos de estos v\u00e9rtices:    Ambos servicios, el de acceso y los de nube convencionales    Servicios desplegados tanto en el plano de datos (NFV) como servicios implementados en el plano de control    Nuestros servicios que son siempre confiables y los no tan confiables de terceros    Extensible y controlable  La CTpd es una plataforma configurable y no es una soluci\u00f3n cerrada, proporciona los medios para que el operador pueda especificar el portafolio de servicios deseados y las dependencias entre esos servicios. Esto permite que la CTpd sea configurable para distintos mercados y redes de acceso: residencial, empresarial y m\u00f3vil. Tambi\u00e9n debe proporcionar los mecanismos para provisionar y parametrizar estos servicios para atenerse a nuestros objetivos de negocio y operacionales.  Eficiencia de la infraestructura Marco Polo  CTpd est\u00e1 concebida para utilizar infraestructura marca blanca, apoy\u00e1ndonos en el conocimiento adquirido en el proyecto Marco Polo lo extendemos a los servidores, conmutadores y terminales \u00f3pticos de central; con el consiguiente ahorro de costes y probada robustez de estos aparatos especificados por el Open Compute Project. La CTpd debe correr sobre servidores y conmutadores marca blanca, trabajando directamente con los fabricantes de microcircuitos  Robustez operativa  La CTpd debe tener en cuenta escenarios de fallo parcial e intermedio, por ello ha sido dise\u00f1ada teniendo en cuenta la posibilidad que el comportamiento en operaci\u00f3n del sistema no est\u00e1 siempre sincronizado con el estado deseado del sistema.  Seguridad multiprop\u00f3sito  La seguridad de la CTpd no debe limitarse a distinguir entre gestores y usuarios del sistema, pero debe ser capaz de asegurar el acceso al sistema de varios actores; como pueden ser operadores globales y locales, desarrolladores de servicios, gestores de servicios y abonados al servicio.",
            "title": "Requisitos y beneficios"
        },
        {
            "location": "/Arquitectura_Caso_1/#decisiones-de-diseno-y-tecnologicas",
            "text": "Como no es pr\u00e1ctico probar todas las variantes de nuestras redes de acceso y segmentos de clientes, esta primera prueba se centra en el acceso G-PON residencial, y el XGS-PON tan pronto est\u00e9 disponible, pero la arquitectura es la misma que utilizaremos para el acceso de cobre, cable, m\u00f3vil y tambi\u00e9n para el segmento empresarial y PyMES.  El dise\u00f1o es para una red innovadora que ofrezca servicios y micro servicios programables, y que ella misma sea programable; no se contempla compatibilidad con funciones de red existentes para los servicios primarios descritos en los casos de uso, por ello se ha realizado un esfuerzo en utilizar elementos programables de c\u00f3digo abierto sobre los cuales vamos a construir la soluci\u00f3n m\u00e1s adecuada a nuestras necesidades.  Inspirados en algunos principios de las redes del \u201cnuevo-IP\u201d para esta prueba los aplicamos seg\u00fan se describe en esta tabla.     Principios  Aplicados al dise\u00f1o de la prueba      Reducir la cantidad de tecnolog\u00edas utilizadas  Utilizar \u00fanicamente IPv6 y transmisi\u00f3n \u00f3ptica.    Utilizar una red para todos los servicios \u2013 internet, TV, empresas, etc.  Una \u00fanica red de paquetes convergente    Dimensionar la red con capacidad para todo el tr\u00e1fico IP sin p\u00e9rdidas de paquetes  Uso m\u00e1s eficiente de los recursos de red, dimensionar la red para el tr\u00e1fico IP en hora punta    Evitar interfaces internas  Minimizar el n\u00famero de interfaces internas o de interconexi\u00f3n.    Distribuir las interconexiones a internet, entregar el tr\u00e1fico saliente r\u00e1pidamente     Gestionada alrededor de dispositivos y su l\u00f3gica   Gesti\u00f3n centrada en abstracciones de la red a trav\u00e9s de programas o aplicaciones, con aplicaciones de c\u00f3digo abierto    La pol\u00edtica de servicio de los paquetes van por fuera de la carga  Separar el plano de control del de datos utilizando las cabeceras nativas IPv6 para codificar la informaci\u00f3n necesaria, como el tipo de servicio, clase de tr\u00e1fico, direcci\u00f3n, etc.    Utilizar IPv6 para todas las funciones y servicios internos  En la red no se soportar\u00e1 IPv4 nativo que se convierte en un servicio                                                                                          El \u201cservicio Ethernet de operadora\u201d basado en IPv6                                                                                                                                   |  |  Rutas determin\u00edsticas y la m\u00e1s corta para todo el tr\u00e1fico en red                   | Las tramas de red ser\u00e1n gestionadas por ONOS de acuerdo a las plantillas de tr\u00e1fico establecidas por los planificadores                                                             |\n|  Los CPD est\u00e1n conectados directamente a los CTpd                                   | Los CPD est\u00e1n directamente conectados a las CTPd para evitar construir interfaces internas adicionales para los grandes flujos de tr\u00e1fico                                           |",
            "title": "Decisiones de dise\u00f1o y tecnol\u00f3gicas"
        },
        {
            "location": "/Arquitectura_Caso_1/#infraestructura-ctpd",
            "text": "La soluci\u00f3n se apoya en infraestructura marca blanca especificada bajo los auspicios del Open Compute Project, toda la infraestructura est\u00e1 montada en bastidores Open CloudServer, que permite un despliegue sencillo y robusto en una central telef\u00f3nica.  Dispositivos Marco Polo  La soluci\u00f3n propuesta consta de 3 elementos que ya han sido especificados por el OCP y est\u00e1n disponibles comercialmente:    Acceso GPON  \u2013 terminal \u00f3ptico en una bandeja con 48 puertos de 2,5 Gbps fabricada por Celestica con el OLT MAC de PMC Sierra    Conmutadores  \u2013 con 32 puertos de 40 Gbps fabricados por Accton modelo 6712 con ASIC de Broadcom    Servidores  \u2013 en placas OCS con doble procesador Intel Xeon, 128 GB de memoria y 4 TB de disco, suministrados por AMAX totalmente montados en su bastidor OCS e integrado de f\u00e1brica con los otros 2 componentes.    El sistema operativo de los servidores es Linux con Open vSwitch. Los conmutadores se basan en la pila Atrium de ONF, incluyendo Open Network Linux y el agente de OpenFlow Indigo (OF 1.3), y el OpenFlow Data Plane Abstraction (OF-DPA).  Configuraci\u00f3n y dimensionamiento  La plataforma debe tener en consideraci\u00f3n la siguiente escala:    800 cabeceras de fibra dispersas en la geograf\u00eda nacional donde residir\u00e1 al menos una plataforma CTpd    Las cabeceras de fibra concentran entre 6.000 y 117.000 l\u00edneas de fibra \u00f3ptica con un objetivo de ocupaci\u00f3n del 50%    Cada l\u00ednea iluminada requerir\u00e1 varios aparatos virtuales para componer su servicio, podemos estimar entre 5 y 10 m\u00e1quinas encadenadas por abonado    En este primer bastidor hemos sobredimensionado la capacidad de procesamiento y conmutaci\u00f3n para poder estresar en las pruebas un m\u00e1ximo que aproxime tr\u00e1fico de una central y que luego pueda ser reutilizado en pruebas reales en una central, e incluye.    Acceso GPON para 6.048 l\u00edneas, distribuido en 2 bandejas 1U de 48 puertos c/u.    Seis conmutadores dispuestos en un tejido CLOS para mover el de oeste a este y permiti\u00e9ndonos dimensionar horizontalmente la CTpd para ajustarse a las demandas particulares de cada cabecera de fibra. Los 6 conmutadores en esta prueba tienen suficiente para conectar bastantes m\u00e1s servidores y bandejas de acceso que las que estamos conectando en este momento.    Ocho servidores con suficiente capacidad para integrar posibles servicios de empresa que queramos integrar de manera independiente y tambi\u00e9n realizar pruebas con servicios de terceros como Microsoft Office 365, Akamai o Facebook.",
            "title": "Infraestructura CTpd"
        },
        {
            "location": "/Arquitectura_Caso_1/#programas-sistema-onlife-ct",
            "text": "El sistema de la CTpd se ha formulado en la integraci\u00f3n de proyectos en c\u00f3digo abierto, pero manteniendo la simplicidad de las soluciones evitando la creaci\u00f3n innecesaria de interfaces o capas de gesti\u00f3n que los sistemas seleccionados deber\u00edan ya tener solucionados.  N\u00facleo  Los componentes seleccionados para esta primera implementaci\u00f3n y realizada en base a la documentaci\u00f3n disponible y la facilidad con que se puede adecuar las soluciones a nuestras necesidades:    ONOS  \u2013 nuevo sistema operativo de la red, dise\u00f1ado y desarrollado espec\u00edficamente para operadores de telecomunicaciones. Utilizaremos la versi\u00f3n m\u00e1s reciente de ONOS que se distribuye trimestralmente y es compatible hacia atr\u00e1s. ONOS se encarga de controlar la conectividad del servicio y el tejido CLOS, instanciar los circuitos que conforman la red programable y aloja los dispositivos virtuales que controlan el plano de datos.    OpenNebula  \u2013 gestor y controlador de nube que gestiona la infraestructura, las m\u00e1quinas y dispositivos virtuales, encaden\u00e1ndolas cuando seg\u00fan sea necesario y se encarga de la creaci\u00f3n y eliminaci\u00f3n din\u00e1mica de las instancias necesarias para proveer el servicio, incluyendo el silic\u00f3n encargado del plano de datos. Tambi\u00e9n gestionar\u00e1 y controlar\u00e1 las aplicaciones virtuales que se generen dentro de contenedores.    OneFlow  \u2013 encargados del ensamblaje y composici\u00f3n de los servicios, este subsistema realiza 3 funciones cl\u00e1sicas: modelo de datos, sincronizador e interfaz    Modelo de datos \u2013 es el estado autoritativo de lo que deber\u00eda ser el servicio    Sincronizador \u2013 fuerza a los elementos para que lleguen a ese estado objetivo, sincronizando los conmutadores, servidores y m\u00e1quinas virtuales    Interfaces \u2013 proveen las funciones de configuraci\u00f3n y controlador      ONOS juega dos papeles en esta implementaci\u00f3n, la primera es gestionar el tejido CLOS e implementar los circuitos virtuales que necesita la plataforma; esto se consigue con Forwarding y VTN, que son un par de aplicaciones de ONOS y se acceder\u00e1 a ellas a trav\u00e9s de la interfaz OneFlow de OpenNebula. El segundo papel de ONOS es la de proporcionar una plataforma donde alojar los programas del plano de control de los servicios primarios.  OneFlow sirven para controlar la CTpd en su conjunto, y por ende es la interfaz superior de la CTpd. Por ahora empezaremos con una interfaz RESTful y varias interfaces gr\u00e1ficas que ya han sido desarrolladas para la primera prueba de CORD.  Inventario de servicios  Los servicios primarios incluidos en esta prueba ya han sido desarrollados en el proyecto CORD y es posible que requieran peque\u00f1as modificaciones de nuestro lado para adaptarlas a nuestro entorno, estos programas se comunican con ONOS utilizando la interfaz FlowObjectives.    vOLT  \u2013 este aplicativo se ejecuta como un componente de ONOS, y cumple las funciones del plano de control que hemos desagregado de los terminales \u00f3pticos monol\u00edticos utilizados s d\u00eda de hoy    v ErC  \u2013 es el servicio encargado de completar y controlar los circuitos del abonado que le permiten conectarse a la red externa, en CORD este servicio es el vRouter que debemos adaptar a nuestra    vPdC  \u2013 es la pasarela de cliente que sustituye parte de las funciones realizadas actualmente por el dispositivo del hogar, que ser\u00e1n activadas y ejecutadas desde la C.T.    Tejido CLOS  \u2013 cumple la funci\u00f3n de distribuir y encaminar el tr\u00e1fico entre la red de acceso, la infraestructura de la central y la red de transporte    Aura vCDN  \u2013 es un servicio de CDN en el borde desarrollado por Akamai y que su despliegue es din\u00e1mico y el\u00e1stico ajust\u00e1ndose a la demanda del momento en la geograf\u00eda servida por la CTpd",
            "title": "Programas \u2013 Sistema Onlife C.T."
        },
        {
            "location": "/Arquitectura_Caso_1/#servicios-todo-como-un-servicio",
            "text": "De los conceptos de nebulizaci\u00f3n que convierten la infraestructura o las plataformas en un servicio y del que Amazon, Google y Microsoft son ahora referentes, utilizaremos en la CTpd la visi\u00f3n de que \u201ctodo es un servicio\u201d y lo trataremos tal cual, lo que nos permite ofrecer soluciones que conjuntan la virtualizaci\u00f3n de funciones de red, la ciberconectividad y los recursos compartidos en un todo al que abstraemos en t\u00e9rminos de servicio.  Como los servicios a los que estamos acostumbrados a provisionar en nuestras redes, estos servicios OnLife debemos crearlos y componerlos en productos para ofrecerlos a nuestros clientes; estos nuevos servicios conllevan un par de retos, el primero, los recursos en los que se apoyan pasan de ser infraestructura monol\u00edtica a funcionar en infraestructura Marco Polo, y segundo, debemos amalgamar la nueva infraestructura con los nuevos componentes l\u00f3gicos para poder estar a la altura de Amazon, Google o Microsoft a la hora de ofrecer nuestros servicios.  Considerando todo como un servicio, lo que estamos haciendo es traer la oferta de la nube tradicional al terreno de Telef\u00f3nica, o cualquier otro operador de telecomunicaciones, pasando de alquilar infraestructura (IaaS) o plataformas (PaaS), a proveer servicios de extremo a extremo en la red de acceso. Apalancados en la proximidad geogr\u00e1fica de nuestras centrales al consumidor de datos, ofreceremos estos servicios en ambas direcciones, a nuestros abonados y a los suministradores de contenido.  Desde un punto de vista de la red esta arquitectura de servicios nos obliga a tener en cuenta que toda la funcionalidad es escalable y que los servicios son multicomponente, que podemos separar en tres capas y que se ajustan a los procesos eTOM que rigen el desarrollo de nuestros sistemas,    Servicios cara al cliente ->  producto    Servicios de usuario ->  servicios    Servicios b\u00e1sicos ->  recursos     Ilustraci\u00f3n 7 Producto, servicio y recurso en procesos eTOM modificado  Para que esta concepci\u00f3n del sistema propuesto sea realizable debemos asumir e incluir en el desarrollo las pautas descritas a continuaci\u00f3n,    Unificar SDN + NFV + nube porque de manera independiente ninguno de estas soluciones puede resolver la problem\u00e1tica de los servicios de telecomunicaci\u00f3n    Soportar una multiplicidad de actores que han de intervenir sobre los servicios programables de la red, reconociendo las peculiaridades de cada uno de ellos.    Desarrolladores de servicios, internos e incluyendo a terceros    Suministradores de servicios, internos e incluyendo a terceros    Abonados    Contemplar la gesti\u00f3n autom\u00e1tica del estado del servicio, sin asumir que el estado de todos los servicios es igual, propiciando que existan 2 tipos de estado    Autoritativo, que define el estado deseado del sistema    Operacional, que define el estado actual, fluctuante y algunas veces err\u00f3neo del sistema en este momento    Potenciar redes o circuitos virtuales, distinguiendo entre red y circuito atendiendo la complejidad de la conexi\u00f3n demandada por el abonado, para el segmento residencial hablamos de circuitos y para empresas de redes.    Principio de menor privilegio    Implementar funcionalidad    Capas de abstracci\u00f3n  Para su gesti\u00f3n creamos una abstracci\u00f3n de las varias capas de los servicios, lo que nos permite pensar en un concepto que podemos denominar plano de control del servicio. En el caso de CORD se ha desarrollado el sistema XOS, que es el mecanismo que realiza la representaci\u00f3n de la estructura de los servicios, este contiene un lenguaje para escribir programas de control, y un ejecutable que aplica estas pol\u00edticas en el sistema operativo    Modelo de datos autoritativo basado en Django    Interfaces programables para las pol\u00edticas tanto RESTful como OASIS-TOSCA    Sincronizador que utiliza Ansible para mantener el estado operacional de los recursos sincronizados con el estado autoritativo de CORD    ONOS define una abstracci\u00f3n del grafo de la red en un conjunto de conmutadores marca blanca, ONE define un conjunto de primitivas de recursos de la nube sobre unos servidores marca blanca, y sobre esta base dual, la CTpd define tres capas de abstracci\u00f3n que podemos resumir as\u00ed.   El  grafo del servicio  existente en CORD, pendiente de adaptar a nuestra arquitectura y dise\u00f1o, representa la relaci\u00f3n de dependencia ente un conjunto de servicios que a la vez conforman un producto. CORD modela la composici\u00f3n del servicio como una relaci\u00f3n de tenencia entre un servicio b\u00e1sico y un servicio de tenencia. La tenencia del servicio est\u00e1 anclada a un  Tenedor Principal , como ser\u00eda un abonado, que est\u00e1 unido a una o m\u00e1s  cuentas de usuario .        El  servicio  representa un programa multitenencia, el\u00e1sticamente escalable, incluyendo la manera de instanciarle, controlarle y escalar su funcionalidad. CORD modela un servicio como un  controlador de servicio  que exporta una interfaz multitenencia y un conjunto de  instancias del servicio  que es el\u00e1sticamente escalable, y que colectivamente se instancian en una  rebanada .    Una r ebanada  representa un conjunto o contenedor de recursos de todo el sistema dentro del que se ejecutan los servicios, incluyendo el c\u00f3mo se especifica la inclusi\u00f3n de estos recursos en la infraestructura que les soporta. En la CTpd modelamos una rebanada como un grupo de  m\u00e1quinas virtuales , que se implementan en OpenNebula y un conjunto de  circuitos virtuales  que las implementa ONOS.    Un  circuito virtual  representa una interconexi\u00f3n de comunicaci\u00f3n entre un conjunto de instancias, se soportan varios tipos de circuitos virtuales, incluy\u00e9ndose los  privados  que conectan instancias con una rebanada, de  acceso directo  utilizado por un servicio de tenencia para acceder a un servicio b\u00e1sico direccionando directamente cada instancia del servicio b\u00e1sico, y de  acceso indirecto  utilizado por servicio de tenencia para acceder al servicio b\u00e1sico direccionando el servicio como un todo.    El mecanismo subyacente en la CTpd que soporta los Circuitos Virtuales se implementa con un par de aplicaciones de control que corren en ONOS. La primera de ellas, llamada VTN, instala reglas de flujo en los OvS que corren en cada servidor para implementar el direccionamiento directo o indirecto. La segunda es Forwarding e implementa flujos entre los servidores y otrs dispositivos f\u00edsicos de la central a trav\u00e9s del tejido de conmutaci\u00f3n.",
            "title": "Servicios: todo como un servicio"
        },
        {
            "location": "/Arquitectura_Caso_1/#terminal-optico-virtual-volt",
            "text": "Este concepto en ONOS es producto de desmantelar los actuales terminales \u00f3pticos monol\u00edticos de las centrales, por una combinaci\u00f3n de silic\u00f3n Marco Polo y una aplicaci\u00f3n de c\u00f3digo abierto. La primera implementaci\u00f3n se ha enfocado en el acceso GPON, pero es igualmente extrapolable a XGS-PON remplazando la bandeja de silic\u00f3n, algo que AT&T ya ha especificado dentro del Open Compute Project.  Las bandejas de puertos \u00f3pticos GPON fabricados bajo la especificaci\u00f3n de AT&T dentro del OCP ya est\u00e1n disponibles en mercado, el proveedor es Celestica quien cuenta con plantas de fabricaci\u00f3n en Valencia. La bandeja incluye los microcircuitos esenciales de GPON MAC controlados por un programa de control remoto que a su vez es gestionado v\u00eda OpenFlow por una aplicaci\u00f3n de alto nivel.   Ilustraci\u00f3n 8 Descomposici\u00f3n del terminal \u00f3ptico de la C.T.  Hay dos piezas de c\u00f3digo que funcionan conjuntamente para implementar la funcionalidad del vOLT. La primera es un agente vOLT que se instancia en una m\u00e1quina virtual y facilita la conexi\u00f3n entre ONOS y el equipo, este agente expone hacia arriba una interfaz OpenFlow lo que permite que sea controlado por ONOS; de ah\u00ed mapea lo mensajes OF a las API nativas del equipo y mensajes OCMI que gestionan las ONT de la red \u00f3ptica pasiva. La segunda pieza de c\u00f3digo es un conjunto de funciones que gestionan algunas de las funciones tradicionales de una terminal \u00f3ptica, como 802.1X, IGMP Snooping, puentes VLAN y OAM; estas funciones de control est\u00e1n implementadas como aplicaciones que se ejecutan sobre ONOS, facilitan instanciar un abonado, autenticaci\u00f3n.  Arquitectura del circuito de acceso GPON  El tr\u00e1fico de los abonados se identifica dentro de la central telef\u00f3nica por dos etiquetas en los circuitos virtuales, el terminal \u00f3ptico en destino y en la central son responsables del etiquetado y desetiquetado del tr\u00e1fico de cada abonado en lo que este va y viene; ONOS le instruye a la OLT que circuito utilizar mediante mensajes OpenFlow.  La siguiente ilustraci\u00f3n muestra donde ocurre el etiquetado seg\u00fan el tr\u00e1fico transita desde la sede del abonado hasta internet. Dentro del hogar no hay etiquetas\u2026   Ilustraci\u00f3n 9 Arquitectura del circuito de acceso PON  Componentes l\u00f3gicos  Agente vOLT  Este agente est\u00e1 construido con  Indigo ,  netconfd , las interfaces propietarias del silic\u00f3n de PMC Sierra, ahora MicroSemi, y una pila OMCI.  Aplicaciones en ONOS    vOLT (onos-app-olt) est\u00e1 encargada de configurar las etiquetas del circuito en la OLT    AAA (onos-app-aaa) tramita la autenticaci\u00f3n entre el dispositivo del hogar y el servidor Radius; cuando el usuario ha sido autorizado, la aplicaci\u00f3n debe instanciar los servicios del usuario en OneFlow",
            "title": "Terminal \u00f3ptico virtual - vOLT"
        },
        {
            "location": "/Arquitectura_Caso_1/#enrutador-de-central-virtualizado-erc",
            "text": "Esta aplicaci\u00f3n de control de la red se ejecuta en ONOS y es la encargada de agregar los circuitos de los abonados y canalizar el tr\u00e1fico desde/hacia la red de transporte. Como es de esperar esta aplicaci\u00f3n, que la podemos visualizar como un servicio, debe modificarse para acomodar las necesidades espec\u00edficas de las redes de transporte de Telef\u00f3nica y de la red de transporte de Telef\u00f3nica de Espa\u00f1a para este case de uso. En CORD esta aplicaci\u00f3n se conoce como vRouter.  Requisitos del ErC    Realizar enrutamiento mono difusi\u00f3n de y hacia la C.T.; participar en protocolos de enrutamiento din\u00e1mico    Se\u00f1alizaci\u00f3n y entrega de multidifusi\u00f3n    Aplicar pol\u00edticas de calidad del servicio (QoS)    Realizar funcionalidad de traducci\u00f3n de direcciones de red (NAT)    No realiza todas las funciones de los actuales BNG monol\u00edticos    Dise\u00f1o  El dise\u00f1o del servicio  ErC  est\u00e1 compuesto de dos partes que se pueden considerar relativamente independientes, la parte de plano de control y la del plano de datos.  Plano de Control  La funcionalidad primordial del  ErC  es la hablar protocolos de enrutamiento con enrutadores externos; para evitar tener que implementar protocolos de enrutamiento dentro de una aplicaci\u00f3n de ONOS, hemos elegido la utilizaci\u00f3n del sistema de enrutamiento Quagga, que es un sistema de c\u00f3digo abierto y soporta una gran variedad de protocolos de enrutamiento. Para el caso de uso de una C.T. no anticipamos problemas de desempe\u00f1o que se podr\u00eda tener con enrutadores globales.  Quagga se configurar\u00e1 para comunicarse con los enrutadores de transporte de Telef\u00f3nica y de cara a la CTpd utilizaremos la interfaz  FIB Push Interface (FPI)  para comunicar las rutas desde Quagga a ONOS. Desde aqu\u00ed la aplicaci\u00f3n  ErC  act\u00faa como el gestor del plano de entrega,  Forwarding Plane Manager (FPM) , y es capaz de recibir y descodificar rutas de Quagga, y entonces utiliza estas rutas para programar el plano de datos de manera correspondiente.  El dise\u00f1o de este plano de control es similar a la aproximaci\u00f3n utilizada en la aplicaci\u00f3n ONOS SDN-IP, es decir utilizando la filosof\u00eda de construir sobre bloques ya desarrollados. La diferencia reside en que necesitamos soportar m\u00e1s que BGP, porque necesitamos tambi\u00e9n soportar un protocolo de pasarela interior,  Interior Gateway Protocol (IGP)  y como mencionamos arriba utilizamos la interfaz FPM para comunicar Quagga y ONOS mientras que en la SDN-IP se utiliza iBGP como conexi\u00f3n.  Circuito de control  Para que Quagga se comunique con los enrutadores aguas arriba, cierto tr\u00e1fico de control debe fluir entre el servidor Quagga y el enrutador externo. Previamente a intercambiar rutas, la primera tarea del  ErC  es programar el plano de datos para que este tr\u00e1fico fluya. El servidor Quagga est\u00e1 conectado a un puerto en el plano de datos del  ErC , y los paquetes de enrutamiento entrantes y salientes se direccionan a ese puerto; esto permite circunvenir la funci\u00f3n usual de enrutamiento del  ErC  porque este tr\u00e1fico es de control destinado \u00fanicamente al enrutador mismo.   Ilustraci\u00f3n 10 Circuito de control del enrutador  Multidifusi\u00f3n  El  ErC  necesita soportar se\u00f1alizaci\u00f3n de multidifusi\u00f3n PIM-SSM aguas arriba, pero Quagga no gestionar\u00e1 PIM-SSM por lo que una aplicaci\u00f3n dedicada de ONOS se har\u00e1 cargo de gestionar los mensajes PIM.  Plano de datos  Por ahora la soluci\u00f3n de CORD utiliza un conmutador dedicado para el plano de datos del vRouter y no utiliza el tejido CLOS, eso se hizo por razones de tiempo y aparente complejidad. En el  ErC  vamos a intentar resolver esto porque no tiene mucho sentido redundar conmutadores fuera del tejido, adem\u00e1s el gestor del tejido dentro de ONOS debemos mejorarlo para que soporte NAT y QoS.",
            "title": "Enrutador de Central virtualizado - ErC"
        },
        {
            "location": "/Arquitectura_Caso_1/#vpdc-pasarela-de-cliente-equivalente-al-vsg-de-cord",
            "text": "La idea de mover a la C.T. parte de la funcionalidad del dispositivo de interconexi\u00f3n del hogar del cliente, no solo por temas de ahorro de coste mantenimiento, sino para poder ofrecer funcionalidad adicional que hoy no es posible ofertar debido en parte al alto coste de despliegue en casa del cliente.  En esta primera fase la funcionalidad que se ha ofrece desde la CTpd se basa en Linux y se ejecutar\u00eda en la m\u00e1quina virtual, o contenedor, de cada cliente. Por ahora son funciones b\u00e1sicas de conectividad con internet y algunos servicios de valor a\u00f1adido, como suspender/reanudar, control parental, etc.  La pasarela cumplir\u00e1 con los principios de arquitectura del  RFC 7368  \u201cRedes IPv6 del hogar\u201d que tiene permite tener m\u00faltiples subredes, por ejemplo, para facilitar tener una red privada y otra de invitados, capas de enlace heterog\u00e9neas, componentes inteligentes de servicios p\u00fablicos, y tener suficiente espacio de direccionamiento disponible para permitir que cada dispositivo tenga una \u00fanica direcci\u00f3n global. No se debe esperar que los usuarios en el hogar configuren sus redes, por tanto, el RFC 7368 asume que en la medida de lo posible la red del hogar se auto organiza y auto configura, es decir, que funcionar\u00e1 sin una gesti\u00f3n dedicada por parte del cliente residencial.  Es importante distinguir entre direccionamiento y ser contactado, mientras IPv6 ofrece direccionamiento global mediante la utilizaci\u00f3n de direcciones \u00fanicas en el hogar, si un dispositivo puede ser contactado globalmente o no depender\u00e1 de alg\u00fan cortafuegos o configuraci\u00f3n de filtrado, y no de la presencia o utilizaci\u00f3n de un NAT como en IPv4.  La vPdC tambi\u00e9n debe tener en cuenta lo descrito en el  RFC 7084  \u201cRequisitos b\u00e1sicos de los enrutadores IPv6 de cliente\u201d, actuar como el borde entre la red del hogar y las redes externas, ser capaz de proporcionar prefijos para la creaci\u00f3n de subredes dentro del hogar, gestionar eficientemente direcciones locales y direcciones globales de cada dispositivo.   Ilustraci\u00f3n 11 Arquitectura de la red IPv6 del hogar",
            "title": "vPdC Pasarela de Cliente equivalente al vSG de CORD"
        },
        {
            "location": "/Arquitectura_Caso_1/#tejido-clos-virtualizacion-y-composicion-de-servicios",
            "text": "La arquitectura de la CTpd y por ende del tejido CLOS tiene las siguientes caracter\u00edsticas:    Es una soluci\u00f3n de ciberconectividad basada en un tejido CLOS, con conmutadores marca blanca y programas de conmutaci\u00f3n de c\u00f3digo abierto, aunque se podr\u00edan utilizar otros protocoles, la arquitectura se basa totalmente en OpenFlow.    El tejido tiene las siguientes caracter\u00edsticas:    La conmutaci\u00f3n de nivel 2 en cada bastidor la hacen los conmutadores TOR    Utilizamos ECMP para el flujo de nivel 3 entre los bastidores    VLAN cross-connect feature to switch QinQ packets entre las bandejas de acceso \u00f3ptico y las instancias del vSG    Multidifusi\u00f3n IPv6 para los flujos de IPTV desde las cabeceras de v\u00eddeo hasta los abonados    Soporte del  ErC  para conectarse a los enrutadores de transporte si es necesario para establecer rutas p\u00fablicas    Facilidad de utilizar el tejido CLOS en despliegues mono o multibastidor    El tejido CLOS forma la red subyacente en una arquitectura de redes sub y supra yacente, la red supra yacente, a veces referida como el tejido exterior, tambi\u00e9n se basa en SDN con estas caracter\u00edsticas:    Utilizaci\u00f3n de conmutadores virtuales, OvS con DPDK, con una conexi\u00f3n a medida para el encadenamiento de servicios    Equilibradores de carga distribuidos para servicios en cada OvS    T\u00faneles VxLAN en OvS para redes virtuales supra yacentes    La ventaja m\u00e1s notoria de utilizar un control de SDN com\u00fan para la infraestructura suprayacente y para el tejido subyacente es que pueden ser orquestados conjuntamente para dar las prestaciones y servicios que demanda una Central Telef\u00f3nica, con la agilidad y eficiencias de las operaciones de un CPD.   Ilustraci\u00f3n 12 Adaptar el controlador del tejido a IPv6  Aplicaciones de control de ONOS  El tejido en el centro de este dise\u00f1o es el encargado de interconectar todos los componentes de la CTpd, incluyendo las bandejas de acceso GPON, nodos de computaci\u00f3n y las tarjetas de interconexi\u00f3n a la red de transporte. La aplicaci\u00f3n de control del tejido se ejecuta sobre ONOS e interact\u00faa con otras aplicaciones necesarias para proporcionar los varios servicios de la CTpd.  En la implementaci\u00f3n para CORD y para facilitar el desarrollo de las varias aplicaciones, se han ejecutado dos instancias de ONOS separando el control del tejido del resto de aplicaciones de servicio, lo recomendado por ON.Lab es utilizar una instancia de ONOS para controlar CORD, y es lo que hemos planificado para la CTpd.  Infraestructura y programas del tejido  Los conmutadores en el tejido se apalancan y utilizan infraestructura y programas de varios proyectos con licencias abiertas, tal como se ilustra a continuaci\u00f3n.   Ilustraci\u00f3n 13 Infraestructura y programas del tejido  Todos los conmutadores son id\u00e9nticos, utilizan los mismos programas de control, gesti\u00f3n y conmutaci\u00f3n, la \u00fanica diferencia es su posici\u00f3n en el tejido, esta soluci\u00f3n fue propuesta y desarrollada en el proyecto Atrium de la ONF, e incluye los componentes utilizados en esta soluci\u00f3n, ONL y ONIE como sistema operativo de los conmutadores, tambi\u00e9n utiliza el OF-DPA de Broadcom para abrir varias de sus interfaces propietarias en t\u00e9rminos entendibles por OpenFlow y que se encuentran en su SDK. Esto permite que, en este caso, ONOS pueda programar todas las tablas de encaminamiento en el ASIC del conmutador, para as\u00ed poder utilizar toda la funcionalidad existente en los ASIC de nueva generaci\u00f3n.  Soporte del tejido para la comunicaci\u00f3n entre la OLT y el vSG  Los abonados residenciales se conectan a la CTpd a trav\u00e9s de la red de acceso GPON, y al descomponer la OLT monol\u00edtica, todas las funciones que se transforman en programas se trasladan a la nube, y una de ellas es el vSG.  Tal como describimos con anterioridad, el tr\u00e1fico entre la cSG se etiqueta doblemente, con la etiqueta externa identificando la red PON a la que pertenece el abonado, y la etiqueta interior identifica al abonado individualmente.  Esta infraestructura del acceso residencial es manejada por la aplicaci\u00f3n de control vOLT, coordinada con OneFlow. Una vez que el cliente ha sido identificado y autenticado, las doble etiquetas del circuito asignadas para el cliente, y se ha instanciado su vSG en un nodo de c\u00f3mputo, la aplicaci\u00f3n que controla el tejido es informada de cu\u00e1l es la OLT a donde est\u00e1 conectado el cliente y el nodo de computaci\u00f3n donde se ha instanciado el vSG de cliente; entonces la aplicaci\u00f3n programa las reglas de encaminamiento dentro del tejido que permiten que la comunicaci\u00f3n fluya dentro del mismo.  Composici\u00f3n del servicio  Los servicios en la CTpd se ensamblan utilizando las mejores pr\u00e1cticas de las operaciones en la nube; los servicios los instancia el operador o gestor de red utilizando OneFlow, a su vez OneFlow le presenta a ONOS un grafo del servicio para el tr\u00e1fico del abonado. Este grafo es luego descompuesto en reglas de tr\u00e1fico que se programan en la infraestructura de red de la CTpd por la aplicaci\u00f3n VTN de ONOS. En esta secci\u00f3n se hace un breve resumen de las alternativas de implementaci\u00f3n hechas para realizar la composici\u00f3n del servicio en la infraestructura de red.  En la CTpd la composici\u00f3n del servicio se implementa utilizando redes suprayacentes y virtualizaci\u00f3n de la red, brevemente,    Los servicios tienen su propio circuito virtual, CV \u2013 las m\u00e1quinas virtuales o contenedores que instancian el servicio son parte del mismo CV, y estas instancias se pueden crear en cualquier lugar de la nube del CTpd, es decir en cualquier nodo de c\u00f3mputo o en distintos bastidores    Incrementar o reducir din\u00e1micamente la cantidad de m\u00e1quinas virtuales / contenedores, y por ende del mismo CV, esta caracter\u00edstica es primordial para tener escalabilidad en la nube    Cada nodo de c\u00f3mputo aloja m\u00e1quinas virtuales o contenedores, pertenecientes a CV de m\u00faltiples servicios, conectadas a OvS que act\u00faan como conmutadores de hipervisor muy programables y controlados con OpenFlow    Cada CV o servicio tiene su propio balanceador de carga distribuido a lo largo de cada OvS en la red. La funci\u00f3n del balanceador de carga es seleccionar la instancia de una m\u00e1quina virtual que est\u00e1 instanciando un servicio, entre todas las m\u00e1quinas virtuales dentro del CV del servicio    Proceso de composici\u00f3n de un servicio: Digamos que una m\u00e1quina virtual S1B, que es una instancia en el CV Servicio 1, descubre que",
            "title": "Tejido CLOS, virtualizaci\u00f3n y composici\u00f3n de servicios"
        },
        {
            "location": "/Arquitectura_Caso_1/#construccion-de-la-prueba",
            "text": "El caso base que vamos a construir y que ser\u00e1 la implementaci\u00f3n de referencia, comprende los siguientes fundamentos:    Est\u00e1 desarrollada con componentes de c\u00f3digo abierto    Es un sistema integrado que est\u00e1 completo para someter a pruebas de campo    Va a ejercitar todas las funcionalidades para las que se ha dise\u00f1ado     Ilustraci\u00f3n 6 Esquema del caso 1  La arquitectura convencional de CTpd ser\u00e1 simplificada en el objetivo de validar el dise\u00f1o de esta implementaci\u00f3n. Esta secci\u00f3n describe los componentes involucrados en este dise\u00f1o.",
            "title": "Construcci\u00f3n de la prueba"
        },
        {
            "location": "/Arquitectura_Caso_1/#componentes",
            "text": "Figura 2.  Arquitectura de la maqueta  En la  Figura 2  podemos identificar los componentes de la prueba de concepto, que est\u00e1n a su vez detallados en la  Tabla 1 .            Nombre  Tipo  Descripci\u00f3n    Nodos de Computaci\u00f3n  Nodo F\u00edsico  2 Nodos F\u00edsicos. Todas las VLANs est\u00e1n conectadas a ambos nodos. Exceptio la red de Cliente que solo est\u00e1 conectada a  test-oln-hn-01 .    ONE  M\u00e1quina Virtual  Instalaci\u00f3n de OpenNebula y de OneFlow. Responsable del despliegue de la MVs y de delegar la configuraci\u00f3n de red en ONOS..    Radius  M\u00e1quina Virtual  Servidor de autenticaci\u00f3n. Ser\u00e1 accedido por la aplicaci\u00f3n AAA de ONOS.    ONOS  M\u00e1quina Virtual  Instalaci\u00f3n del SDN ONOS. Las siguientes aplicaci\u00f3n estar\u00e1 desplegadas:                                                -   AAA\n\n                                           -   OLT\n\n                                           -   FWD\n\n                                           -   VR     |  | vOLT                      | M\u00e1quina Virtual | Esta MV act\u00faa como el punto de entrada del tr\u00e1fico del cliente al CTpd. Dispone de una instancia de Open vSwitch que est\u00e1 controlado por ONOS y que enruta el tr\u00e1fico al tejido CLOS.                                                                                                                                                                                                                                   \n                                               Ya que est\u00e1 MV est\u00e1 conectada a la red del Cliente deber\u00e1 ser ejecutada en  test-oln-hn-01 .      |                                                                             \n| CLOS                      | M\u00e1quina Virtual | Una instancia de mininet que simula el tejido CLOS.                                                                                                                                   |\n| DNS                       | M\u00e1quina Virtual | Servicio DNS.                                                                                                                                                                         |\n| Portal Cautivo            | M\u00e1quina Virtual | El servicio Telco 3.0, que estar\u00e1 conectado a la base de datos de los usuarios y determinar\u00e1 si un cliente tiene acceso o no a internet.                                              |\n| vErC                      | M\u00e1quina Virtual | Enrutador de la Central. Provee acceso a internet.                                                                                                                                    |\n| vPdC                      | M\u00e1quina Virtual | Pasarela de Cliente. Esta MV ser\u00e1 instanciada por OneFlow para cada cliente e implementar\u00e1 los servicios b\u00e1sicos de conectividad: configuraci\u00f3n DNS, DHCP y anuncio de prefijos IPv6. |\n| Bridge de red de Cliente  | Bridge de Linux | Este bridge est\u00e1 conectado a un HGU (Home Gateway Unit) que generar\u00e1 el tr\u00e1fico de Cliente. Se trata de una red puramente ethernet/L2 que est\u00e1 conectada al vOLT.                     |\n| Bridge de red de Servicio | Bridge de Linux | La red interna de Servicios.                                                                                                                                                          |\n| Bridge de red de Gesti\u00f3n  | Bridge de Linux | Todas las MVs estar\u00e1n conectadas a la red de Gesti\u00f3n de atrav\u00e9s de este bridge.                                                                                                       |  Table 1.  PoC Architecture Components",
            "title": "Componentes"
        },
        {
            "location": "/Arquitectura_Caso_1/#redes",
            "text": "Existen varias redes utilizadas en este proyecto, que est\u00e1n detalladas en la  Tabla 2 .             Nombre  VLAN  Tipo/Direccionamiento  Descripci\u00f3n    Gesti\u00f3n (OLN_MGMT)  302  Ethernet L2  Red de Gesti\u00f3n para todas las MVs de la maqueta.    Cliente (OLN_CLI)  304  IPv6 2a02:9008:4:b110::/64  Red de Cliente.    Servicio (OLN_SRV)  303  IPv4     10.95.84.0/26  Red de Servicio, usada por los servicios internos del CLOS.       Tabla 2.  Redes de la Prueba de Concepto.",
            "title": "Redes"
        },
        {
            "location": "/Arquitectura_Caso_1/#recursos-de-hardware",
            "text": "Los recursos de Hardware utilizados en esta prueba de concepto est\u00e1n descritos en la  Tabla 3 .           Nombre  Descripci\u00f3n    test-oln-cn-01  Nodo de Computaci\u00f3n que ser\u00e1 utilizado como un hipervisor de OpenNebula, utilizando la tecnolog\u00eda KVM como herramienta de virtualizaci\u00f3n.                   -   CPU: 8x Intel(R) Xeon(R) CPU E5450 @ 3.00GHz\n\n              -   Memoria: 16 GB\n\n              -   Almacenamiento: 236 GB                                                                                                                                  |  | test-oln-hn-01 | Nodo de Computaci\u00f3n que ser\u00e1 utilizado como un hipervisor de OpenNebula, utilizando la tecnolog\u00eda KVM como herramienta de virtualizaci\u00f3n.                                  -   CPU: 8x Intel(R) Xeon(R) CPU E5450 @ 3.00GHz\n\n              -   Memoria: 32 GB\n\n              -   Almacenamiento: 224 GB                                                                                                                                  |  | HGU            | Un Home Gateway Unit que generar\u00e1 y simular\u00e1 el tr\u00e1fico del Cliente.                                                                                                       Estar\u00e7a conectado a la maqueta a trav\u00e9s de la red del Cliente y del componente vOLT                                                                         |  | Soporte de Red | La prueba de concepto tiene lugar en el seno de una red que particiona las VLANs 302, 303 y 304, y que a su vez provee enrutado para el acceso a internet. |  Tabla 3.  PoC Hardware Resources",
            "title": "Recursos de Hardware"
        },
        {
            "location": "/Arquitectura_Caso_1/#opennebula",
            "text": "",
            "title": "OpenNebula"
        },
        {
            "location": "/Arquitectura_Caso_1/#diseno",
            "text": "En la  Tabla 4  se muestra un resumen de la configuraci\u00f3n del despliegue de OpenNebula.           Sistema Operativo  Ubuntu 14.04 tanto en el Front-end como en los nodos de computaci\u00f3n.    Hipervisor  2 nodos de computaci\u00f3n usando KVM.    Base de Datos  MySQL    Redes  Bridges de Linux y tecnolog\u00eda 802.1Q para la implementaci\u00f3n de la segmentaci\u00f3n por VLANs.    Almacenamiento  Almacenamiento local del Hipervisor. ~ 230 GB.    Autenticaci\u00f3n  Nativo de OpenNebula    Interfaces  OneFlow, CLI, Sunstone     \nTable 4.  PoC Hardware Resources  Tanto la MV de OpenNebula como los nodos f\u00edsicos de Computaci\u00f3n han sido provisionados usando Ansible. El playbook est\u00e1 disponible aqu\u00ed:  https://github.com/Telefonica/ctpd/tree/master/ansible . Esto permitir\u00e1 una reinstalaci\u00f3n uniforme e id\u00e9ntica en el futuro. Dado que este playbook contiene los detalles exactos de la provisi\u00f3n, para m\u00e1s detalles revisar este recurso.",
            "title": "Dise\u00f1o"
        },
        {
            "location": "/Arquitectura_Caso_1/#front-end-de-opennebula",
            "text": "OpenNebula ha sido desplegada en una M\u00e1quina Virtual que est\u00e1 siendo ejecutada en el nodo  test-oln-cn-01 . Ha sido desplegada manualmente y configurada para exponer los siguientes servicios:    OpenNebula 4.14.2 (oned)    Planificador (mm_sched)    OneFlow (oneflow-server)    Sunstone (sunstone-server)    L\u00ednea de Comandos de Linux    Actualmente solo se ha desplegado un \u00fanico Front-end, por lo que no hay Alta Disponibilidad. Sin embargo el entorno est\u00e1 preparado para configurarse en modo de Alta Disponibilidad tras desplegar un segundo nodo de Front-end.",
            "title": "Front-end de OpenNebula"
        },
        {
            "location": "/Arquitectura_Caso_1/#nodos-de-computacion",
            "text": "Los nodos de computaci\u00f3n son responsables de proveer a las M\u00e1quinas Virtuales con los recursos necesario (por ejemplo CPU, Memoria, acceso a la red). OnLife es homog\u00e9nea en cuanto a la tecnolog\u00eda utilizada para virtualizar, manejando 2 nodos con el hipervisor KVM. La configuraci\u00f3n de los nodos tambi\u00e9n es homog\u00e9nea en t\u00e9rmino de componentes de software instalados.  Para los nodos de computaci\u00f3n se han instalado el siguiente software:    Libvirt y qemu-kvm    Paquete de nodo de OpenNebula 4.14.2 KVM    El usuario  oneadmin  tiene acceso de administraci\u00f3n al almacenamiento, redes y virtualizaci\u00f3n.    Conexi\u00f3n SSH sin password entre los nodos y el Front-end.    Ruby instalado (utilizado por las sondas y drivers de OpenNebula)",
            "title": "Nodos de Computaci\u00f3n"
        },
        {
            "location": "/Arquitectura_Caso_1/#almacenamiento",
            "text": "OnLife utilizar\u00e1 el almacenamiento local disponible en los nodos, usando el driver SSH. Esto significa que no se ha implementado ninguna configuraci\u00f3n espec\u00edfica para el almacenamiento.",
            "title": "Almacenamiento"
        },
        {
            "location": "/Arquitectura_Caso_1/#redes_1",
            "text": "Todav\u00eda por determinar.",
            "title": "Redes"
        },
        {
            "location": "/Arquitectura_Caso_1/#autenticacion",
            "text": "Los usuarios se mantendr\u00e1n en la base de datos de OpenNebula, como usuarios nativos, y la autenticaci\u00f3n se realizar\u00e1 mediante usuario/password.",
            "title": "Autenticaci\u00f3n"
        },
        {
            "location": "/Arquitectura_Caso_1/#configuracion-del-front-end",
            "text": "OpenNebula se ha instalado y configurado para ejecutar los servicios b\u00e1sicos de OpenNebula:  oned , el planificador  mm_sched ,  OneFlow  y  Sunstone ).  El sistema operativo es Ubuntu 14.04 obtenido directamente de los repositorios de Ubuntu.  En la  Tabla 5  se detallan las modificaciones a los ficheros de configuraci\u00f3n pertenecientes a OpenNebula.           Fichero de Configuraci\u00f3n  Descripci\u00f3n    oned.conf  # Sample configuration for MySQL                                 DB = \\[ backend = \"mysql\",\n\n                            server = \"localhost\",\n\n                            port = 0,\n\n                            user = \"oneadmin\",\n\n                            passwd = \"opennebula\",\n\n                            db\\_name = \"opennebula\" \\]         |  | sunstone-server.conf         | :host: 0.0.0.0                    |  \nTabla 5.  Resumen de los cambios de configuraci\u00f3n de OpenNebula (relativos a /etc/one)",
            "title": "Configuraci\u00f3n del Front-end"
        },
        {
            "location": "/Arquitectura_Caso_1/#configuracion-de-los-nodos-de-computacion",
            "text": "Los nodos de computaci\u00f3n han sido provisionados siguiendo este procedimiento:    Habilitar el repositorio de OpenNebula.    Instalaci\u00f3n del paquete  opennebula-node .    Los ficheros de configuraci\u00f3n alterados durante la instalaci\u00f3n est\u00e1n descritos en la  Tabla 6.           Fichero de Configuraci\u00f3n  Descripci\u00f3n    /etc/apparmor.d/abstractions/libvirt-qemu  /srv/** rw,                                              /var/lib/one/datastores/\\*\\* rw  |  \nTabla 6.  Resumen de los cambios en los ficheros de configuraci\u00f3n.",
            "title": "Configuraci\u00f3n de los Nodos de Computaci\u00f3n"
        },
        {
            "location": "/Arquitectura_Caso_1/#imagenes-y-templates",
            "text": "Se har\u00e1 disponible una \u00fanica imagen en la Nube de OpenNebula: una Ubuntu 14.04. Esta imagen ser\u00e1 utilizada como Sistema Operativo para todas las M\u00e1quinas Virtuales y servicios que se utilizar\u00e1n en esta maqueta: vOLT, ONOS, Portal Cautivo, etc.  Cuando se despliegue un nuevo servicio basado en una M\u00e1quina Virtual, la imagen de base deber\u00e1 ser clonada y convertida en persistente. A su vez se deber\u00e1 de crear un nuevo template referenciando esa imagen. Opcionalmente las IPs se podr\u00e1n listar de manera espec\u00edfica en el template de forma que un mismo servicio siempre tenga la misma IP.",
            "title": "Im\u00e1genes y Templates"
        },
        {
            "location": "/Arquitectura_Caso_1/#nodos-clusters-vdcs",
            "text": "Los dos nodos de KVM son homog\u00e9neos. No se crear\u00e1n clusters y todos los recursos ser\u00e1n incluidos en el VDC por defecto. Los dos nodos registrados en la nube no disponen de la misma capacidad, sin embargo OpenNebula monitorizar\u00e1 la capacidad disponible y escoger\u00e1 d\u00f3nde desplegar cada M\u00e1quina Virtual.  El nodo  test-oln-hn-01  es el \u00fanico conectado a la red  OLN_CLI,  por lo que la M\u00e1quina Virtual de vOLT deber\u00e1 ser ejecutada en este nodo espec\u00edficamente.",
            "title": "Nodos / Clusters / VDCs"
        },
        {
            "location": "/Arquitectura_Caso_1/#almacenamiento-y-datastores",
            "text": "El storage por defecto ser\u00e1 local en los nodos de Computaci\u00f3n. Por lo tanto, tanto el Datastore de Sistema como el de Im\u00e1genes deber\u00e1n utilizar TM_MAD=\"ssh\". Las im\u00e1genes ser\u00e1n almacenadas en formato Qcow2.",
            "title": "Almacenamiento y Datastores"
        },
        {
            "location": "/Arquitectura_Caso_1/#redes-virtuales",
            "text": "Por determinar.",
            "title": "Redes Virtuales."
        },
        {
            "location": "/Arquitectura_Caso_1/#usuarios-y-grupos",
            "text": "Existir\u00e1 un \u00fanico usuario:  oneadmin , al cual le pertenecer\u00e1n todos los recursos.",
            "title": "Usuarios y Grupos"
        },
        {
            "location": "/Arquitectura_Caso_1/#onos-telcaria",
            "text": "",
            "title": "ONOS (Telcaria)"
        },
        {
            "location": "/Arquitectura_Caso_1/#sistema-de-supervision-y-gestion",
            "text": "Este servicio debe ser capaz de supervisar, y si es necesario, analizar la marcha de distintos componentes de la CTpd en la prestaci\u00f3n de los servicios encomendados. Los principales objetivos y requisitos de este sistema son    Ser una plataforma gen\u00e9rica para el an\u00e1lisis    Debe ser escalable y soportar multitenencia    Debe ser posible introducir instrumentos o sondas en los servicios m\u00e1s all\u00e1 de los dispositivos de c\u00f3mputo y conmutaci\u00f3n    Debe ser posible ajustar el nivel de sondeo en los dispositivos subyacentes    Debe ser posible agrupar la informaci\u00f3n de las sondas    Debe ser posible redireccionar los flujos de tr\u00e1fico a trav\u00e9s de una \u201csonda virtual\u201d para obtener una inspecci\u00f3n m\u00e1s profunda que no es posible obtener de los dispositivos subyacentes    Como se muestra en la siguiente ilustraci\u00f3n, el sistema de supervisi\u00f3n consigue la informaci\u00f3n de las sondas desde distintos elementos de red en la CTpd, incluyendo nodos de c\u00f3mputo, conmutadores, dispositivos de acceso a central y servicios programables ejecut\u00e1ndose en los nodos de c\u00f3mputo; poni\u00e9ndola a disposici\u00f3n de otras aplicaciones de an\u00e1lisis ejecut\u00e1ndose en la CTpd. El sistema se integrar\u00e1 con los servicios residenciales como el vSG, el ErC, la vOLT, aunque la arquitectura permite extenderlo a futuras plataformas de la CTpd como el acceso m\u00f3vil o empresarial.",
            "title": "Sistema de supervisi\u00f3n y gesti\u00f3n"
        },
        {
            "location": "/Arquitectura_Caso_1/#ensamblaje-del-bastidor",
            "text": "Estas son las instrucciones esquematizadas para armar el bastidor, en los repositorios  github  referenciados se encuentran las instrucciones detalladas. El bastidor de la prueba ya vendr\u00e1 con la infraestructura ensamblada de origen y los programas precargados en la planta de ensamblaje del suministrador.  Infraestructura  Los servidores, conmutadores y bandejas de acceso se pueden ensamblar en varias configuraciones virtuales como se ilustra a seguir   Describir circuito de control y administraci\u00f3n basado en el OCP/OCS de Microsoft  Programas  La instalaci\u00f3n de los programas que conforman el sistema se har\u00e1 en el orden descrito a continuaci\u00f3n    OpenNebula    ONOS-CTpd    ONOS-Tejido    OpenMano + OneFlow    Instalaci\u00f3n de OpenNebula  Las instrucciones para instalar OpenNebula se encuentran en esta direcci\u00f3n IP  http://docs.opennebula.org/4.14/design_and_installation/building_your_cloud/ignc.html  y se puede descargar desde este sitio  http://downloads.opennebula.org/packages/opennebula-4.14.2/  La instalaci\u00f3n ser\u00eda la siguiente:    # wget -q -O-  http://downloads.opennebula.org/repo/Ubuntu/repo.key  | apt-key add -    # echo \"deb  http://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/  stable opennebula\" \\\u00a0\u00a0 > /etc/apt/sources.list.d/opennebula.list    # apt-get update    # apt-get install opennebula opennebula-sunstone nfs-kernel-server    Cambiar en el Fichero\u00a0/etc/one/sunstone-server.conf\u00a0 \u201c:host: 127.0.0.1\u201d por \u00a0\u201c:host: 0.0.0.0\u201d    # /etc/init.d/opennebula-sunstone restart    A\u00f1adir al Fichero vi /etc/exports file \u00a0\u201c/var/lib/one/ *(rw,sync,no_subtree_check,root_squash)\u201d    # service nfs-kernel-server restart    # su - oneadmin    $ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys    $ cat << EOT > ~/.ssh/config    Host *    StrictHostKeyChecking no    UserKnownHostsFile /dev/null    EOT    $ chmod 600 ~/.ssh/config    # wget -q -O-  http://downloads.opennebula.org/repo/Ubuntu/repo.key  | apt-key add -    # echo \"deb  http://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/  stable opennebula\" > \\\u00a0\u00a0\u00a0\u00a0 /etc/apt/sources.list.d/opennebula.list    # apt-get update    # apt-get install opennebula-node nfs-common bridge-utils    Configurar fichero red: /etc/network/interfaces    auto lo    iface lo inet loopback    auto br0    iface br0 inet dhcp    bridge_ports eth0    bridge_fd 9    bridge_hello 2    bridge_maxage 12    bridge_stp off    # /etc/init.d/networking restart    # cat << EOT > /etc/libvirt/qemu.conf    user\u00a0 = \"oneadmin\"    group = \"oneadmin\"    dynamic_ownership = 0    EOT    # service libvirt-bin restart    Con esta ejecuci\u00f3n se puede acceder al portal de OpenNebula en la URL:\u00a0  http://IP:9869 . La password de oneadmin est\u00e1 en el fichero \u00a0~/.one/one_auth.  Instalaci\u00f3n de ONOS  Las instrucciones para instalar ONOS se encuentran en esta direcci\u00f3n IP, empezando en el punto 2. de las mismas  https://wiki.onosproject.org/display/ONOS/ONOS+from+Scratch  Pero las instrucciones se modificaron de esta manera:  Instala  Git :  $sudo apt-get install git-core  Descarga  Karaf  y  Maven :  Create two directories called\u00a0~/Downloads\u00a0and\u00a0~/Applications. Download the\u00a0 Karaf 3.0.5 \u00a0and\u00a0 Maven 3.3.9 \u00a0binaries (the\u00a0tar.gz\u00a0versions of both) into\u00a0~/Downloads\u00a0and extract it to\u00a0~/Applications. Keep the tar archives in\u00a0~/Downloads; we'll need that later.  | Si decides utilizar directorios distintos, como se puede ver a continuaci\u00f3n, entonces tendr\u00e1s que editar el fichero ~/onos/tools/dev/bash_profile (en el caso de la raspberry pi es /home/pi/onos/tools/dev/bash_profile) y cambiar la configuraci\u00f3n all\u00ed.   $ mkdir Downloads                                                                                                                                                                                                                                               $ mkdir /opt/Apps                                                                                                                                                                                                                                               $ cd Downloads                                                                                                                                                                                                                                                  $ wget  http://archive.apache.org/dist/karaf/3.0.5/apache-karaf-3.0.5.tar.gz                                                                                                            $ wget  http://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz                                                                  $ tar -zxvf apache-karaf-3.0.5.tar.gz -C /opt/Apps/                                                                                                                                                                                                                $ tar -zxvf apache-maven-3.3.9-bin.tar.gz -C /opt/Apps/        Instala  java8 :  $ sudo apt-get install software-properties-common -y  $ sudo add-apt-repository ppa:webupd8team/java -y  $ sudo apt-get update  $ sudo apt-get install oracle-java8-installer oracle-java8-set-default -y  En el caso de que no funcionara, descarga manualmente java ( http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html )  E inst\u00e1lalo:  $sudo update-alternatives --install \"/usr/bin/java\" \"java\" \"/opt/Apps/jdk1.8.0_77/bin/java\" 1  $sudo update-alternatives --install \"/usr/bin/javac\" \"javac\" \"/opt/Apps/jdk1.8.0_77/bin/javac\" 1  $sudo update-alternatives --set java /opt/Apps/jdk1.8.0_77/bin/java  $sudo update-alternatives --set javac /opt/Apps/jdk1.8.0_77/bin/javac  M\u00e1s info en:  http://www.rpiblog.com/2014/03/installing-oracle-jdk-8-on-raspberry-pi.html  Descarga ONOS:  $git clone  https://gerrit.onosproject.org/onos  A partir de ahora suponemos que ONOS est\u00e1 instalado en el home del usuario, e.g. /home/pi/onos  Incluye las siguientes lineas en el fichero  .bashrc :  . ~/onos/tools/dev/bash_profile  export ONOS_ROOT=~/onos  source $ONOS_ROOT/tools/dev/bash_profile  Ejecuta el  bash  para que se carguen:  $bash  Comprueba que se han cargado correctamente:  $ echo $ONOS_ROOT  /home/pi/onos  $ echo $KARAF_ROOT  /opt/Apps/apache-karaf-3.0.5  Configura la instalaci\u00f3n:  Edita el fichero de configuraci\u00f3n:  $sudo nano /opt/Apps/apache-karaf-3.0.5/etc/org.apache.karaf.features.cfg  Incluye el siguiente texto en la secci\u00f3n de  featuresRepositories:  mvn:org.onosproject/onos-features/1.5.0-SNAPSHOT/xml/features  Compila con  maven  ONOS:  $cd ~/onos  $ mvn clean install\u00a0 # or use the alias 'mci'  Si todo ha ido bien deber\u00eda compilarse entero, si tienes problemas, aqu\u00ed hay posibles soluciones:    Actualizar java a la \u00faltima versi\u00f3n disponible    No hay forma de compilar onos-incubator-rpc-grpc, por lo que la \u00fanica opci\u00f3n es no hacerlo, para ello basta con comentar la siguiente l\u00ednea en el fichero ~/onos/incubator/pom.xml     <!--  <module>rpc-grpc</module>  -->   Ahora hay que construir ONOS:  https://wiki.onosproject.org/display/ONOS/Running+ONOS+locally+on+development+machine  $cd /home/pi/onos/tools/build  $onos-build  Ejecutar ONOS:  $onos-karaf  Para acceder v\u00eda web:  http://192.168.0.200:8181/onos/ui/login.html#/topo  karaf/karaf  onos/rocks  Antes de que se me olvide, al intentar instalar mininet dice esto (hay que comprobar que funciona para CentOS):  Install.sh currently only supports Ubuntu, Debian, RedHat and Fedora.  Los entregables definidos a la fecha son",
            "title": "Ensamblaje del bastidor"
        },
        {
            "location": "/Arquitectura_Caso_1/#caso-2-encadenamiento-de-servicios",
            "text": "Esta prueba tiene como objetivo:    Conectar un servicio de conectividad privada (MPLS, principalmente) con un elemento de computaci\u00f3n en la nube que permita activar funciones de red dedicadas al cliente.    Verificar la capacidad de orquestaci\u00f3n global del entorno desde el BSS hasta la configuraci\u00f3n del servicio de manera autom\u00e1tica.    Verificar las necesidades operativas que se derivan de este nuevo tipo de servicios    Analizar la estructura de costes de estos servicios y proponer soluciones eficientes en costes.    Algunas conclusiones:    La automatizaci\u00f3n debe incluir tambi\u00e9n una parte, aunque sea controlada de configuraci\u00f3n autom\u00e1tica de la red.    Se requieren nuevos conocimientos tecnol\u00f3gicos que compaginen conocimientos de routing, programaci\u00f3n y sistemas de gesti\u00f3n, perfiles que a\u00fan no est\u00e1n desarrollados en las empresas como tales.    Se requiere un alto volumen de negocio para justificar las inversiones globales. Se deben probar soluciones de bajo coste (open source) que permitan reducir costes.     Ilustraci\u00f3n 14 Esquema del caso 2 - encadenamiento de servicios   Ilustraci\u00f3n 15 Diagrama arquitect\u00f3nico de los servicios residenciales del caso 2   Ilustraci\u00f3n 16 Arquitectura del sistema de administraci\u00f3n y gesti\u00f3n  Los beneficios a obtener en este caso   Los entregables planificados son",
            "title": "Caso 2: Encadenamiento de servicios"
        },
        {
            "location": "/Arquitectura_Caso_1/#caso-3-sdn-a-ip-sdn-ip-interconexion-de-redes-ciberdefinidas-con-redes-ip-legadas",
            "text": "Esta prueba tiene como objetivo:    Proporcionar conectividad nivel 3 sin utilizar enrutadores legados en el n\u00facleo de red    Transformar con OpenFlow los Sistemas Aut\u00f3nomos (AS) en redes de tr\u00e1nsito IP BGP    Conectar redes ciberdefinidas con redes IP legadas utilizando BGP    Probar un mecanismo para la migraci\u00f3n progresiva de nuestras redes IP a redes ciberdefinidas    Demostrar que podemos sustituir en el n\u00facleo de la red los enrutadores por conmutadores    Agregar en confederaciones BGP distintos dominios administrativos ciberdefinidos, haciendo el plano de control m\u00e1s escalable    Actualmente ONOS se ha desplegado globalmente en varias redes de Am\u00e9rica y Europa, como una soluci\u00f3n de conectividad en nivel 3 para los usuarios, puramente en OpenFlow v\u00eda BGP, sin la necesidad de enrutadores en el n\u00facleo.   Ilustraci\u00f3n 17 Despliegue actual de ONOS/SDN-IP en el mundo   Ilustraci\u00f3n 18 Despliegue de ONOS SDN enlazando 3 continentes  El primer despliegue sobre una red en explotaci\u00f3n se realiz\u00f3 en EE. UU. en la red Internet2.     Ilustraci\u00f3n 19 Mapa geogr\u00e1fico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S  La arquitectura utilizada se muestra en la siguiente ilustraci\u00f3n, en nuestro caso queda por definirla para esta prueba, intentando que sea algo muy parecido.     Ilustraci\u00f3n 20 Diagrama arquitect\u00f3nico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S  ONOS se ha desplegado al resto de Am\u00e9rica aprovechado la RedCLARA universitaria y de investigaci\u00f3n y se ha extendido a Europa en la red G\u00c9ANT     Ilustraci\u00f3n 21 Mapa geogr\u00e1fico del despliegue de ONOS / SDN-IP en la red FIU/RedClara  La arquitectura que es similar a la utilizada en Internet2 se muestra en la siguiente ilustraci\u00f3n   q   Ilustraci\u00f3n 22 Diagrama arquitect\u00f3nico del despliegue de ONOS / SDN-IP en la red FIU/RedClara   Ilustraci\u00f3n 23 Despliegue de ONOS en Europa  En el despliegue europeo se ha utilizado una aplicaci\u00f3n desarrollada para ONOS, llamada ICONA y que permite gestionar de manera eficiente la intercomunicaci\u00f3n de clusters de ONOS geogr\u00e1ficamente distribuidos.",
            "title": "Caso 3: SDN a IP (SDN-IP) interconexi\u00f3n de redes ciberdefinidas con redes IP legadas"
        },
        {
            "location": "/Arquitectura_Caso_1/#caso-4-servicios-remotos-para-empresas",
            "text": "Este caso de uso se defini\u00f3 conjuntamente con el proyecto Atrium de la Open Networking Foundation con la finalidad de incorporar en ONOS funcionalidad suficiente para permitir realizar \u201crouter peering\u201d con soluciones abiertas y de marca blanca.   Ilustraci\u00f3n 24 Diagrama arquitect\u00f3nico de servicios remotos para empresas   Ilustraci\u00f3n 25 Caso de uso de integraci\u00f3n con Atrium \"peering router\"  Este a\u00f1o se realiz\u00f3 la prueba del caso de uso \u201crouter peering\u201d con enrutadores localizados en Australia y EE.UU.   Ilustraci\u00f3n 26 Prueba del caso de uso \"router peering\"",
            "title": "Caso 4: Servicios remotos para empresas"
        },
        {
            "location": "/Arquitectura_Caso_1/#caso-4-gestion-y-programabilidad-de-las-redes-ip-opticas",
            "text": "Este caso de uso patrocinado por AT&T puede resumirse como una implementaci\u00f3n de paquetes \u00f3pticos ciberconectados SDN para redes. Incluido en este caso de uso hay aspectos como abstracciones de red multicapa en SDN, una aplicaci\u00f3n, calendario, de ancho de banda bajo demanda de paquetes \u00f3pticos y refinamientos de la interfaz de usuario ONOS para orquestaci\u00f3n, configuraci\u00f3n de capa de datos y visualizaci\u00f3n de las redes flexibles y el\u00e1sticas de transporte \u00f3pticas.     Ilustraci\u00f3n 27 Caso de uso programabilidad red IP sobre \u00d3ptico   Ilustraci\u00f3n 28 Topolog\u00eda del caso de uso",
            "title": "Caso 4: Gesti\u00f3n y programabilidad de las redes IP \u00f3pticas"
        },
        {
            "location": "/Arquitectura_Caso_1/#sistema-de-administracion-y-gestion",
            "text": "El sistema para la administraci\u00f3n y gesti\u00f3n de la virtualizaci\u00f3n y ciberconectividad, as\u00ed como su integraci\u00f3n con los otros sistemas de gesti\u00f3n de la empresa, es clave para el \u00e9xito de estas soluciones, y a la vez est\u00e1 probando ser el punto de inflexi\u00f3n en las distintas integraciones que est\u00e1n intentando vender distintos fabricantes.  Para mantener cierta independencia de los fabricantes tradicionales, vamos a utilizar componentes desarrollados en c\u00f3digo abierto por comunidades de usuarios y no tanto por fabricantes tradicionales, esto nos permitir\u00e1 conjuntar un sistema de administraci\u00f3n m\u00e1s acorde con nuestras necesidades globales, pero que al mismo tiempo nos permita reutilizar infraestructura que ya est\u00e1 en producci\u00f3n. Para ello hemos seleccionado OpenMano con OpenNebula, ONOS y queda pendiente verificar la necesidad del componente XOS, soluciones estas muy ligeras de c\u00f3digo pero con las prestaciones que necesita una operadora de telecomunicaciones, a diferencia de soluciones m\u00e1s comerciales como OpenStack, OpenDaylight u otras.   Ilustraci\u00f3n 29 Sistema de administraci\u00f3n y gesti\u00f3n   Los componentes del sistema de gesti\u00f3n y administraci\u00f3n a utilizar son:     OpenMano  \u2013 orquestador de las funciones de red virtualizadas    OpenNebula  \u2013 gestor y orquestador de nubes que funciona sobre infraestructura heterog\u00e9nea y de forma nativa resuelve la creaci\u00f3n, gesti\u00f3n y administraci\u00f3n de CPDs virtuales geogr\u00e1ficamente dispersos    ONOS  \u2013 sistema operativo de la red desarrollado por ON.Lab, versi\u00f3n Cardinal o Drake",
            "title": "Sistema de administraci\u00f3n y gesti\u00f3n"
        },
        {
            "location": "/Arquitectura_Caso_1/#integracion",
            "text": "El sistema debe ser capaz de administrar una topolog\u00eda nube como la ilustrada   Ilustraci\u00f3n 30 Entorno de administraci\u00f3n y gesti\u00f3n de red  Teniendo en cuenta que los emplazamientos pueden estar dispersos en una amplia zona geogr\u00e1fica de un pa\u00eds o pueden prestar servicios de una operadora a otra en el mismo o distintos continentes.",
            "title": "Integraci\u00f3n"
        },
        {
            "location": "/Arquitectura_Caso_1/#definicion-y-dimensionamiento-de-componentes",
            "text": "Los principales componentes y su dimensionamiento a utilizar\u2026    Infraestructura    Servidores:  OCP    Almacenamiento:  OCP + CEPH    Conmutadores:  OCP Accton    CPE: cualquiera    OLT: OCP PMC-Sierra      SO   Computaci\u00f3n:  Linux     Hipervisor:  KVM    Gestor de la nube y servicios:  OpenNebula    S.O. de la red:  ONOS    Aplicaciones:  vCPE NEC, OpenWRT , etc.",
            "title": "Definici\u00f3n y dimensionamiento de componentes"
        },
        {
            "location": "/Arquitectura_Caso_1/#cronograma-de-actividades",
            "text": "El cronograma de actividades se resume en la siguiente tabla   Ilustraci\u00f3n 31 Cronograma",
            "title": "Cronograma de actividades"
        },
        {
            "location": "/Arquitectura_Caso_1/#evaluacion-de-nuestra-capacidad-para-disenar-desarrollar-desplegar-y-operar-las-tecnologias-de-red-y-sistemas",
            "text": "En las varias implantaciones de estas tecnolog\u00edas en operadoras de nuestra envergadura, como ATT, DT, NTT y Verizon, ha quedado demostrado que el conocimiento interno ha sido un factor determinante para conseguir resultados significativos que les han permitido avanzar en implantar redes virtualizadas. En esta prueba vamos a determinar el tipo de conocimiento interno que debemos adquirir para dise\u00f1ar, desarrollar, implantar, operar y mantener esta soluci\u00f3n.",
            "title": "Evaluaci\u00f3n de nuestra capacidad para dise\u00f1ar, desarrollar, desplegar y operar las tecnolog\u00edas de red y sistemas"
        }
    ]
}