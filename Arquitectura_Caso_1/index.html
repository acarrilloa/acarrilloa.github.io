<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">

	<title>Arquitectura - Documentos de la CTpd</title>

        <link href="../css/bootstrap-3.0.3.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">
        <link href="../css/base.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href="..">Documentos de la CTpd</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            
                <li >
                    <a href="..">Inicio</a>
                </li>
            
            
            
                <li class="active">
                    <a href="./">Arquitectura</a>
                </li>
            
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                <li >
                    <a rel="next" href="..">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li class="disabled">
                    <a rel="prev" >
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#prueba-de-concepto-del-onlife-network">prueba de concepto del Onlife Network</a></li>
        
            <li><a href="#acondicionar-la-ct-como-un-centro-de-servicios-programables">Acondicionar la C.T. como un centro de servicios programables</a></li>
        
            <li><a href="#requisitos-y-beneficios">Requisitos y beneficios</a></li>
        
            <li><a href="#decisiones-de-diseno-y-tecnologicas">Decisiones de diseño y tecnológicas</a></li>
        
            <li><a href="#construccion-de-la-prueba">Construcción de la prueba</a></li>
        
            <li><a href="#opennebula">OpenNebula</a></li>
        
            <li><a href="#onos-telcaria">ONOS (Telcaria)</a></li>
        
            <li><a href="#sistema-de-supervision-y-gestion">Sistema de supervisión y gestión</a></li>
        
            <li><a href="#caso-2-encadenamiento-de-servicios">Caso 2: Encadenamiento de servicios</a></li>
        
            <li><a href="#caso-3-sdn-a-ip-sdn-ip-interconexion-de-redes-ciberdefinidas-con-redes-ip-legadas">Caso 3: SDN a IP (SDN-IP) interconexión de redes ciberdefinidas con redes IP legadas</a></li>
        
            <li><a href="#caso-4-servicios-remotos-para-empresas">Caso 4: Servicios remotos para empresas</a></li>
        
            <li><a href="#sistema-de-administracion-y-gestion">Sistema de administración y gestión</a></li>
        
            <li><a href="#definicion-y-dimensionamiento-de-componentes">Definición y dimensionamiento de componentes</a></li>
        
            <li><a href="#cronograma-de-actividades">Cronograma de actividades</a></li>
        
            <li><a href="#evaluacion-de-nuestra-capacidad-para-disenar-desarrollar-desplegar-y-operar-las-tecnologias-de-red-y-sistemas">Evaluación de nuestra capacidad para diseñar, desarrollar, desplegar y operar las tecnologías de red y sistemas</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<p><strong>Tabla de ilustraciones</strong></p>
<p><a href="#_Toc449436414"><em>Ilustración 1 Arquitectura objetivo de la red programable</em> 3</a></p>
<p><a href="#_Toc449436415"><em>Ilustración 2 Simplificación de protocolos de la red de servicios programables</em> 4</a></p>
<p><a href="#_Toc449436416"><em>Ilustración 3 Diagrama arquitectónico de los servicios residenciales del caso 1</em> 5</a></p>
<p><a href="#_Toc449436417"><em>Ilustración 4 Arquitectura del sistema de administración y gestión</em> 6</a></p>
<p><a href="#_Toc449436418"><em>Ilustración 5 Esquema del caso 1</em> 6</a></p>
<p><a href="#_Toc449436419"><em>Ilustración 6 Descomposición del terminal óptico de la C.T.</em> 13</a></p>
<p><a href="#_Toc449436420"><em>Ilustración 7 Circuito de control del enrutador</em> 15</a></p>
<p><a href="#_Toc449436421"><em>Ilustración 8 Adaptar el controlador del tejido a IPv6</em> 16</a></p>
<p><a href="#_Toc449436422"><em>Ilustración 9 Esquema del caso 2 - encadenamiento de servicios</em> 23</a></p>
<p><a href="#_Toc449436423"><em>Ilustración 10 Diagrama arquitectónico de los servicios residenciales del caso 2</em> 23</a></p>
<p><a href="#_Toc449436424"><em>Ilustración 11 Arquitectura del sistema de administración y gestión</em> 24</a></p>
<p><a href="#_Toc449436425"><em>Ilustración 12 Despliegue actual de ONOS/SDN-IP en el mundo</em> 26</a></p>
<p><a href="#_Toc449436426"><em>Ilustración 13 Despliegue de ONOS SDN enlazando 3 continentes</em> 26</a></p>
<p><a href="#_Toc449436427"><em>Ilustración 14 Mapa geográfico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S</em> 26</a></p>
<p><a href="#_Toc449436428"><em>Ilustración 15 Diagrama arquitectónico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S</em> 27</a></p>
<p><a href="#_Toc449436429"><em>Ilustración 16 Mapa geográfico del despliegue de ONOS / SDN-IP en la red FIU/RedClara</em> 27</a></p>
<p><a href="#_Toc449436430"><em>Ilustración 17 Diagrama arquitectónico del despliegue de ONOS / SDN-IP en la red FIU/RedClara</em> 28</a></p>
<p><a href="#_Toc449436431"><em>Ilustración 18 Despliegue de ONOS en Europa</em> 28</a></p>
<p><a href="#_Toc449436432"><em>Ilustración 19 Diagrama arquitectónico de servicios remotos para empresas</em> 29</a></p>
<p><a href="#_Toc449436433"><em>Ilustración 20 Caso de uso de integración con Atrium "peering router"</em> 29</a></p>
<p><a href="#_Toc449436434"><em>Ilustración 21 Prueba del caso de uso "router peering"</em> 30</a></p>
<p><a href="#_Toc449436435"><em>Ilustración 22 Caso de uso programabilidad red IP sobre Óptico</em> 31</a></p>
<p><a href="#_Toc449436436"><em>Ilustración 23 Topología del caso de uso</em> 31</a></p>
<p><a href="#_Toc449436437"><em>Ilustración 24 Sistema de administración y gestión</em> 32</a></p>
<p><a href="#_Toc449436438"><em>Ilustración 25 Entorno de administración y gestión de red</em> 32</a></p>
<p><a href="#_Toc449436439"><em>Ilustración 26 Cronograma</em> 34</a></p>
<table>
<thead>
<tr>
<th><strong>Versiones</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Versión número</strong></td>
</tr>
<tr>
<td>V0.9</td>
</tr>
<tr>
<td>V0.5</td>
</tr>
<tr>
<td>V0.4</td>
</tr>
</tbody>
</table>
<h1 id="prueba-de-concepto-del-onlife-network">prueba de concepto del Onlife Network</h1>
<p>La idea de estas pruebas se fundamenta en una arquitectura de red de servicios programables, con casos de uso de servicios extremo a extremo y no en la utilización de componentes aislados de la red o los sistemas; de esta manera nos aseguramos que cumplimos con las expectativas de las distintas áreas usuarias de nuestros recursos tecnológicos.</p>
<p>Ilustración 1 Arquitectura objetivo de la red programable</p>
<p>La motivación es realizar una prueba de concepto que nos permita validar las siguientes hipótesis,</p>
<ul>
<li>
<p>Casos de uso de servicios extremo a extremo</p>
</li>
<li>
<p>Utilización de componentes físicos marca blanca</p>
</li>
<li>
<p>Utilización de desarrollos en código abierto</p>
</li>
<li>
<p>Integración como un servicio programable más las aplicaciones o servicios de terceros, permitiéndoles estar muy próximos al borde de la red y a nosotros ofrecerles servicios de valor añadido</p>
</li>
<li>
<p>Comprobación de la facilidad de integración de distintos y diversos componentes en varias tecnologías</p>
</li>
<li>
<p>Evaluación de nuestra capacidad interna para diseñar, desarrollar, desplegar y operar las tecnologías de red y sistemas</p>
</li>
</ul>
<p>El proyecto OnLife Network se ha planificado en varias fases coincidentes con la metodología establecida en la Innovation Call 2016 que consta de varias fases. En la primera fase nos dedicaremos a comprobar y validar las hipótesis propuestas en un entorno de laboratorio. En la segunda fase se espera realizar pruebas de campo en los entornos de prueba de Telefónica de España.</p>
<p>Ilustración 2 Fases del proyecto OnLife Network</p>
<p>Esto requiere de una simplificación extrema de la red que hemos venido construyendo a lo largo de los años, la siguiente figura ilustra el impacto de esta arquitectura en el entorno de una central telefónica.</p>
<p>Ilustración 3 Simplificación de protocolos de la red de servicios programables</p>
<h2 id="acondicionar-la-ct-como-un-centro-de-servicios-programables">Acondicionar la C.T. como un centro de servicios programables</h2>
<p>Este caso de uso se centra en la provisión de servicios de conectividad residenciales desde una central telefónica utilizando virtualización, ciberconectividad y procesamiento en la nube; teniendo en cuenta que las tres vertientes tecnológicas suman un todo dentro de la central haciéndola un ente autónomo.</p>
<p>El caso es similar a uno patrocinado por AT&amp;T y demostrado exitosamente en el Open Networking Summit 2016, nosotros vamos a modificar ciertos componentes para flexibilizar la solución basados en conceptos similares a los utilizados por Deutsche Telekom en su híper simplificación de la red, con el claro objetivo de hacer despliegue más sencillo y estable.</p>
<p>Utilizando la última versión de ONOS CORD, la solución de código abierto para operadores que está desarrollando el ON.Lab, vamos a facilitar la adecuación a nuestra arquitectura objetivo de los casos de uso y aprovechar en la medida de lo factible el desarrollo que ya está disponible.</p>
<p>Ilustración 4 Diagrama arquitectónico de los servicios residenciales del caso 1</p>
<p>La nebulización de funciones de red (NFaaS) se realizará con una visión de servicio, contrario a la virtualización de elementos aislados e inconexos que están haciendo algunos fabricantes tradicionales. El caso de uso se enfocará en una red de acceso GPON, pero que sin mayor esfuerzo o coste puede migrar a XGS-PON.</p>
<p>A diferencia de la propuesta de arquitectura del ETSI, cuya integración con elementos legados está encontrando demasiadas dificultades, debido en gran parte a la complejidad de su diseño y el no haber tenido en cuenta desde un inicio el ingrediente fundamental de la ciberconectividad; nosotros vamos a resolver el caso de uso en el plano de los servicios y no de sus componentes, utilizando componentes virtuales menos complejos y mejor adecuados a la funcionalidad que exigen la ciberconectividad y la programabilidad de la nueva red en un entorno de nube come se muestra en la siguiente ilustración.</p>
<p>Ilustración 5 Arquitectura del sistema de administración y gestión</p>
<h2 id="requisitos-y-beneficios">Requisitos y beneficios</h2>
<p>Para asegurar su validez, la prueba de campo deberá demostrar los siguientes beneficios</p>
<p>Y al igual que CORD hemos tenido en cuenta cumplir con 5 requisitos básicos:</p>
<p><strong><span style="font-variant:small-caps;">Habilitar servicios innovadores</span></strong></p>
<p>La CTpd como un centro de servicios programables debe habilitar un amplio abanico de servicios, no limitados a servicios de acceso ni debe constreñir sin necesidad la implementación de nuevos servicios. Específicamente la CTpd debe habilitar servicios extraídos de estos vértices:</p>
<ul>
<li>
<p>Ambos servicios, el de acceso y los de nube convencionales</p>
</li>
<li>
<p>Servicios desplegados tanto en el plano de datos (NFV) como servicios implementados en el plano de control</p>
</li>
<li>
<p>Nuestros servicios que son siempre confiables y los no tan confiables de terceros</p>
</li>
</ul>
<p><strong><span style="font-variant:small-caps;">Extensible y controlable</span></strong></p>
<p>La CTpd es una plataforma configurable y no es una solución cerrada, proporciona los medios para que el operador pueda especificar el portafolio de servicios deseados y las dependencias entre esos servicios. Esto permite que la CTpd sea configurable para distintos mercados y redes de acceso: residencial, empresarial y móvil. También debe proporcionar los mecanismos para provisionar y parametrizar estos servicios para atenerse a nuestros objetivos de negocio y operacionales.</p>
<p><strong><span style="font-variant:small-caps;">Eficiencia de la infraestructura Marco Polo</span></strong></p>
<p>CTpd está concebida para utilizar infraestructura marca blanca, apoyándonos en el conocimiento adquirido en el proyecto Marco Polo lo extendemos a los servidores, conmutadores y terminales ópticos de central; con el consiguiente ahorro de costes y probada robustez de estos aparatos especificados por el Open Compute Project. La CTpd debe correr sobre servidores y conmutadores marca blanca, trabajando directamente con los fabricantes de microcircuitos</p>
<p><strong><span style="font-variant:small-caps;">Robustez operativa</span></strong></p>
<p>La CTpd debe tener en cuenta escenarios de fallo parcial e intermedio, por ello ha sido diseñada teniendo en cuenta la posibilidad que el comportamiento en operación del sistema no está siempre sincronizado con el estado deseado del sistema.</p>
<p><strong><span style="font-variant:small-caps;">Seguridad multipropósito</span></strong></p>
<p>La seguridad de la CTpd no debe limitarse a distinguir entre gestores y usuarios del sistema, pero debe ser capaz de asegurar el acceso al sistema de varios actores; como pueden ser operadores globales y locales, desarrolladores de servicios, gestores de servicios y abonados al servicio.</p>
<h2 id="decisiones-de-diseno-y-tecnologicas">Decisiones de diseño y tecnológicas</h2>
<p>Como no es práctico probar todas las variantes de nuestras redes de acceso y segmentos de clientes, esta primera prueba se centra en el acceso G-PON residencial, y el XGS-PON tan pronto esté disponible, pero la arquitectura es la misma que utilizaremos para el acceso de cobre, cable, móvil y también para el segmento empresarial y PyMES.</p>
<p>El diseño es para una red innovadora que ofrezca servicios y micro servicios programables, y que ella misma sea programable; no se contempla compatibilidad con funciones de red existentes para los servicios primarios descritos en los casos de uso, por ello se ha realizado un esfuerzo en utilizar elementos programables de código abierto sobre los cuales vamos a construir la solución más adecuada a nuestras necesidades.</p>
<p>Inspirados en algunos principios de las redes del “nuevo-IP” para esta prueba los aplicamos según se describe en esta tabla.</p>
<table>
<thead>
<tr>
<th><strong>Principios</strong></th>
<th><strong>Aplicados al diseño de la prueba</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Reducir la cantidad de tecnologías utilizadas</strong></td>
<td>Utilizar únicamente IPv6 y transmisión óptica.</td>
</tr>
<tr>
<td><strong>Utilizar una red para todos los servicios – internet, TV, empresas, etc.</strong></td>
<td>Una única red de paquetes convergente</td>
</tr>
<tr>
<td><strong>Dimensionar la red con capacidad para todo el tráfico IP sin pérdidas de paquetes</strong></td>
<td>Uso más eficiente de los recursos de red, dimensionar la red para el tráfico IP en hora punta</td>
</tr>
<tr>
<td><strong>Evitar interfaces internas</strong></td>
<td>Minimizar el número de interfaces internas o de interconexión.</td>
</tr>
<tr>
<td>Distribuir las interconexiones a internet, entregar el tráfico saliente rápidamente</td>
<td></td>
</tr>
<tr>
<td><strong>Gestionada alrededor de dispositivos y su lógica </strong></td>
<td>Gestión centrada en abstracciones de la red a través de programas o aplicaciones, con aplicaciones de código abierto</td>
</tr>
<tr>
<td><strong>La política de servicio de los paquetes van por fuera de la carga</strong></td>
<td>Separar el plano de control del de datos utilizando las cabeceras nativas IPv6 para codificar la información necesaria, como el tipo de servicio, clase de tráfico, dirección, etc.</td>
</tr>
<tr>
<td><strong>Utilizar IPv6 para todas las funciones y servicios internos</strong></td>
<td>En la red no se soportará IPv4 nativo que se convierte en un servicio</td>
</tr>
</tbody>
</table>
<pre><code>                                                                                     El “servicio Ethernet de operadora” basado en IPv6                                                                                                                                   |
</code></pre>
<p><strong>Rutas determinísticas y la más corta para todo el tráfico en red</strong> | Las tramas de red serán gestionadas por ONOS de acuerdo a las plantillas de tráfico establecidas por los planificadores |
<strong>Los CPD están conectados directamente a los CTpd</strong> | Los CPD están directamente conectados a las CTPd para evitar construir interfaces internas adicionales para los grandes flujos de tráfico |</p>
<h3 id="infraestructura-ctpd">Infraestructura CTpd</h3>
<p>La solución se apoya en infraestructura marca blanca especificada bajo los auspicios del Open Compute Project, toda la infraestructura está montada en bastidores Open CloudServer, que permite un despliegue sencillo y robusto en una central telefónica.</p>
<p><strong><span style="font-variant:small-caps;">Dispositivos Marco Polo</span></strong></p>
<p>La solución propuesta consta de 3 elementos que ya han sido especificados por el OCP y están disponibles comercialmente:</p>
<ul>
<li>
<p><strong>Acceso GPON</strong> – terminal óptico en una bandeja con 48 puertos de 2,5 Gbps fabricada por Celestica con el OLT MAC de PMC Sierra</p>
</li>
<li>
<p><strong>Conmutadores</strong> – con 32 puertos de 40 Gbps fabricados por Accton modelo 6712 con ASIC de Broadcom</p>
</li>
<li>
<p><strong>Servidores</strong> – en placas OCS con doble procesador Intel Xeon, 128 GB de memoria y 4 TB de disco, suministrados por AMAX totalmente montados en su bastidor OCS e integrado de fábrica con los otros 2 componentes.</p>
</li>
</ul>
<p>El sistema operativo de los servidores es Linux con Open vSwitch. Los conmutadores se basan en la pila Atrium de ONF, incluyendo Open Network Linux y el agente de OpenFlow Indigo (OF 1.3), y el OpenFlow Data Plane Abstraction (OF-DPA).</p>
<p><strong><span style="font-variant:small-caps;">Configuración y dimensionamiento</span></strong></p>
<p>La plataforma debe tener en consideración la siguiente escala:</p>
<ul>
<li>
<p>800 cabeceras de fibra dispersas en la geografía nacional donde residirá al menos una plataforma CTpd</p>
</li>
<li>
<p>Las cabeceras de fibra concentran entre 6.000 y 117.000 líneas de fibra óptica con un objetivo de ocupación del 50%</p>
</li>
<li>
<p>Cada línea iluminada requerirá varios aparatos virtuales para componer su servicio, podemos estimar entre 5 y 10 máquinas encadenadas por abonado</p>
</li>
</ul>
<p>En este primer bastidor hemos sobredimensionado la capacidad de procesamiento y conmutación para poder estresar en las pruebas un máximo que aproxime tráfico de una central y que luego pueda ser reutilizado en pruebas reales en una central, e incluye.</p>
<ul>
<li>
<p>Acceso GPON para 6.048 líneas, distribuido en 2 bandejas 1U de 48 puertos c/u.</p>
</li>
<li>
<p>Seis conmutadores dispuestos en un tejido CLOS para mover el de oeste a este y permitiéndonos dimensionar horizontalmente la CTpd para ajustarse a las demandas particulares de cada cabecera de fibra. Los 6 conmutadores en esta prueba tienen suficiente para conectar bastantes más servidores y bandejas de acceso que las que estamos conectando en este momento.</p>
</li>
<li>
<p>Ocho servidores con suficiente capacidad para integrar posibles servicios de empresa que queramos integrar de manera independiente y también realizar pruebas con servicios de terceros como Microsoft Office 365, Akamai o Facebook.</p>
</li>
</ul>
<h3 id="programas-sistema-onlife-ct">Programas – Sistema Onlife C.T.</h3>
<p>El sistema de la CTpd se ha formulado en la integración de proyectos en código abierto, pero manteniendo la simplicidad de las soluciones evitando la creación innecesaria de interfaces o capas de gestión que los sistemas seleccionados deberían ya tener solucionados.</p>
<p><strong><span style="font-variant:small-caps;">Núcleo</span></strong></p>
<p>Los componentes seleccionados para esta primera implementación y realizada en base a la documentación disponible y la facilidad con que se puede adecuar las soluciones a nuestras necesidades:</p>
<ul>
<li>
<p><strong>ONOS</strong> – nuevo sistema operativo de la red, diseñado y desarrollado específicamente para operadores de telecomunicaciones. Utilizaremos la versión más reciente de ONOS que se distribuye trimestralmente y es compatible hacia atrás. ONOS se encarga de controlar la conectividad del servicio y el tejido CLOS, instanciar los circuitos que conforman la red programable y aloja los dispositivos virtuales que controlan el plano de datos.</p>
</li>
<li>
<p><strong>OpenNebula</strong> – gestor y controlador de nube que gestiona la infraestructura, las máquinas y dispositivos virtuales, encadenándolas cuando según sea necesario y se encarga de la creación y eliminación dinámica de las instancias necesarias para proveer el servicio, incluyendo el silicón encargado del plano de datos. También gestionará y controlará las aplicaciones virtuales que se generen dentro de contenedores.</p>
</li>
<li>
<p><strong>OneFlow</strong> – encargados del ensamblaje y composición de los servicios, este subsistema realiza 3 funciones clásicas: modelo de datos, sincronizador e interfaz</p>
<ol>
<li>
<p>Modelo de datos – es el estado autoritativo de lo que debería ser el servicio</p>
</li>
<li>
<p>Sincronizador – fuerza a los elementos para que lleguen a ese estado objetivo, sincronizando los conmutadores, servidores y máquinas virtuales</p>
</li>
<li>
<p>Interfaces – proveen las funciones de configuración y controlador</p>
</li>
</ol>
</li>
</ul>
<p>ONOS juega dos papeles en esta implementación, la primera es gestionar el tejido CLOS e implementar los circuitos virtuales que necesita la plataforma; esto se consigue con Forwarding y VTN, que son un par de aplicaciones de ONOS y se accederá a ellas a través de la interfaz OneFlow de OpenNebula. El segundo papel de ONOS es la de proporcionar una plataforma donde alojar los programas del plano de control de los servicios primarios.</p>
<p>OneFlow sirven para controlar la CTpd en su conjunto, y por ende es la interfaz superior de la CTpd. Por ahora empezaremos con una interfaz RESTful y varias interfaces gráficas que ya han sido desarrolladas para la primera prueba de CORD.</p>
<p><strong><span style="font-variant:small-caps;">Inventario de servicios</span></strong></p>
<p>Los servicios primarios incluidos en esta prueba ya han sido desarrollados en el proyecto CORD y es posible que requieran pequeñas modificaciones de nuestro lado para adaptarlas a nuestro entorno, estos programas se comunican con ONOS utilizando la interfaz FlowObjectives.</p>
<ul>
<li>
<p><strong>vOLT</strong> – este aplicativo se ejecuta como un componente de ONOS, y cumple las funciones del plano de control que hemos desagregado de los terminales ópticos monolíticos utilizados s día de hoy</p>
</li>
<li>
<p><strong>v<span style="font-variant:small-caps;">ErC </span></strong>– es el servicio encargado de completar y controlar los circuitos del abonado que le permiten conectarse a la red externa, en CORD este servicio es el vRouter que debemos adaptar a nuestra</p>
</li>
<li>
<p><strong>vPdC</strong> – es la pasarela de cliente que sustituye parte de las funciones realizadas actualmente por el dispositivo del hogar, que serán activadas y ejecutadas desde la C.T.</p>
</li>
<li>
<p><strong>Tejido CLOS</strong> – cumple la función de distribuir y encaminar el tráfico entre la red de acceso, la infraestructura de la central y la red de transporte</p>
</li>
<li>
<p><strong>Aura vCDN</strong> – es un servicio de CDN en el borde desarrollado por Akamai y que su despliegue es dinámico y elástico ajustándose a la demanda del momento en la geografía servida por la CTpd</p>
</li>
</ul>
<h3 id="servicios-todo-como-un-servicio">Servicios: todo como un servicio</h3>
<p>De los conceptos de nebulización que convierten la infraestructura o las plataformas en un servicio y del que Amazon, Google y Microsoft son ahora referentes, utilizaremos en la CTpd la visión de que “todo es un servicio” y lo trataremos tal cual, lo que nos permite ofrecer soluciones que conjuntan la virtualización de funciones de red, la ciberconectividad y los recursos compartidos en un todo al que abstraemos en términos de servicio.</p>
<p>Como los servicios a los que estamos acostumbrados a provisionar en nuestras redes, estos servicios OnLife debemos crearlos y componerlos en productos para ofrecerlos a nuestros clientes; estos nuevos servicios conllevan un par de retos, el primero, los recursos en los que se apoyan pasan de ser infraestructura monolítica a funcionar en infraestructura Marco Polo, y segundo, debemos amalgamar la nueva infraestructura con los nuevos componentes lógicos para poder estar a la altura de Amazon, Google o Microsoft a la hora de ofrecer nuestros servicios.</p>
<p>Considerando todo como un servicio, lo que estamos haciendo es traer la oferta de la nube tradicional al terreno de Telefónica, o cualquier otro operador de telecomunicaciones, pasando de alquilar infraestructura (IaaS) o plataformas (PaaS), a proveer servicios de extremo a extremo en la red de acceso. Apalancados en la proximidad geográfica de nuestras centrales al consumidor de datos, ofreceremos estos servicios en ambas direcciones, a nuestros abonados y a los suministradores de contenido.</p>
<p>Desde un punto de vista de la red esta arquitectura de servicios nos obliga a tener en cuenta que toda la funcionalidad es escalable y que los servicios son multicomponente, que podemos separar en tres capas y que se ajustan a los procesos eTOM que rigen el desarrollo de nuestros sistemas,</p>
<ul>
<li>
<p>Servicios cara al cliente -&gt; <strong>producto</strong></p>
</li>
<li>
<p>Servicios de usuario -&gt; <strong>servicios</strong></p>
</li>
<li>
<p>Servicios básicos -&gt; <strong>recursos</strong></p>
</li>
</ul>
<p>Ilustración 7 Producto, servicio y recurso en procesos eTOM modificado</p>
<p>Para que esta concepción del sistema propuesto sea realizable debemos asumir e incluir en el desarrollo las pautas descritas a continuación,</p>
<ul>
<li>
<p>Unificar SDN + NFV + nube porque de manera independiente ninguno de estas soluciones puede resolver la problemática de los servicios de telecomunicación</p>
</li>
<li>
<p>Soportar una multiplicidad de actores que han de intervenir sobre los servicios programables de la red, reconociendo las peculiaridades de cada uno de ellos.</p>
</li>
<li>
<p>Desarrolladores de servicios, internos e incluyendo a terceros</p>
</li>
<li>
<p>Suministradores de servicios, internos e incluyendo a terceros</p>
</li>
<li>
<p>Abonados</p>
</li>
<li>
<p>Contemplar la gestión automática del estado del servicio, sin asumir que el estado de todos los servicios es igual, propiciando que existan 2 tipos de estado</p>
</li>
<li>
<p>Autoritativo, que define el estado deseado del sistema</p>
</li>
<li>
<p>Operacional, que define el estado actual, fluctuante y algunas veces erróneo del sistema en este momento</p>
</li>
<li>
<p>Potenciar redes o circuitos virtuales, distinguiendo entre red y circuito atendiendo la complejidad de la conexión demandada por el abonado, para el segmento residencial hablamos de circuitos y para empresas de redes.</p>
</li>
<li>
<p>Principio de menor privilegio</p>
</li>
<li>
<p>Implementar funcionalidad</p>
</li>
</ul>
<p><strong><span style="font-variant:small-caps;">Capas de abstracción</span></strong></p>
<p>Para su gestión creamos una abstracción de las varias capas de los servicios, lo que nos permite pensar en un concepto que podemos denominar plano de control del servicio. En el caso de CORD se ha desarrollado el sistema XOS, que es el mecanismo que realiza la representación de la estructura de los servicios, este contiene un lenguaje para escribir programas de control, y un ejecutable que aplica estas políticas en el sistema operativo</p>
<ul>
<li>
<p>Modelo de datos autoritativo basado en Django</p>
</li>
<li>
<p>Interfaces programables para las políticas tanto RESTful como OASIS-TOSCA</p>
</li>
<li>
<p>Sincronizador que utiliza Ansible para mantener el estado operacional de los recursos sincronizados con el estado autoritativo de CORD</p>
</li>
</ul>
<p>ONOS define una abstracción del grafo de la red en un conjunto de conmutadores marca blanca, ONE define un conjunto de primitivas de recursos de la nube sobre unos servidores marca blanca, y sobre esta base dual, la CTpd define tres capas de abstracción que podemos resumir así.</p>
<ul>
<li>El <em>grafo del servicio</em> existente en CORD, pendiente de adaptar a nuestra arquitectura y diseño, representa la relación de dependencia ente un conjunto de servicios que a la vez conforman un producto. CORD modela la composición del servicio como una relación de tenencia entre un servicio básico y un servicio de tenencia. La tenencia del servicio está anclada a un <em>Tenedor Principal</em>, como sería un abonado, que está unido a una o más <em>cuentas de usuario</em>.</li>
</ul>
<!-- -->

<ul>
<li>
<p>El <em>servicio</em> representa un programa multitenencia, elásticamente escalable, incluyendo la manera de instanciarle, controlarle y escalar su funcionalidad. CORD modela un servicio como un <em>controlador de servicio</em> que exporta una interfaz multitenencia y un conjunto de <em>instancias del servicio</em> que es elásticamente escalable, y que colectivamente se instancian en una <em>rebanada</em>.</p>
</li>
<li>
<p>Una r<em>ebanada</em> representa un conjunto o contenedor de recursos de todo el sistema dentro del que se ejecutan los servicios, incluyendo el cómo se especifica la inclusión de estos recursos en la infraestructura que les soporta. En la CTpd modelamos una rebanada como un grupo de <em>máquinas virtuales</em>, que se implementan en OpenNebula y un conjunto de <em>circuitos virtuales</em> que las implementa ONOS.</p>
</li>
<li>
<p>Un <em>circuito virtual</em> representa una interconexión de comunicación entre un conjunto de instancias, se soportan varios tipos de circuitos virtuales, incluyéndose los <em>privados</em> que conectan instancias con una rebanada, de <em>acceso directo</em> utilizado por un servicio de tenencia para acceder a un servicio básico direccionando directamente cada instancia del servicio básico, y de <em>acceso indirecto</em> utilizado por servicio de tenencia para acceder al servicio básico direccionando el servicio como un todo.</p>
</li>
</ul>
<p>El mecanismo subyacente en la CTpd que soporta los Circuitos Virtuales se implementa con un par de aplicaciones de control que corren en ONOS. La primera de ellas, llamada VTN, instala reglas de flujo en los OvS que corren en cada servidor para implementar el direccionamiento directo o indirecto. La segunda es Forwarding e implementa flujos entre los servidores y otrs dispositivos físicos de la central a través del tejido de conmutación.</p>
<h3 id="terminal-optico-virtual-volt">Terminal óptico virtual - vOLT</h3>
<p>Este concepto en ONOS es producto de desmantelar los actuales terminales ópticos monolíticos de las centrales, por una combinación de silicón Marco Polo y una aplicación de código abierto. La primera implementación se ha enfocado en el acceso GPON, pero es igualmente extrapolable a XGS-PON remplazando la bandeja de silicón, algo que AT&amp;T ya ha especificado dentro del Open Compute Project.</p>
<p>Las bandejas de puertos ópticos GPON fabricados bajo la especificación de AT&amp;T dentro del OCP ya están disponibles en mercado, el proveedor es Celestica quien cuenta con plantas de fabricación en Valencia. La bandeja incluye los microcircuitos esenciales de GPON MAC controlados por un programa de control remoto que a su vez es gestionado vía OpenFlow por una aplicación de alto nivel.</p>
<p>Ilustración 8 Descomposición del terminal óptico de la C.T.</p>
<p>Hay dos piezas de código que funcionan conjuntamente para implementar la funcionalidad del vOLT. La primera es un agente vOLT que se instancia en una máquina virtual y facilita la conexión entre ONOS y el equipo, este agente expone hacia arriba una interfaz OpenFlow lo que permite que sea controlado por ONOS; de ahí mapea lo mensajes OF a las API nativas del equipo y mensajes OCMI que gestionan las ONT de la red óptica pasiva. La segunda pieza de código es un conjunto de funciones que gestionan algunas de las funciones tradicionales de una terminal óptica, como 802.1X, IGMP Snooping, puentes VLAN y OAM; estas funciones de control están implementadas como aplicaciones que se ejecutan sobre ONOS, facilitan instanciar un abonado, autenticación.</p>
<p><strong><span style="font-variant:small-caps;">Arquitectura del circuito de acceso GPON</span></strong></p>
<p>El tráfico de los abonados se identifica dentro de la central telefónica por dos etiquetas en los circuitos virtuales, el terminal óptico en destino y en la central son responsables del etiquetado y desetiquetado del tráfico de cada abonado en lo que este va y viene; ONOS le instruye a la OLT que circuito utilizar mediante mensajes OpenFlow.</p>
<p>La siguiente ilustración muestra donde ocurre el etiquetado según el tráfico transita desde la sede del abonado hasta internet. Dentro del hogar no hay etiquetas…</p>
<p>Ilustración 9 Arquitectura del circuito de acceso PON</p>
<p><strong><span style="font-variant:small-caps;">Componentes lógicos</span></strong></p>
<p><em>Agente vOLT</em></p>
<p>Este agente está construido con <em>Indigo</em>, <em>netconfd</em>, las interfaces propietarias del silicón de PMC Sierra, ahora MicroSemi, y una pila OMCI.</p>
<p><em>Aplicaciones en ONOS</em></p>
<ul>
<li>
<p>vOLT (onos-app-olt) está encargada de configurar las etiquetas del circuito en la OLT</p>
</li>
<li>
<p>AAA (onos-app-aaa) tramita la autenticación entre el dispositivo del hogar y el servidor Radius; cuando el usuario ha sido autorizado, la aplicación debe instanciar los servicios del usuario en OneFlow</p>
</li>
</ul>
<h3 id="enrutador-de-central-virtualizado-erc">Enrutador de Central virtualizado - <span style="font-variant:small-caps;">ErC</span></h3>
<p>Esta aplicación de control de la red se ejecuta en ONOS y es la encargada de agregar los circuitos de los abonados y canalizar el tráfico desde/hacia la red de transporte. Como es de esperar esta aplicación, que la podemos visualizar como un servicio, debe modificarse para acomodar las necesidades específicas de las redes de transporte de Telefónica y de la red de transporte de Telefónica de España para este case de uso. En CORD esta aplicación se conoce como vRouter.</p>
<p><strong><span style="font-variant:small-caps;">Requisitos del ErC</span></strong></p>
<ul>
<li>
<p>Realizar enrutamiento mono difusión de y hacia la C.T.; participar en protocolos de enrutamiento dinámico</p>
</li>
<li>
<p>Señalización y entrega de multidifusión</p>
</li>
<li>
<p>Aplicar políticas de calidad del servicio (QoS)</p>
</li>
<li>
<p>Realizar funcionalidad de traducción de direcciones de red (NAT)</p>
</li>
<li>
<p>No realiza todas las funciones de los actuales BNG monolíticos</p>
</li>
</ul>
<p><strong><span style="font-variant:small-caps;">Diseño</span></strong></p>
<p>El diseño del servicio <strong><span style="font-variant:small-caps;">ErC</span></strong> está compuesto de dos partes que se pueden considerar relativamente independientes, la parte de plano de control y la del plano de datos.</p>
<p><em>Plano de Control</em></p>
<p>La funcionalidad primordial del <strong><span style="font-variant:small-caps;">ErC</span></strong> es la hablar protocolos de enrutamiento con enrutadores externos; para evitar tener que implementar protocolos de enrutamiento dentro de una aplicación de ONOS, hemos elegido la utilización del sistema de enrutamiento Quagga, que es un sistema de código abierto y soporta una gran variedad de protocolos de enrutamiento. Para el caso de uso de una C.T. no anticipamos problemas de desempeño que se podría tener con enrutadores globales.</p>
<p>Quagga se configurará para comunicarse con los enrutadores de transporte de Telefónica y de cara a la CTpd utilizaremos la interfaz <em>FIB Push Interface (FPI)</em> para comunicar las rutas desde Quagga a ONOS. Desde aquí la aplicación <strong><span style="font-variant:small-caps;">ErC</span></strong> actúa como el gestor del plano de entrega, <em>Forwarding Plane Manager (FPM)</em>, y es capaz de recibir y descodificar rutas de Quagga, y entonces utiliza estas rutas para programar el plano de datos de manera correspondiente.</p>
<p>El diseño de este plano de control es similar a la aproximación utilizada en la aplicación ONOS SDN-IP, es decir utilizando la filosofía de construir sobre bloques ya desarrollados. La diferencia reside en que necesitamos soportar más que BGP, porque necesitamos también soportar un protocolo de pasarela interior, <em>Interior Gateway Protocol (IGP)</em> y como mencionamos arriba utilizamos la interfaz FPM para comunicar Quagga y ONOS mientras que en la SDN-IP se utiliza iBGP como conexión.</p>
<p><em>Circuito de control</em></p>
<p>Para que Quagga se comunique con los enrutadores aguas arriba, cierto tráfico de control debe fluir entre el servidor Quagga y el enrutador externo. Previamente a intercambiar rutas, la primera tarea del <strong><span style="font-variant:small-caps;">ErC</span></strong> es programar el plano de datos para que este tráfico fluya. El servidor Quagga está conectado a un puerto en el plano de datos del <strong><span style="font-variant:small-caps;">ErC</span></strong>, y los paquetes de enrutamiento entrantes y salientes se direccionan a ese puerto; esto permite circunvenir la función usual de enrutamiento del <strong><span style="font-variant:small-caps;">ErC</span></strong> porque este tráfico es de control destinado únicamente al enrutador mismo.</p>
<p>Ilustración 10 Circuito de control del enrutador</p>
<p><em>Multidifusión</em></p>
<p>El <strong><span style="font-variant:small-caps;">ErC</span></strong> necesita soportar señalización de multidifusión PIM-SSM aguas arriba, pero Quagga no gestionará PIM-SSM por lo que una aplicación dedicada de ONOS se hará cargo de gestionar los mensajes PIM.</p>
<p><em>Plano de datos</em></p>
<p>Por ahora la solución de CORD utiliza un conmutador dedicado para el plano de datos del vRouter y no utiliza el tejido CLOS, eso se hizo por razones de tiempo y aparente complejidad. En el <strong><span style="font-variant:small-caps;">ErC</span></strong> vamos a intentar resolver esto porque no tiene mucho sentido redundar conmutadores fuera del tejido, además el gestor del tejido dentro de ONOS debemos mejorarlo para que soporte NAT y QoS.</p>
<h3 id="vpdc-pasarela-de-cliente-equivalente-al-vsg-de-cord">vPdC Pasarela de Cliente equivalente al vSG de CORD</h3>
<p>La idea de mover a la C.T. parte de la funcionalidad del dispositivo de interconexión del hogar del cliente, no solo por temas de ahorro de coste mantenimiento, sino para poder ofrecer funcionalidad adicional que hoy no es posible ofertar debido en parte al alto coste de despliegue en casa del cliente.</p>
<p>En esta primera fase la funcionalidad que se ha ofrece desde la CTpd se basa en Linux y se ejecutaría en la máquina virtual, o contenedor, de cada cliente. Por ahora son funciones básicas de conectividad con internet y algunos servicios de valor añadido, como suspender/reanudar, control parental, etc.</p>
<p>La pasarela cumplirá con los principios de arquitectura del <a href="https://tools.ietf.org/html/rfc7368"><em>RFC 7368</em></a> “Redes IPv6 del hogar” que tiene permite tener múltiples subredes, por ejemplo, para facilitar tener una red privada y otra de invitados, capas de enlace heterogéneas, componentes inteligentes de servicios públicos, y tener suficiente espacio de direccionamiento disponible para permitir que cada dispositivo tenga una única dirección global. No se debe esperar que los usuarios en el hogar configuren sus redes, por tanto, el RFC 7368 asume que en la medida de lo posible la red del hogar se auto organiza y auto configura, es decir, que funcionará sin una gestión dedicada por parte del cliente residencial.</p>
<p>Es importante distinguir entre direccionamiento y ser contactado, mientras IPv6 ofrece direccionamiento global mediante la utilización de direcciones únicas en el hogar, si un dispositivo puede ser contactado globalmente o no dependerá de algún cortafuegos o configuración de filtrado, y no de la presencia o utilización de un NAT como en IPv4.</p>
<p>La vPdC también debe tener en cuenta lo descrito en el <a href="https://tools.ietf.org/html/rfc7084"><em>RFC 7084</em></a> “Requisitos básicos de los enrutadores IPv6 de cliente”, actuar como el borde entre la red del hogar y las redes externas, ser capaz de proporcionar prefijos para la creación de subredes dentro del hogar, gestionar eficientemente direcciones locales y direcciones globales de cada dispositivo.</p>
<p>Ilustración 11 Arquitectura de la red IPv6 del hogar</p>
<h3 id="tejido-clos-virtualizacion-y-composicion-de-servicios">Tejido CLOS, virtualización y composición de servicios</h3>
<p>La arquitectura de la CTpd y por ende del tejido CLOS tiene las siguientes características:</p>
<ul>
<li>
<p>Es una solución de ciberconectividad basada en un tejido CLOS, con conmutadores marca blanca y programas de conmutación de código abierto, aunque se podrían utilizar otros protocoles, la arquitectura se basa totalmente en OpenFlow.</p>
</li>
<li>
<p>El tejido tiene las siguientes características:</p>
</li>
<li>
<p>La conmutación de nivel 2 en cada bastidor la hacen los conmutadores TOR</p>
</li>
<li>
<p>Utilizamos ECMP para el flujo de nivel 3 entre los bastidores</p>
</li>
<li>
<p>VLAN cross-connect feature to switch QinQ packets entre las bandejas de acceso óptico y las instancias del vSG</p>
</li>
<li>
<p>Multidifusión IPv6 para los flujos de IPTV desde las cabeceras de vídeo hasta los abonados</p>
</li>
<li>
<p>Soporte del <strong><span style="font-variant:small-caps;">ErC</span></strong> para conectarse a los enrutadores de transporte si es necesario para establecer rutas públicas</p>
</li>
<li>
<p>Facilidad de utilizar el tejido CLOS en despliegues mono o multibastidor</p>
</li>
<li>
<p>El tejido CLOS forma la red subyacente en una arquitectura de redes sub y supra yacente, la red supra yacente, a veces referida como el tejido exterior, también se basa en SDN con estas características:</p>
</li>
<li>
<p>Utilización de conmutadores virtuales, OvS con DPDK, con una conexión a medida para el encadenamiento de servicios</p>
</li>
<li>
<p>Equilibradores de carga distribuidos para servicios en cada OvS</p>
</li>
<li>
<p>Túneles VxLAN en OvS para redes virtuales supra yacentes</p>
</li>
</ul>
<p>La ventaja más notoria de utilizar un control de SDN común para la infraestructura suprayacente y para el tejido subyacente es que pueden ser orquestados conjuntamente para dar las prestaciones y servicios que demanda una Central Telefónica, con la agilidad y eficiencias de las operaciones de un CPD.</p>
<p>Ilustración 12 Adaptar el controlador del tejido a IPv6</p>
<p><strong><span style="font-variant:small-caps;">Aplicaciones de control de ONOS</span></strong></p>
<p>El tejido en el centro de este diseño es el encargado de interconectar todos los componentes de la CTpd, incluyendo las bandejas de acceso GPON, nodos de computación y las tarjetas de interconexión a la red de transporte. La aplicación de control del tejido se ejecuta sobre ONOS e interactúa con otras aplicaciones necesarias para proporcionar los varios servicios de la CTpd.</p>
<p>En la implementación para CORD y para facilitar el desarrollo de las varias aplicaciones, se han ejecutado dos instancias de ONOS separando el control del tejido del resto de aplicaciones de servicio, lo recomendado por ON.Lab es utilizar una instancia de ONOS para controlar CORD, y es lo que hemos planificado para la CTpd.</p>
<p><strong><span style="font-variant:small-caps;">Infraestructura y programas del tejido</span></strong></p>
<p>Los conmutadores en el tejido se apalancan y utilizan infraestructura y programas de varios proyectos con licencias abiertas, tal como se ilustra a continuación.</p>
<p>Ilustración 13 Infraestructura y programas del tejido</p>
<p>Todos los conmutadores son idénticos, utilizan los mismos programas de control, gestión y conmutación, la única diferencia es su posición en el tejido, esta solución fue propuesta y desarrollada en el proyecto Atrium de la ONF, e incluye los componentes utilizados en esta solución, ONL y ONIE como sistema operativo de los conmutadores, también utiliza el OF-DPA de Broadcom para abrir varias de sus interfaces propietarias en términos entendibles por OpenFlow y que se encuentran en su SDK. Esto permite que, en este caso, ONOS pueda programar todas las tablas de encaminamiento en el ASIC del conmutador, para así poder utilizar toda la funcionalidad existente en los ASIC de nueva generación.</p>
<p><strong><span style="font-variant:small-caps;">Soporte del tejido para la comunicación entre la OLT y el vSG</span></strong></p>
<p>Los abonados residenciales se conectan a la CTpd a través de la red de acceso GPON, y al descomponer la OLT monolítica, todas las funciones que se transforman en programas se trasladan a la nube, y una de ellas es el vSG.</p>
<p>Tal como describimos con anterioridad, el tráfico entre la cSG se etiqueta doblemente, con la etiqueta externa identificando la red PON a la que pertenece el abonado, y la etiqueta interior identifica al abonado individualmente.</p>
<p>Esta infraestructura del acceso residencial es manejada por la aplicación de control vOLT, coordinada con OneFlow. Una vez que el cliente ha sido identificado y autenticado, las doble etiquetas del circuito asignadas para el cliente, y se ha instanciado su vSG en un nodo de cómputo, la aplicación que controla el tejido es informada de cuál es la OLT a donde está conectado el cliente y el nodo de computación donde se ha instanciado el vSG de cliente; entonces la aplicación programa las reglas de encaminamiento dentro del tejido que permiten que la comunicación fluya dentro del mismo.</p>
<p><strong><span style="font-variant:small-caps;">Composición del servicio</span></strong></p>
<p>Los servicios en la CTpd se ensamblan utilizando las mejores prácticas de las operaciones en la nube; los servicios los instancia el operador o gestor de red utilizando OneFlow, a su vez OneFlow le presenta a ONOS un grafo del servicio para el tráfico del abonado. Este grafo es luego descompuesto en reglas de tráfico que se programan en la infraestructura de red de la CTpd por la aplicación VTN de ONOS. En esta sección se hace un breve resumen de las alternativas de implementación hechas para realizar la composición del servicio en la infraestructura de red.</p>
<p>En la CTpd la composición del servicio se implementa utilizando redes suprayacentes y virtualización de la red, brevemente,</p>
<ul>
<li>
<p>Los servicios tienen su propio circuito virtual, CV – las máquinas virtuales o contenedores que instancian el servicio son parte del mismo CV, y estas instancias se pueden crear en cualquier lugar de la nube del CTpd, es decir en cualquier nodo de cómputo o en distintos bastidores</p>
</li>
<li>
<p>Incrementar o reducir dinámicamente la cantidad de máquinas virtuales / contenedores, y por ende del mismo CV, esta característica es primordial para tener escalabilidad en la nube</p>
</li>
<li>
<p>Cada nodo de cómputo aloja máquinas virtuales o contenedores, pertenecientes a CV de múltiples servicios, conectadas a OvS que actúan como conmutadores de hipervisor muy programables y controlados con OpenFlow</p>
</li>
<li>
<p>Cada CV o servicio tiene su propio balanceador de carga distribuido a lo largo de cada OvS en la red. La función del balanceador de carga es seleccionar la instancia de una máquina virtual que está instanciando un servicio, entre todas las máquinas virtuales dentro del CV del servicio</p>
</li>
<li>
<p>Proceso de composición de un servicio: Digamos que una máquina virtual S1B, que es una instancia en el CV Servicio 1, descubre que</p>
</li>
</ul>
<h2 id="construccion-de-la-prueba">Construcción de la prueba</h2>
<p>El caso base que vamos a construir y que será la implementación de referencia, comprende los siguientes fundamentos:</p>
<ul>
<li>
<p>Está desarrollada con componentes de código abierto</p>
</li>
<li>
<p>Es un sistema integrado que está completo para someter a pruebas de campo</p>
</li>
<li>
<p>Va a ejercitar todas las funcionalidades para las que se ha diseñado</p>
</li>
</ul>
<p>Ilustración 6 Esquema del caso 1</p>
<p>La arquitectura convencional de CTpd será simplificada en el objetivo de validar el diseño de esta implementación. Esta sección describe los componentes involucrados en este diseño.</p>
<h3 id="componentes">Componentes</h3>
<p><strong>Figura 2.</strong> Arquitectura de la maqueta</p>
<p>En la <em>Figura 2</em> podemos identificar los componentes de la prueba de concepto, que están a su vez detallados en la <em>Tabla 1</em>.</p>
<table>
<thead>
<tr>
<th><strong>Nombre</strong></th>
<th><strong>Tipo</strong></th>
<th><strong>Descripción</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Nodos de Computación</td>
<td>Nodo Físico</td>
<td>2 Nodos Físicos. Todas las VLANs están conectadas a ambos nodos. Exceptio la red de Cliente que solo está conectada a <em>test-oln-hn-01</em>.</td>
</tr>
<tr>
<td>ONE</td>
<td>Máquina Virtual</td>
<td>Instalación de OpenNebula y de OneFlow. Responsable del despliegue de la MVs y de delegar la configuración de red en ONOS..</td>
</tr>
<tr>
<td>Radius</td>
<td>Máquina Virtual</td>
<td>Servidor de autenticación. Será accedido por la aplicación AAA de ONOS.</td>
</tr>
<tr>
<td>ONOS</td>
<td>Máquina Virtual</td>
<td>Instalación del SDN ONOS. Las siguientes aplicación estará desplegadas:</td>
</tr>
</tbody>
</table>
<pre><code>                                           `AAA                                                                                                                                                                                                                                                                                                                                                                                                                    OLT`

                                           `FWD`

                                           `VR``                                                                                                                                                ``                               ``                                                                                                                                                                                                ``              ``                                                                                                                                                                                                                                                                                                                                                                               ``                              ``             ``                      `  |
</code></pre>
<p>| vOLT                      | Máquina Virtual | Esta MV actúa como el punto de entrada del tráfico del cliente al CTpd. Dispone de una instancia de Open vSwitch que está controlado por ONOS y que enruta el tráfico al tejido CLOS. Ya que está MV está conectada a la red del Cliente deberá ser ejecutada en <em>test-oln-hn-01</em>.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| CLOS                      | Máquina Virtual | Una instancia de mininet que simula el tejido CLOS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| DNS                       | Máquina Virtual | Servicio DNS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Portal Cautivo            | Máquina Virtual | El servicio Telco 3.0, que estará conectado a la base de datos de los usuarios y determinará si un cliente tiene acceso o no a internet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| VErC                      | Máquina Virtual | Enrutador de la Central. Provee acceso a internet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| vPdC                      | Máquina Virtual | Pasarela de Cliente. Esta MV será instanciada por OneFlow para cada cliente e implementará los servicios básicos de conectividad: configuración DNS, DHCP y anuncio de prefijos IPv6.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Bridge de red de Cliente  | Bridge de Linux | Este bridge está conectado a un HGU (Home Gateway Unit) que generará el tráfico de Cliente. Se trata de una red puramente ethernet/L2 que está conectada al vOLT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Bridge de red de Servicio | Bridge de Linux | La red interna de Servicios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Bridge de red de Gestión  | Bridge de Linux | Todas las MVs estarán conectadas a la red de Gestión de a través de este bridge.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |</p>
<p><strong>Table 1.</strong> PoC Architecture Components</p>
<h3 id="redes">Redes</h3>
<p>Existen varias redes utilizadas en este proyecto, que están detalladas en la <em>Tabla 2</em>.</p>
<table>
<thead>
<tr>
<th><strong>Nombre</strong></th>
<th><strong>VLAN</strong></th>
<th><strong>Tipo/Direccionamiento</strong></th>
<th><strong>Descripción</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Gestión (OLN_MGMT)</td>
<td>302</td>
<td>Ethernet L2</td>
<td>Red de Gestión para todas las MVs de la maqueta.</td>
</tr>
<tr>
<td>Cliente (OLN_CLI)</td>
<td>304</td>
<td>IPv6 2a02:9008:4:b110::/64</td>
<td>Red de Cliente.</td>
</tr>
<tr>
<td>Servicio (OLN_SRV)</td>
<td>303</td>
<td>IPv4</td>
<td></td>
</tr>
<tr>
<td>10.95.84.0/26</td>
<td>Red de Servicio, usada por los servicios internos del CLOS.</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Tabla 2.</strong> Redes de la Prueba de Concepto.</p>
<h3 id="recursos-de-hardware">Recursos de Hardware</h3>
<p>Los recursos de Hardware utilizados en esta prueba de concepto están descritos en la <em>Tabla 3</em>.</p>
<table>
<thead>
<tr>
<th><strong>Nombre</strong></th>
<th><strong>Descripción</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>test-oln-cn-01</td>
<td>Nodo de Computación que será utilizado como un hipervisor de OpenNebula, utilizando la tecnología KVM como herramienta de virtualización.</td>
</tr>
</tbody>
</table>
<pre><code>              -   CPU: 8x Intel(R) Xeon(R) CPU E5450 @ 3.00GHz

              -   Memoria: 16 GB

              -   Almacenamiento: 236 GB                                                                                                                                  |
</code></pre>
<p>test-oln-hn-01 | Nodo de Computación que será utilizado como un hipervisor de OpenNebula, utilizando la tecnología KVM como herramienta de virtualización. - CPU: 8x Intel(R) Xeon(R) CPU E5450 @ 3.00GHz - Memoria: 32 GB - Almacenamiento: 224 GB |
HGU | Un Home Gateway Unit que generará y simulará el tráfico del Cliente. Estarça conectado a la maqueta a través de la red del Cliente y del componente vOLT |
Soporte de Red | La prueba de concepto tiene lugar en el seno de una red que particiona las VLANs 302, 303 y 304, y que a su vez provee enrutado para el acceso a internet. |</p>
<p><strong>Tabla 3.</strong> PoC Hardware Resources</p>
<h2 id="opennebula">OpenNebula</h2>
<h3 id="diseno">Diseño</h3>
<p>En la <em>Tabla 4</em> se muestra un resumen de la configuración del despliegue de OpenNebula.</p>
<table>
<thead>
<tr>
<th><strong>Sistema Operativo</strong></th>
<th>Ubuntu 14.04 tanto en el Front-end como en los nodos de computación.</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hipervisor</strong></td>
<td>2 nodos de computación usando KVM.</td>
</tr>
<tr>
<td><strong>Base de Datos</strong></td>
<td>MySQL</td>
</tr>
<tr>
<td><strong>Redes</strong></td>
<td>Bridges de Linux y tecnología 802.1Q para la implementación de la segmentación por VLANs.</td>
</tr>
<tr>
<td><strong>Almacenamiento</strong></td>
<td>Almacenamiento local del Hipervisor. ~ 230 GB.</td>
</tr>
<tr>
<td><strong>Autenticación</strong></td>
<td>Nativo de OpenNebula</td>
</tr>
<tr>
<td><strong>Interfaces</strong></td>
<td>OneFlow, CLI, Sunstone</td>
</tr>
</tbody>
</table>
<p><strong>Table 4.</strong> PoC Hardware Resources</p>
<p>Tanto la MV de OpenNebula como los nodos físicos de Computación han sido provisionados usando Ansible. El playbook está disponible aquí: <a href="https://github.com/Telefonica/ctpd/tree/master/ansible"><em>https://github.com/Telefonica/ctpd/tree/master/ansible</em></a>. Esto permitirá una reinstalación uniforme e idéntica en el futuro. Dado que este playbook contiene los detalles exactos de la provisión, para más detalles revisar este recurso.</p>
<h4 id="front-end-de-opennebula">Front-end de OpenNebula</h4>
<p>OpenNebula ha sido desplegada en una Máquina Virtual que está siendo ejecutada en el nodo <em>test-oln-cn-01</em>. Ha sido desplegada manualmente y configurada para exponer los siguientes servicios:</p>
<ul>
<li>
<p>OpenNebula 4.14.2 (oned)</p>
</li>
<li>
<p>Planificador (mm_sched)</p>
</li>
<li>
<p>OneFlow (oneflow-server)</p>
</li>
<li>
<p>Sunstone (sunstone-server)</p>
</li>
<li>
<p>Línea de Comandos de Linux</p>
</li>
</ul>
<p>Actualmente solo se ha desplegado un único Front-end, por lo que no hay Alta Disponibilidad. Sin embargo el entorno está preparado para configurarse en modo de Alta Disponibilidad tras desplegar un segundo nodo de Front-end.</p>
<h4 id="nodos-de-computacion">Nodos de Computación</h4>
<p>Los nodos de computación son responsables de proveer a las Máquinas Virtuales con los recursos necesario (por ejemplo CPU, Memoria, acceso a la red). OnLife es homogénea en cuanto a la tecnología utilizada para virtualizar, manejando 2 nodos con el hipervisor KVM. La configuración de los nodos también es homogénea en término de componentes de software instalados.</p>
<p>Para los nodos de computación se han instalado el siguiente software:</p>
<ul>
<li>
<p>Libvirt y qemu-kvm</p>
</li>
<li>
<p>Paquete de nodo de OpenNebula 4.14.2 KVM</p>
</li>
<li>
<p>El usuario <em>oneadmin</em> tiene acceso de administración al almacenamiento, redes y virtualización.</p>
</li>
<li>
<p>Conexión SSH sin password entre los nodos y el Front-end.</p>
</li>
<li>
<p>Ruby instalado (utilizado por las sondas y drivers de OpenNebula)</p>
</li>
</ul>
<h4 id="almacenamiento">Almacenamiento</h4>
<p>OnLife utilizará el almacenamiento local disponible en los nodos, usando el driver SSH. Esto significa que no se ha implementado ninguna configuración específica para el almacenamiento.</p>
<h4 id="redes_1">Redes</h4>
<p>Todavía por determinar.</p>
<h4 id="autenticacion">Autenticación</h4>
<p>Los usuarios se mantendrán en la base de datos de OpenNebula, como usuarios nativos, y la autenticación se realizará mediante usuario/password.</p>
<h3 id="configuracion-del-front-end">Configuración del Front-end</h3>
<p>OpenNebula se ha instalado y configurado para ejecutar los servicios básicos de OpenNebula: <em>oned</em>, el planificador <em>mm_sched</em>, <em>OneFlow</em> y <em>Sunstone</em>).</p>
<p>El sistema operativo es Ubuntu 14.04 obtenido directamente de los repositorios de Ubuntu.</p>
<p>En la <em>Tabla 5</em> se detallan las modificaciones a los ficheros de configuración pertenecientes a OpenNebula.</p>
<table>
<thead>
<tr>
<th><strong>Fichero de Configuración</strong></th>
<th><strong>Descripción</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>oned.conf</td>
<td># Sample configuration for MySQL</td>
</tr>
</tbody>
</table>
<pre><code>                            DB = \[ backend = "mysql",

                            server = "localhost",

                            port = 0,

                            user = "oneadmin",

                            passwd = "opennebula",

                            db\_name = "opennebula" \]         |
</code></pre>
<p>sunstone-server.conf | :host: 0.0.0.0 |</p>
<p><strong>Tabla 5.</strong> Resumen de los cambios de configuración de OpenNebula (relativos a /etc/one)</p>
<h3 id="configuracion-de-los-nodos-de-computacion">Configuración de los Nodos de Computación</h3>
<p>Los nodos de computación han sido provisionados siguiendo este procedimiento:</p>
<ul>
<li>
<p>Habilitar el repositorio de OpenNebula.</p>
</li>
<li>
<p>Instalación del paquete <em>opennebula-node</em>.</p>
</li>
</ul>
<p>Los ficheros de configuración alterados durante la instalación están descritos en la <em>Tabla 6.</em></p>
<table>
<thead>
<tr>
<th><strong>Fichero de Configuración</strong></th>
<th><strong>Descripción</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>/etc/apparmor.d/abstractions/libvirt-qemu</td>
<td>/srv/** rw,</td>
</tr>
</tbody>
</table>
<pre><code>                                         /var/lib/one/datastores/\*\* rw  |
</code></pre>
<p><strong>Tabla 6.</strong> Resumen de los cambios en los ficheros de configuración.</p>
<h3 id="imagenes-y-templates">Imágenes y Templates</h3>
<p>Se hará disponible una única imagen en la Nube de OpenNebula: una Ubuntu 14.04. Esta imagen será utilizada como Sistema Operativo para todas las Máquinas Virtuales y servicios que se utilizarán en esta maqueta: vOLT, ONOS, Portal Cautivo, etc.</p>
<p>Cuando se despliegue un nuevo servicio basado en una Máquina Virtual, la imagen de base deberá ser clonada y convertida en persistente. A su vez se deberá de crear un nuevo template referenciando esa imagen. Opcionalmente las IPs se podrán listar de manera específica en el template de forma que un mismo servicio siempre tenga la misma IP.</p>
<h3 id="nodos-clusters-vdcs">Nodos / Clusters / VDCs</h3>
<p>Los dos nodos de KVM son homogéneos. No se crearán clusters y todos los recursos serán incluidos en el VDC por defecto. Los dos nodos registrados en la nube no disponen de la misma capacidad, sin embargo OpenNebula monitorizará la capacidad disponible y escogerá dónde desplegar cada Máquina Virtual.</p>
<p>El nodo <em>test-oln-hn-01</em> es el único conectado a la red <em>OLN_CLI,</em> por lo que la Máquina Virtual de vOLT deberá ser ejecutada en este nodo específicamente.</p>
<h3 id="almacenamiento-y-datastores">Almacenamiento y Datastores</h3>
<p>El storage por defecto será local en los nodos de Computación. Por lo tanto, tanto el Datastore de Sistema como el de Imágenes deberán utilizar TM_MAD="ssh". Las imágenes serán almacenadas en formato Qcow2.</p>
<h3 id="redes-virtuales">Redes Virtuales.</h3>
<p>Por determinar.</p>
<h3 id="usuarios-y-grupos">Usuarios y Grupos</h3>
<p>Existirá un único usuario: <em>oneadmin</em>, al cual le pertenecerán todos los recursos.</p>
<h2 id="onos-telcaria">ONOS (Telcaria)</h2>
<h3 id="_1"></h3>
<h2 id="sistema-de-supervision-y-gestion">Sistema de supervisión y gestión</h2>
<p>Este servicio debe ser capaz de supervisar, y si es necesario, analizar la marcha de distintos componentes de la CTpd en la prestación de los servicios encomendados. Los principales objetivos y requisitos de este sistema son</p>
<ul>
<li>
<p>Ser una plataforma genérica para el análisis</p>
</li>
<li>
<p>Debe ser escalable y soportar multitenencia</p>
</li>
<li>
<p>Debe ser posible introducir instrumentos o sondas en los servicios más allá de los dispositivos de cómputo y conmutación</p>
</li>
<li>
<p>Debe ser posible ajustar el nivel de sondeo en los dispositivos subyacentes</p>
</li>
<li>
<p>Debe ser posible agrupar la información de las sondas</p>
</li>
<li>
<p>Debe ser posible redireccionar los flujos de tráfico a través de una “sonda virtual” para obtener una inspección más profunda que no es posible obtener de los dispositivos subyacentes</p>
</li>
</ul>
<p>Como se muestra en la siguiente ilustración, el sistema de supervisión consigue la información de las sondas desde distintos elementos de red en la CTpd, incluyendo nodos de cómputo, conmutadores, dispositivos de acceso a central y servicios programables ejecutándose en los nodos de cómputo; poniéndola a disposición de otras aplicaciones de análisis ejecutándose en la CTpd. El sistema se integrará con los servicios residenciales como el vSG, el ErC, la vOLT, aunque la arquitectura permite extenderlo a futuras plataformas de la CTpd como el acceso móvil o empresarial.</p>
<h3 id="ensamblaje-del-bastidor">Ensamblaje del bastidor</h3>
<p>Estas son las instrucciones esquematizadas para armar el bastidor, en los repositorios <em>github</em> referenciados se encuentran las instrucciones detalladas. El bastidor de la prueba ya vendrá con la infraestructura ensamblada de origen y los programas precargados en la planta de ensamblaje del suministrador.</p>
<p><strong><span style="font-variant:small-caps;">Infraestructura</span></strong></p>
<p>Los servidores, conmutadores y bandejas de acceso se pueden ensamblar en varias configuraciones virtuales como se ilustra a seguir</p>
<p><em>Describir circuito de control y administración basado en el OCP/OCS de Microsoft</em></p>
<p><strong><span style="font-variant:small-caps;">Programas</span></strong></p>
<p>La instalación de los programas que conforman el sistema se hará en el orden descrito a continuación</p>
<ul>
<li>
<p>OpenNebula</p>
</li>
<li>
<p>ONOS-CTpd</p>
</li>
<li>
<p>ONOS-Tejido</p>
</li>
<li>
<p>OpenMano + OneFlow</p>
</li>
</ul>
<p><em>Instalación de OpenNebula</em></p>
<p>Las instrucciones para instalar OpenNebula se encuentran en esta dirección IP</p>
<p><a href="http://docs.opennebula.org/4.14/design_and_installation/building_your_cloud/ignc.html"><em>http://docs.opennebula.org/4.14/design_and_installation/building_your_cloud/ignc.html</em></a></p>
<p>y se puede descargar desde este sitio</p>
<p><a href="http://downloads.opennebula.org/packages/opennebula-4.14.2/"><em>http://downloads.opennebula.org/packages/opennebula-4.14.2/</em></a></p>
<p>La instalación sería la siguiente:</p>
<ul>
<li>
<p># wget -q -O- <a href="http://downloads.opennebula.org/repo/Ubuntu/repo.key"><em>http://downloads.opennebula.org/repo/Ubuntu/repo.key</em></a> | apt-key add -</p>
</li>
<li>
<p># echo "deb <a href="http://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/"><em>http://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/</em></a> stable opennebula" \   &gt; /etc/apt/sources.list.d/opennebula.list</p>
</li>
<li>
<p># apt-get update</p>
</li>
<li>
<p># apt-get install opennebula opennebula-sunstone nfs-kernel-server</p>
</li>
<li>
<p>Cambiar en el Fichero /etc/one/sunstone-server.conf  “:host: 127.0.0.1” por  “:host: 0.0.0.0”</p>
</li>
<li>
<p># /etc/init.d/opennebula-sunstone restart</p>
</li>
<li>
<p>Añadir al Fichero vi /etc/exports file  “/var/lib/one/ *(rw,sync,no_subtree_check,root_squash)”</p>
</li>
<li>
<p># service nfs-kernel-server restart</p>
</li>
<li>
<p># su - oneadmin</p>
</li>
<li>
<p>$ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys</p>
</li>
<li>
<p>$ cat &lt;&lt; EOT &gt; ~/.ssh/config</p>
</li>
<li>
<p>Host *</p>
</li>
<li>
<p>StrictHostKeyChecking no</p>
</li>
<li>
<p>UserKnownHostsFile /dev/null</p>
</li>
<li>
<p>EOT</p>
</li>
<li>
<p>$ chmod 600 ~/.ssh/config</p>
</li>
<li>
<p># wget -q -O- <a href="http://downloads.opennebula.org/repo/Ubuntu/repo.key"><em>http://downloads.opennebula.org/repo/Ubuntu/repo.key</em></a> | apt-key add -</p>
</li>
<li>
<p># echo "deb <a href="http://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/"><em>http://downloads.opennebula.org/repo/4.12/Ubuntu/14.04/</em></a> stable opennebula" &gt; \     /etc/apt/sources.list.d/opennebula.list</p>
</li>
<li>
<p># apt-get update</p>
</li>
<li>
<p># apt-get install opennebula-node nfs-common bridge-utils</p>
</li>
<li>
<p>Configurar fichero red: /etc/network/interfaces</p>
</li>
<li>
<p>auto lo</p>
</li>
<li>
<p>iface lo inet loopback</p>
</li>
<li>
<p>auto br0</p>
</li>
<li>
<p>iface br0 inet dhcp</p>
</li>
<li>
<p>bridge_ports eth0</p>
</li>
<li>
<p>bridge_fd 9</p>
</li>
<li>
<p>bridge_hello 2</p>
</li>
<li>
<p>bridge_maxage 12</p>
</li>
<li>
<p>bridge_stp off</p>
</li>
<li>
<p># /etc/init.d/networking restart</p>
</li>
<li>
<p># cat &lt;&lt; EOT &gt; /etc/libvirt/qemu.conf</p>
</li>
<li>
<p>user  = "oneadmin"</p>
</li>
<li>
<p>group = "oneadmin"</p>
</li>
<li>
<p>dynamic_ownership = 0</p>
</li>
<li>
<p>EOT</p>
</li>
<li>
<p># service libvirt-bin restart</p>
</li>
</ul>
<p>Con esta ejecución se puede acceder al portal de OpenNebula en la URL:  <a href="http://IP:9869"><em>http://IP:9869</em></a>. La password de oneadmin está en el fichero  ~/.one/one_auth.</p>
<p><em>Instalación de ONOS</em></p>
<p>Las instrucciones para instalar ONOS se encuentran en esta dirección IP, empezando en el punto 2. de las mismas</p>
<p><a href="https://wiki.onosproject.org/display/ONOS/ONOS+from+Scratch"><em>https://wiki.onosproject.org/display/ONOS/ONOS+from+Scratch</em></a></p>
<p>Pero las instrucciones se modificaron de esta manera:</p>
<p><strong>Instala <em>Git</em>:</strong></p>
<p>$sudo apt-get install git-core</p>
<p><strong>Descarga <em>Karaf</em> y <em>Maven</em>:</strong></p>
<p>Create two directories called <sub>/Downloads and </sub>/Applications. Download the <a href="http://download.nextag.com/apache/karaf/3.0.5/apache-karaf-3.0.5.tar.gz"><em>Karaf 3.0.5</em></a> and <a href="http://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz"><em>Maven 3.3.9</em></a> binaries (the tar.gz versions of both) into ~/Downloads and extract it to ~/Applications. Keep the tar archives in ~/Downloads; we'll need that later.</p>
<p>| Si decides utilizar directorios distintos, como se puede ver a continuación, entonces tendrás que editar el fichero ~/onos/tools/dev/bash_profile (en el caso de la raspberry pi es /home/pi/onos/tools/dev/bash_profile) y cambiar la configuración allí.</p>
<p>$ mkdir Downloads</p>
<p>$ mkdir /opt/Apps</p>
<p>$ cd Downloads</p>
<p>$ wget <a href="http://archive.apache.org/dist/karaf/3.0.5/apache-karaf-3.0.5.tar.gz"><em>http://archive.apache.org/dist/karaf/3.0.5/apache-karaf-3.0.5.tar.gz</em></a></p>
<p>$ wget <a href="http://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz"><em>http://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz</em></a></p>
<p>$ tar -zxvf apache-karaf-3.0.5.tar.gz -C /opt/Apps/</p>
<table>
<thead>
<tr>
<th>$ tar -zxvf apache-maven-3.3.9-bin.tar.gz -C /opt/Apps/</th>
</tr>
</thead>
<tbody></tbody>
</table>
<p><strong>Instala <em>java8</em>:</strong></p>
<p>$ sudo apt-get install software-properties-common -y</p>
<p>$ sudo add-apt-repository ppa:webupd8team/java -y</p>
<p>$ sudo apt-get update</p>
<p>$ sudo apt-get install oracle-java8-installer oracle-java8-set-default -y</p>
<p>En el caso de que no funcionara, descarga manualmente java (<a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html"><em>http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</em></a>)</p>
<p>E instálalo:</p>
<p>$sudo update-alternatives --install "/usr/bin/java" "java" "/opt/Apps/jdk1.8.0_77/bin/java" 1</p>
<p>$sudo update-alternatives --install "/usr/bin/javac" "javac" "/opt/Apps/jdk1.8.0_77/bin/javac" 1</p>
<p>$sudo update-alternatives --set java /opt/Apps/jdk1.8.0_77/bin/java</p>
<p>$sudo update-alternatives --set javac /opt/Apps/jdk1.8.0_77/bin/javac</p>
<p>Más info en: <a href="http://www.rpiblog.com/2014/03/installing-oracle-jdk-8-on-raspberry-pi.html"><em>http://www.rpiblog.com/2014/03/installing-oracle-jdk-8-on-raspberry-pi.html</em></a></p>
<p><strong>Descarga ONOS:</strong></p>
<p>$git clone <a href="https://gerrit.onosproject.org/onos"><em>https://gerrit.onosproject.org/onos</em></a></p>
<p>A partir de ahora suponemos que ONOS está instalado en el home del usuario, e.g. /home/pi/onos</p>
<p><strong>Incluye las siguientes lineas en el fichero <em>.bashrc</em>:</strong></p>
<p>. ~/onos/tools/dev/bash_profile</p>
<p>export ONOS_ROOT=~/onos</p>
<p>source $ONOS_ROOT/tools/dev/bash_profile</p>
<p><strong>Ejecuta el <em>bash</em> para que se carguen:</strong></p>
<p>$bash</p>
<p><strong>Comprueba que se han cargado correctamente:</strong></p>
<p>$ echo $ONOS_ROOT</p>
<p>/home/pi/onos</p>
<p>$ echo $KARAF_ROOT</p>
<p>/opt/Apps/apache-karaf-3.0.5</p>
<p><strong>Configura la instalación:</strong></p>
<p>Edita el fichero de configuración:</p>
<p>$sudo nano /opt/Apps/apache-karaf-3.0.5/etc/org.apache.karaf.features.cfg</p>
<p>Incluye el siguiente texto en la sección de <em>featuresRepositories:</em></p>
<p>mvn:org.onosproject/onos-features/1.5.0-SNAPSHOT/xml/features</p>
<p><strong>Compila con <em>maven</em> ONOS:</strong></p>
<p>$cd ~/onos</p>
<p>$ mvn clean install  # or use the alias 'mci'</p>
<p>Si todo ha ido bien debería compilarse entero, si tienes problemas, aquí hay posibles soluciones:</p>
<ul>
<li>
<p>Actualizar java a la última versión disponible</p>
</li>
<li>
<p>No hay forma de compilar onos-incubator-rpc-grpc, por lo que la única opción es no hacerlo, para ello basta con comentar la siguiente línea en el fichero ~/onos/incubator/pom.xml</p>
</li>
</ul>
<blockquote>
<p>&lt;!--</p>
<p>&lt;module&gt;rpc-grpc&lt;/module&gt;</p>
<p>--&gt;</p>
</blockquote>
<p><strong>Ahora hay que construir ONOS:</strong></p>
<p><a href="https://wiki.onosproject.org/display/ONOS/Running+ONOS+locally+on+development+machine"><em>https://wiki.onosproject.org/display/ONOS/Running+ONOS+locally+on+development+machine</em></a></p>
<p>$cd /home/pi/onos/tools/build</p>
<p>$onos-build</p>
<p><strong>Ejecutar ONOS:</strong></p>
<p>$onos-karaf</p>
<p><strong>Para acceder vía web:</strong></p>
<p><a href="http://192.168.0.200:8181/onos/ui/login.html#/topo"><em>http://192.168.0.200:8181/onos/ui/login.html#/topo</em></a></p>
<p>karaf/karaf</p>
<p>onos/rocks</p>
<p>Antes de que se me olvide, al intentar instalar mininet dice esto (hay que comprobar que funciona para CentOS):</p>
<p>Install.sh currently only supports Ubuntu, Debian, RedHat and Fedora.</p>
<p>Los entregables definidos a la fecha son</p>
<h2 id="caso-2-encadenamiento-de-servicios">Caso 2: Encadenamiento de servicios</h2>
<p>Esta prueba tiene como objetivo:</p>
<ul>
<li>
<p>Conectar un servicio de conectividad privada (MPLS, principalmente) con un elemento de computación en la nube que permita activar funciones de red dedicadas al cliente.</p>
</li>
<li>
<p>Verificar la capacidad de orquestación global del entorno desde el BSS hasta la configuración del servicio de manera automática.</p>
</li>
<li>
<p>Verificar las necesidades operativas que se derivan de este nuevo tipo de servicios</p>
</li>
<li>
<p>Analizar la estructura de costes de estos servicios y proponer soluciones eficientes en costes.</p>
</li>
</ul>
<p>Algunas conclusiones:</p>
<ul>
<li>
<p>La automatización debe incluir también una parte, aunque sea controlada de configuración automática de la red.</p>
</li>
<li>
<p>Se requieren nuevos conocimientos tecnológicos que compaginen conocimientos de routing, programación y sistemas de gestión, perfiles que aún no están desarrollados en las empresas como tales.</p>
</li>
<li>
<p>Se requiere un alto volumen de negocio para justificar las inversiones globales. Se deben probar soluciones de bajo coste (open source) que permitan reducir costes.</p>
</li>
</ul>
<p>Ilustración 14 Esquema del caso 2 - encadenamiento de servicios</p>
<p>Ilustración 15 Diagrama arquitectónico de los servicios residenciales del caso 2</p>
<p>Ilustración 16 Arquitectura del sistema de administración y gestión</p>
<p>Los beneficios a obtener en este caso</p>
<p>Los entregables planificados son</p>
<h2 id="caso-3-sdn-a-ip-sdn-ip-interconexion-de-redes-ciberdefinidas-con-redes-ip-legadas">Caso 3: SDN a IP (SDN-IP) interconexión de redes ciberdefinidas con redes IP legadas</h2>
<p>Esta prueba tiene como objetivo:</p>
<ul>
<li>
<p>Proporcionar conectividad nivel 3 sin utilizar enrutadores legados en el núcleo de red</p>
</li>
<li>
<p>Transformar con OpenFlow los Sistemas Autónomos (AS) en redes de tránsito IP BGP</p>
</li>
<li>
<p>Conectar redes ciberdefinidas con redes IP legadas utilizando BGP</p>
</li>
<li>
<p>Probar un mecanismo para la migración progresiva de nuestras redes IP a redes ciberdefinidas</p>
</li>
<li>
<p>Demostrar que podemos sustituir en el núcleo de la red los enrutadores por conmutadores</p>
</li>
<li>
<p>Agregar en confederaciones BGP distintos dominios administrativos ciberdefinidos, haciendo el plano de control más escalable</p>
</li>
</ul>
<p>Actualmente ONOS se ha desplegado globalmente en varias redes de América y Europa, como una solución de conectividad en nivel 3 para los usuarios, puramente en OpenFlow vía BGP, sin la necesidad de enrutadores en el núcleo.</p>
<p>Ilustración 17 Despliegue actual de ONOS/SDN-IP en el mundo</p>
<p>Ilustración 18 Despliegue de ONOS SDN enlazando 3 continentes</p>
<p>El primer despliegue sobre una red en explotación se realizó en EE. UU. en la red Internet2.</p>
<p>Ilustración 19 Mapa geográfico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S</p>
<p>La arquitectura utilizada se muestra en la siguiente ilustración, en nuestro caso queda por definirla para esta prueba, intentando que sea algo muy parecido.</p>
<p>Ilustración 20 Diagrama arquitectónico del despliegue de ONOS / SDN-IP en la red Internet2 AL2S</p>
<p>ONOS se ha desplegado al resto de América aprovechado la RedCLARA universitaria y de investigación y se ha extendido a Europa en la red GÉANT</p>
<p>Ilustración 21 Mapa geográfico del despliegue de ONOS / SDN-IP en la red FIU/RedClara</p>
<p>La arquitectura que es similar a la utilizada en Internet2 se muestra en la siguiente ilustración</p>
<blockquote>
<p>q</p>
</blockquote>
<p>Ilustración 22 Diagrama arquitectónico del despliegue de ONOS / SDN-IP en la red FIU/RedClara</p>
<p>Ilustración 23 Despliegue de ONOS en Europa</p>
<p>En el despliegue europeo se ha utilizado una aplicación desarrollada para ONOS, llamada ICONA y que permite gestionar de manera eficiente la intercomunicación de clusters de ONOS geográficamente distribuidos.</p>
<h2 id="caso-4-servicios-remotos-para-empresas">Caso 4: Servicios remotos para empresas</h2>
<p>Este caso de uso se definió conjuntamente con el proyecto Atrium de la Open Networking Foundation con la finalidad de incorporar en ONOS funcionalidad suficiente para permitir realizar “router peering” con soluciones abiertas y de marca blanca.</p>
<p>Ilustración 24 Diagrama arquitectónico de servicios remotos para empresas</p>
<p>Ilustración 25 Caso de uso de integración con Atrium "peering router"</p>
<p>Este año se realizó la prueba del caso de uso “router peering” con enrutadores localizados en Australia y EE.UU.</p>
<p>Ilustración 26 Prueba del caso de uso "router peering"</p>
<h3 id="caso-4-gestion-y-programabilidad-de-las-redes-ip-opticas">Caso 4: <em><a href="onenote:SDN%20+%20NFV.one#Redes%20de%20paquetes%20y%20óptica&amp;section-id=%7B0AD79E2E-4073-4569-8D92-0C7F95B19B6F%7D&amp;page-id=%7B9D584D41-1740-412E-8D80-77B10D6CDB7E%7D&amp;base-path=https://telefonicacorp-my.sharepoint.com/personal/alfonsoaurelio_carrilloaspiazu_telefonica_com/Documents">Gestión y programabilidad de las redes IP óptica</a>s</em></h3>
<p>Este caso de uso patrocinado por AT&amp;T puede resumirse como una implementación de paquetes ópticos ciberconectados SDN para redes. Incluido en este caso de uso hay aspectos como abstracciones de red multicapa en SDN, una aplicación, calendario, de ancho de banda bajo demanda de paquetes ópticos y refinamientos de la interfaz de usuario ONOS para orquestación, configuración de capa de datos y visualización de las redes flexibles y elásticas de transporte ópticas.</p>
<p>Ilustración 27 Caso de uso programabilidad red IP sobre Óptico</p>
<p>Ilustración 28 Topología del caso de uso</p>
<h2 id="sistema-de-administracion-y-gestion">Sistema de administración y gestión</h2>
<p>El sistema para la administración y gestión de la virtualización y ciberconectividad, así como su integración con los otros sistemas de gestión de la empresa, es clave para el éxito de estas soluciones, y a la vez está probando ser el punto de inflexión en las distintas integraciones que están intentando vender distintos fabricantes.</p>
<p>Para mantener cierta independencia de los fabricantes tradicionales, vamos a utilizar componentes desarrollados en código abierto por comunidades de usuarios y no tanto por fabricantes tradicionales, esto nos permitirá conjuntar un sistema de administración más acorde con nuestras necesidades globales, pero que al mismo tiempo nos permita reutilizar infraestructura que ya está en producción. Para ello hemos seleccionado OpenMano con OpenNebula, ONOS y queda pendiente verificar la necesidad del componente XOS, soluciones estas muy ligeras de código pero con las prestaciones que necesita una operadora de telecomunicaciones, a diferencia de soluciones más comerciales como OpenStack, OpenDaylight u otras.</p>
<p>Ilustración 29 Sistema de administración y gestión</p>
<blockquote>
<p>Los componentes del sistema de gestión y administración a utilizar son:</p>
</blockquote>
<ul>
<li>
<p><strong>OpenMano</strong> – orquestador de las funciones de red virtualizadas</p>
</li>
<li>
<p><strong>OpenNebula</strong> – gestor y orquestador de nubes que funciona sobre infraestructura heterogénea y de forma nativa resuelve la creación, gestión y administración de CPDs virtuales geográficamente dispersos</p>
</li>
<li>
<p><strong>ONOS</strong> – sistema operativo de la red desarrollado por ON.Lab, versión Cardinal o Drake</p>
</li>
</ul>
<h3 id="integracion">Integración</h3>
<p>El sistema debe ser capaz de administrar una topología nube como la ilustrada</p>
<p>Ilustración 30 Entorno de administración y gestión de red</p>
<p>Teniendo en cuenta que los emplazamientos pueden estar dispersos en una amplia zona geográfica de un país o pueden prestar servicios de una operadora a otra en el mismo o distintos continentes.</p>
<h2 id="definicion-y-dimensionamiento-de-componentes">Definición y dimensionamiento de componentes</h2>
<p>Los principales componentes y su dimensionamiento a utilizar…</p>
<ul>
<li>
<p>Infraestructura</p>
<ul>
<li>
<p>Servidores: <strong>OCP</strong></p>
</li>
<li>
<p>Almacenamiento: <strong>OCP + CEPH</strong></p>
</li>
<li>
<p>Conmutadores: <strong>OCP Accton</strong></p>
</li>
<li>
<p>CPE: cualquiera</p>
</li>
<li>
<p>OLT: OCP PMC-Sierra</p>
</li>
</ul>
</li>
<li>
<p>SO</p>
<ul>
<li>Computación: <strong>Linux</strong></li>
</ul>
</li>
<li>
<p>Hipervisor: <strong>KVM</strong></p>
</li>
<li>
<p>Gestor de la nube y servicios: <strong>OpenNebula</strong></p>
</li>
<li>
<p>S.O. de la red: <strong>ONOS</strong></p>
</li>
<li>
<p>Aplicaciones: <strong>vCPE NEC, OpenWRT</strong>, etc.</p>
</li>
</ul>
<h2 id="cronograma-de-actividades">Cronograma de actividades</h2>
<p>El cronograma de actividades se resume en la siguiente tabla</p>
<p>Ilustración 31 Cronograma</p>
<h2 id="evaluacion-de-nuestra-capacidad-para-disenar-desarrollar-desplegar-y-operar-las-tecnologias-de-red-y-sistemas">Evaluación de nuestra capacidad para diseñar, desarrollar, desplegar y operar las tecnologías de red y sistemas</h2>
<p>En las varias implantaciones de estas tecnologías en operadoras de nuestra envergadura, como ATT, DT, NTT y Verizon, ha quedado demostrado que el conocimiento interno ha sido un factor determinante para conseguir resultados significativos que les han permitido avanzar en implantar redes virtualizadas. En esta prueba vamos a determinar el tipo de conocimiento interno que debemos adquirir para diseñar, desarrollar, implantar, operar y mantener esta solución.</p></div>
        </div>

	<footer class="col-md-12">
		<hr>
		
		<p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
	</footer>


        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script src="../js/base.js"></script>
    </body>
</html>